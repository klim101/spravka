import os
import re
import html
import json
import time
import pickle
import logging
import asyncio
import threading
import functools
from pathlib import Path
from typing import List, Dict, Any, Tuple, Callable

import requests
import aiohttp
import nest_asyncio
import pandas as pd
import numpy as np
import streamlit as st
import tldextract
import openai
# tiktoken / BeautifulSoup Ğ¾Ğ¿Ñ†Ğ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹; Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ Ğ½Ğ° Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞµ
# import tiktoken
# from bs4 import BeautifulSoup

nest_asyncio.apply()
logging.basicConfig(level=logging.WARNING)

# Ğ¡ĞµĞºÑ€ĞµÑ‚Ñ‹ Ğ¸Ğ· Streamlit
KEYS = {
    "OPENAI_API_KEY": st.secrets["OPENAI_API_KEY"],
    "GOOGLE_API_KEY": st.secrets["GOOGLE_API_KEY"],
    "GOOGLE_CX":      st.secrets["GOOGLE_CX"],
    "CHECKO_API_KEY": st.secrets["CHECKO_API_KEY"],
    "DYXLESS_TOKEN":  st.secrets["DYXLESS_TOKEN"],
}
openai.api_key = KEYS["OPENAI_API_KEY"]
DYXLESS_TOKEN = KEYS["DYXLESS_TOKEN"]

# ĞšÑÑˆ Google
CACHE_FILE = Path(".google_cache.pkl")
try:
    _cache = pickle.loads(CACHE_FILE.read_bytes())
    GOOGLE_CACHE: dict = _cache.get("cache", {})
    QUERY_HISTORY: list = _cache.get("history", [])
except Exception:
    GOOGLE_CACHE, QUERY_HISTORY = {}, []

def _save_cache():
    try:
        CACHE_FILE.write_bytes(pickle.dumps({"cache": GOOGLE_CACHE,
                                             "history": QUERY_HISTORY}))
    except Exception:
        pass

def clear_google_cache():
    GOOGLE_CACHE.clear()
    QUERY_HISTORY.clear()
    _save_cache()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞÑ‚Ñ€Ğ°ÑĞ»Ğ¸ Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GROUPS = ["Industrials", "Consumer", "O&G", "M&M", "Retail",
          "Logistics", "FIG", "Services", "Agro", "TMT"]

GROUP_QUERY_TEMPLATES: dict[str, list[Callable[[str], str]]] = {
    "Industrials": [
        lambda c: f'"{c}" Ğ¾Ğ±ÑŠÑ‘Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°',
        lambda c: f'"{c}" Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ',
        lambda c: f'"{c}" Ñ„Ğ°Ğ±Ñ€Ğ¸ĞºĞ°',
        lambda c: f'"{c}" Ğ»Ğ¾Ğ³Ğ¸ÑÑ‚Ğ¸ĞºĞ°',
        lambda c: f'"{c}" Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ',
        lambda c: f'"{c}" r&d',
        lambda c: f'"{c}" Ğ¿Ğ°Ñ‚ĞµĞ½Ñ‚Ñ‹',
        lambda c: f'"{c}" ÑĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚ ISO',
        lambda c: f'"{c}" ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ',
        lambda c: f'"{c}" Ğ²Ğ¸Ğ´Ñ‹ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ†Ğ¸Ğ¸',
    ],
    "Consumer": [
        lambda c: f'"{c}" Ğ±Ñ€ĞµĞ½Ğ´Ñ‹',
        lambda c: f'"{c}" Ğ»Ğ¾Ğ³Ğ¾Ñ‚Ğ¸Ğ¿',
        lambda c: f'"{c}" Ñ†ĞµĞ»ĞµĞ²Ğ°Ñ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ',
    ],
    "O&G": [
        lambda c: f'"{c}" Ğ¾Ğ±ÑŠÑ‘Ğ¼ Ğ´Ğ¾Ğ±Ñ‹Ñ‡Ğ¸',
        lambda c: f'"{c}" Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ°',
        lambda c: f'"{c}" Ğ·Ğ°Ğ¿Ğ°ÑÑ‹',
    ],
    "M&M": [
        lambda c: f'"{c}" Ğ¾Ğ±ÑŠÑ‘Ğ¼ Ğ´Ğ¾Ğ±Ñ‹Ñ‡Ğ¸',
        lambda c: f'"{c}" Ğ·Ğ°Ğ¿Ğ°ÑÑ‹',
    ],
    "Retail": [
        lambda c: f'"{c}" ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¼Ğ°Ğ³Ğ°Ğ·Ğ¸Ğ½Ğ¾Ğ²',
        lambda c: f'"{c}" Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ Ğ¼Ğ°Ğ³Ğ°Ğ·Ğ¸Ğ½Ğ¾Ğ²',
        lambda c: f'"{c}" ĞºĞ²Ğ°Ğ´Ñ€Ğ°Ñ‚Ğ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ñ‹',
    ],
    "Logistics": [
        lambda c: f'"{c}" Ğ¿Ğ°Ñ€Ğº',
        lambda c: f'"{c}" Ğ¾Ğ±ÑŠÑ‘Ğ¼ Ğ³Ñ€ÑƒĞ·Ğ¾Ğ²',
        lambda c: f'"{c}" Ğ¾Ğ±ÑŠÑ‘Ğ¼ Ğ¿Ğ°ÑÑĞ°Ğ¶Ğ¸Ñ€Ğ¾Ğ²',
    ],
    "FIG": [
        lambda c: f'"{c}" Ğ±Ğ°Ğ»Ğ°Ğ½Ñ',
        lambda c: f'"{c}" Ñ‡Ğ¸ÑÑ‚Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ñ‹',
        lambda c: f'"{c}" Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ´Ğ¾Ñ…Ğ¾Ğ´',
        lambda c: f'"{c}" ĞºĞ¾Ğ¼Ğ¸ÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¾Ñ…Ğ¾Ğ´',
        lambda c: f'"{c}" Ğ´Ğ¾Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ ÑÑ‚Ñ€Ğ°Ñ…Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ',
    ],
    "Services": [
        lambda c: f'"{c}" gmv ÑƒÑĞ»ÑƒĞ³',
        lambda c: f'"{c}" Ğ¿Ñ€Ğ¾Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğµ',
    ],
    "Agro": [
        lambda c: f'"{c}" Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ Ğ·ĞµĞ¼Ğ»Ğ¸',
        lambda c: f'"{c}" Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½',
        lambda c: f'"{c}" ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ñ‹',
        lambda c: f'"{c}" Ğ¶Ğ¸Ğ²Ğ¾Ñ‚Ğ½Ñ‹Ğµ',
        lambda c: f'"{c}" Ğ¾Ğ±ÑŠÑ‘Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°',
        lambda c: f'"{c}" ÑĞºĞ»Ğ°Ğ´Ñ‹',
        lambda c: f'"{c}" ÑĞ»ĞµĞ²Ğ°Ñ‚Ğ¾Ñ€Ñ‹',
    ],
    "TMT": [
        lambda c: f'"{c}" ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹',
        lambda c: f'"{c}" ĞµĞ¶ĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ñ‹',
        lambda c: f'"{c}" Ğ¿Ñ€Ğ¾Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğµ',
        lambda c: f'"{c}" Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸',
        lambda c: f'"{c}" gmv',
        lambda c: f'"{c}" gbv',
        lambda c: f'"{c}" ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ğ¾Ğ²',
    ],
}

GROUP_SUMMARY_HINTS: dict[str, str] = {
    "Industrials": (
        "ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ğ¹ Ğ¾Ğ±ÑŠÑ‘Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ° Ğ¸ capacity, Ñ€Ğ°ÑĞ¿Ğ¾Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ñ„Ğ°Ğ±Ñ€Ğ¸ĞºĞ¸, "
        "Ğ»Ğ¾Ğ³Ğ¸ÑÑ‚Ğ¸ĞºÑƒ, Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ, Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ R&D Ğ¸ Ğ¿Ğ°Ñ‚ĞµĞ½Ñ‚Ğ¾Ğ², "
        "ÑĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚Ñ‹ ISO, ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ¾Ğ±Ğ¾Ñ€ÑƒĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ Ñ…Ğ°Ñ€Ğ°ĞºÑ‚ĞµÑ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°Ğ¼Ğ¸ Ğ¸ Ğ²Ğ¸Ğ´Ñ‹ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ†Ğ¸Ğ¸"
    ),
    "Consumer": "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞ¹ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ±Ñ€ĞµĞ½Ğ´Ğ¾Ğ² Ñ Ğ»Ğ¾Ğ³Ğ¾Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸ Ğ¸ Ñ†ĞµĞ»ĞµĞ²ÑƒÑ Ğ°ÑƒĞ´Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ñ",
    "O&G": "ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ¹ Ğ¾Ğ±ÑŠÑ‘Ğ¼ Ğ´Ğ¾Ğ±Ñ‹Ñ‡Ğ¸, Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸, Ğ·Ğ°Ğ¿Ğ°ÑÑ‹ Ğ¸ Ñ€ĞµÑÑƒÑ€ÑÑ‹",
    "M&M": "ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ¹ Ğ¾Ğ±ÑŠÑ‘Ğ¼ Ğ´Ğ¾Ğ±Ñ‹Ñ‡Ğ¸, Ğ·Ğ°Ğ¿Ğ°ÑÑ‹ Ğ¸ Ñ€ĞµÑÑƒÑ€ÑÑ‹",
    "Retail": "Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¸ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ Ğ¼Ğ°Ğ³Ğ°Ğ·Ğ¸Ğ½Ğ¾Ğ², Ğ¾Ğ±Ñ‰ÑƒÑ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ÑŒ Ğ² ĞºĞ². Ğ¼ĞµÑ‚Ñ€Ğ°Ñ…",
    "Logistics": "Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ¹ Ğ¿Ğ°Ñ€Ğº Ğ¸ Ğ¾Ğ±ÑŠÑ‘Ğ¼Ñ‹ Ğ³Ñ€ÑƒĞ·Ğ¾Ğ² Ğ¸Ğ»Ğ¸ Ğ¿Ğ°ÑÑĞ°Ğ¶Ğ¸Ñ€Ğ¾Ğ²",
    "FIG": (
        "Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ, Ñ‡Ğ¸ÑÑ‚Ñ‹Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ñ‹, Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¸ ĞºĞ¾Ğ¼Ğ¸ÑÑĞ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¾Ñ…Ğ¾Ğ´, "
        "Ğ´Ğ¾Ñ…Ğ¾Ğ´ Ğ¾Ñ‚ ÑÑ‚Ñ€Ğ°Ñ…Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¸ Ğ¿Ñ€Ğ¾Ñ‡Ğ¸Ñ… Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¾Ğ²"
    ),
    "Services": "ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ¹ GMV ÑƒÑĞ»ÑƒĞ³ Ğ¸ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ Ğ¿Ñ€Ğ¾Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ñ",
    "Agro": (
        "Ğ¾Ğ¿Ğ¸ÑÑ‹Ğ²Ğ°Ğ¹ Ğ¾Ğ±ÑŠÑ‘Ğ¼ Ğ·ĞµĞ¼Ğ»Ğ¸ Ğ¸ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½, Ğ²Ñ‹Ñ€Ğ°Ñ‰Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ ĞºÑƒĞ»ÑŒÑ‚ÑƒÑ€Ñ‹ Ğ¸Ğ»Ğ¸ Ğ¶Ğ¸Ğ²Ğ¾Ñ‚Ğ½Ñ‹Ñ…, "
        "Ğ¾Ğ±ÑŠÑ‘Ğ¼Ñ‹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°, Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ğµ ÑĞºĞ»Ğ°Ğ´Ğ¾Ğ² Ğ¸ ÑĞ»ĞµĞ²Ğ°Ñ‚Ğ¾Ñ€Ğ¾Ğ²"
    ),
    "TMT": (
        "Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ĞµĞ¹, ĞµĞ¶ĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€Ñ‹, Ğ¿Ñ€Ğ¾Ğ½Ğ¸ĞºĞ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğµ, "
        "Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ğ¸ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸"
    ),
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# UI helpers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_URL_PAT = re.compile(r"https?://[^\s)]+", flags=re.I)
def _linkify(text) -> str:
    if not isinstance(text, str):
        text = "" if text is None else str(text)
    def repl(m):
        u = html.escape(m.group(0))
        return f'<a href="{u}" target="_blank">ÑÑÑ‹Ğ»ĞºĞ°</a>'
    return _URL_PAT.sub(repl, text)

def long_job(total: int, key: str):
    for i in range(total + 1):
        time.sleep(1)
        st.session_state[key] = i / total
    st.session_state[key] = 1.0

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Dyxless (ĞºĞ°Ğº Ğ±Ñ‹Ğ»Ğ¾)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=3_600, show_spinner=False)
def dyxless_query(query: str,
                  token: str,
                  max_rows: int = 20_000) -> Dict[str, Any]:
    url = "https://api-dyxless.cfd/query"
    try:
        r = requests.post(url, json={"query": query, "token": token}, timeout=15)
        r.raise_for_status()
        res = r.json()
        if res.get("status") and isinstance(res.get("data"), list):
            full = len(res["data"])
            if full > max_rows:
                res["data"] = res["data"][:max_rows]
                res.update(truncated=True, original_counts=full, counts=max_rows)
        return res
    except requests.RequestException as e:
        return {"status": False, "error": str(e)}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Google Search helpers (Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸: PDF Ñ€Ğ°Ğ·Ñ€ĞµÑˆÑ‘Ğ½, Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ q=128, ĞºÑÑˆ, rerank)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞĞ• Ğ±Ğ°Ğ½Ğ¸Ğ¼ PDF (Ğ²Ğ°Ğ¶Ğ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸). Ğ‘Ğ°Ğ½Ğ¸Ğ¼ ÑĞ¾Ñ†ÑĞµÑ‚Ğ¸/ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞ¸.
_BAD = ("vk.com", "facebook.", ".jpg", ".png")
HEADERS = {"User-Agent": "Mozilla/5.0 (Win64) AppleWebKit/537.36 Chrome/125 Safari/537.36"}

def _bad(u: str) -> bool:
    return any(b in u.lower() for b in _BAD)

def unique(seq):
    seen = set()
    out = []
    for x in seq:
        k = x.strip().lower()
        if k and k not in seen:
            seen.add(k); out.append(x.strip())
    return out

# Â«ÑĞ¸Ğ³Ğ½Ğ°Ğ»Ñ‹ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸Â» Ğ´Ğ»Ñ rerank
POWER_SIGNALS = [
    r"\bÑ‚/ÑÑƒÑ‚\b", r"\bÑ‚Ğ¾Ğ½Ğ½[Ğ°Ñ‹] Ğ² ÑÑƒÑ‚ĞºĞ¸\b", r"\bÑ‚/Ğ³Ğ¾Ğ´\b", r"\bÑ‚Ğ¾Ğ½Ğ½[Ğ°Ñ‹] Ğ² Ğ³Ğ¾Ğ´\b",
    r"\bĞ¼Ğ¾Ñ‰Ğ½\w+\b", r"\bcapacity\b", r"\bĞ¼Â²\b", r"\bĞºĞ²\.?\s*Ğ¼\b",
    r"\bISO\s*9\d{2,}\b", r"\bMW\b|\bĞœĞ’Ñ‚\b|\bGWh\b",
]
_SIG = [re.compile(p, flags=re.I) for p in POWER_SIGNALS]

def score_snip(url: str, txt: str) -> int:
    s = 0
    for rgx in _SIG:
        if rgx.search(url) or rgx.search(txt):
            s += 1
    if url.lower().endswith(".pdf"): s += 1
    if re.search(r"\.(gov|edu)\b", url.lower()): s += 1
    return s

def rerank_snippets(snips: list[tuple[str, str]]) -> list[tuple[str, str]]:
    seen, uniq_snips = set(), []
    for u, t in snips:
        if u not in seen:
            seen.add(u)
            uniq_snips.append((u, t))
    return sorted(uniq_snips, key=lambda x: score_snip(*x), reverse=True)

async def _google(sess: aiohttp.ClientSession, q: str, n: int = 3):
    q = re.sub(r'[\"\'â€œâ€]', " ", q)[:128]
    cache_key = (q, n)
    if cache_key in GOOGLE_CACHE:
        QUERY_HISTORY.append(q); return GOOGLE_CACHE[cache_key]

    params = {
        "key": KEYS["GOOGLE_API_KEY"], "cx": KEYS["GOOGLE_CX"],
        "q": q, "num": n, "hl": "ru", "gl": "ru"
    }
    try:
        async with sess.get("https://www.googleapis.com/customsearch/v1",
                            params=params, headers=HEADERS, timeout=8) as r:
            if r.status != 200:
                logging.warning(f"Google error {r.status}")
                return []
            js = await r.json()
            res = [(it["link"], it.get("snippet", "")) for it in js.get("items", []) if not _bad(it["link"])]
    except asyncio.TimeoutError:
        logging.warning("[Google] timeout")
        res = []

    GOOGLE_CACHE[cache_key] = res
    QUERY_HISTORY.append(q)
    _save_cache()
    return res

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OpenAI helpers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def _gpt(messages, *, model="gpt-4o-mini", T=0.2) -> str:
    chat = await openai.ChatCompletion.acreate(model=model, temperature=T, messages=messages)
    return chat.choices[0].message.content.strip()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SiteRAG (ĞºĞ°Ğº Ğ±Ñ‹Ğ»Ğ¾, Ğ±ĞµĞ· Ğ»Ğ¸ÑˆĞ½Ğ¸Ñ… Ğ´ÑƒĞ±Ğ»ĞµĞ¹)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class SiteRAG:
    def __init__(self, url: str, *, model="gpt-4o-mini", max_chunk: int = 6_000, T: float = 0.18):
        if url and not url.startswith(("http://", "https://")):
            url = "http://" + url
        self.url       = url
        self.model     = model
        self.max_chunk = max_chunk
        self.T         = T

    async def _fetch(self) -> str:
        if not self.url:
            raise RuntimeError("URL is empty")
        h = {"User-Agent": "Mozilla/5.0"}
        async with aiohttp.ClientSession(headers=h) as sess:
            async with sess.get(self.url, timeout=20) as r:
                if r.status == 200 and "text/html" in (r.headers.get("Content-Type", "")):
                    return await r.text("utf-8", errors="ignore")
                raise RuntimeError(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ ÑĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ {self.url} (status={r.status})")

    def _split(self, html_raw: str) -> list[str]:
        body = re.split(r"</?(?:body|div|section|article)[^>]*>", html_raw, flags=re.I)
        chunks, buf = [], ""
        for part in body:
            if len(buf) + len(part) > self.max_chunk:
                chunks.append(buf); buf = part
            else:
                buf += part
        if buf:
            chunks.append(buf)
        return chunks

    async def _summarise_chunk(self, n: int, total: int, chunk: str) -> str:
        sys = (
            "Ğ¢Ñ‹ â€“ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº. ĞŸÑ€Ğ¾Ñ‡Ğ¸Ñ‚Ğ°Ğ¹ HTML-Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚ Ğ¸ Ğ²Ñ‹Ğ¿Ğ¸ÑˆĞ¸ Ğ’Ğ¡Ğ• Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ñ‹ "
            "(Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ñ‹, ÑƒÑĞ»ÑƒĞ³Ğ¸, Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ, Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ, ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹, Ñ†Ğ¸Ñ„Ñ€Ñ‹, ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°, ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹ Ğ¸ Ğ¿Ñ€.). "
            "Ğ˜Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒĞ¹ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ/footer/ÑĞºÑ€Ğ¸Ğ¿Ñ‚Ñ‹. ĞŸĞ¸ÑˆĞ¸ ĞºÑ€Ğ°Ñ‚ĞºĞ¾, Ğ°Ğ±Ğ·Ğ°Ñ†Ğ°Ğ¼Ğ¸."
        )
        return await _gpt(
            [{"role": "system", "content": sys},
             {"role": "user", "content": f"HTML_CHUNK_{n}/{total} (len={len(chunk):,}):\n{chunk}"}],
            model=self.model, T=self.T
        )

    async def _summarise_overall(self, parts: list[str]) -> str:
        sys = (
            "ĞĞ¸Ğ¶Ğµ ĞºĞ¾Ğ½ÑĞ¿ĞµĞºÑ‚Ñ‹ Ñ‡Ğ°ÑÑ‚ĞµĞ¹ ÑĞ°Ğ¹Ñ‚Ğ°. ĞĞ° Ğ¸Ñ… Ğ¾ÑĞ½Ğ¾Ğ²Ğµ ÑĞ¾ÑÑ‚Ğ°Ğ²ÑŒ Ğ¾Ğ´Ğ¸Ğ½ ĞŸĞĞ›ĞĞ«Ğ™ Ğ¸ ÑĞ²ÑĞ·Ğ½Ñ‹Ğ¹ Ğ¿Ğ°ÑĞ¿Ğ¾Ñ€Ñ‚ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸: "
            "ĞºÑ‚Ğ¾ Ğ¾Ğ½Ğ¸; Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ñ‹/ÑƒÑĞ»ÑƒĞ³Ğ¸; Ñ€Ñ‹Ğ½Ğ¾Ğº Ğ¸ ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñ‹; Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¸ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ; Ğ³ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ Ğ¸ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ñ‹; "
            "Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»Ğ¸/ĞºĞ¾Ğ¼Ğ°Ğ½Ğ´Ğ°; Ğ»ÑĞ±Ñ‹Ğµ Ñ†Ğ¸Ñ„Ñ€Ñ‹ Ğ¸ Ñ„Ğ°ĞºÑ‚Ñ‹; Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¾ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ Ğ¿ĞµÑ€ÑĞ¿ĞµĞºÑ‚Ğ¸Ğ²Ğ°Ñ…."
        )
        merged = "\n\n".join(parts)
        return await _gpt(
            [{"role": "system", "content": sys},
             {"role": "user", "content": merged}],
            model=self.model, T=self.T
        )

    async def _run_async(self):
        html_raw = await self._fetch()
        chunks   = self._split(html_raw)
        part_summaries = []
        for idx, ch in enumerate(chunks, 1):
            part_summaries.append(await self._summarise_chunk(idx, len(chunks), ch))
        summary_final = await self._summarise_overall(part_summaries)
        return {"summary": summary_final, "chunks_out": part_summaries,
                "html_size": f"{len(html_raw):,} bytes", "url": self.url}

    def run(self) -> dict:
        try:
            loop = asyncio.get_event_loop()
            if loop and loop.is_running():
                return loop.run_until_complete(self._run_async())
        except RuntimeError:
            pass
        return asyncio.run(self._run_async())

def _site_passport_sync(url: str, *, max_chunk: int = 6_000) -> str:
    if not url:
        return ""
    try:
        return SiteRAG(url, max_chunk=max_chunk).run()["summary"]
    except Exception as exc:
        return f"[site passport error: {exc}]"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ¨Ğ°Ğ±Ğ»Ğ¾Ğ½Ñ‹ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def BASE_TEMPLATES(C: str) -> list[str]:
    return unique([
        f'"{C}" Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ',
        f'"{C}" Ğ±Ñ€ĞµĞ½Ğ´Ñ‹',
        f'"{C}" Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ',
        f'"{C}" ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑĞ¾Ñ‚Ñ€ÑƒĞ´Ğ½Ğ¸ĞºĞ¾Ğ²',
        f'"{C}" Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸',
        f'"{C}" Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚ÑŒ Ğ·Ğ°Ğ²Ğ¾Ğ´Ğ°',
        f'"{C}" Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´ĞºĞ°',
        f'"{C}" Ğ¾Ğ±ÑŠÑ‘Ğ¼ Ğ²Ñ‹Ğ¿ÑƒÑĞºĞ°',
        f'"{C}" Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ',
        f'"{C}" Ğ¸Ğ½Ğ²ĞµÑÑ‚Ğ¸Ñ†Ğ¸Ğ¸',
        f'"{C}" Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğµ',
        f'"{C}" ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ·Ğ°Ğ²Ğ¾Ğ´Ğ°',
        f'"{C}" Ğ°Ğ´Ñ€ĞµÑ',
        f'"{C}" Ğ¾Ñ„Ğ¸Ñ',
        f'"{C}" Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ',
        f'"{C}" Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ',
        f'"{C}" Ğ¾Ğ±ÑŠÑ‘Ğ¼ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²Ğ°',
        f'"{C}" ĞºĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ñ‹',
        f'"{C}" Ñ€ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³',
        f'Ñ„Ğ¾Ñ€ÑƒĞ¼ "{C}"',
        f'site:news.* "{C}"',
        f'"{C}" filetype:pdf',
        f'"{C}" Ğ¿Ñ€ĞµĞ·ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ filetype:pdf',
        f'"{C}" Ñ‚ĞµĞ½Ğ´ĞµÑ€ filetype:pdf',
    ])

def SOCIAL_TEMPLATES(C: str, dom: str) -> list[str]:
    base = [f'"{C}" site:{s}' for s in ["vk.com","facebook.com","linkedin.com","youtube.com","ok.ru"]]
    if dom:
        base.append(f'"{C}" site:{dom}')
    base += [f'"{C}" ÑĞ°Ğ¹Ñ‚', f'"{C}" linkedin', f'"{C}" youtube', f'"{C}" Ğ²Ğº']
    return unique(base)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RAG (Ğ¿ĞµÑ€ĞµĞ¿Ğ¸ÑĞ°Ğ½: Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², rerank, 2-ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ summary)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class RAG:
    """
    summary     â€“ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ (Google-ÑĞ½Ğ¸Ğ¿Ğ¿ĞµÑ‚Ñ‹ + Ğ¿Ğ°ÑĞ¿Ğ¾Ñ€Ñ‚ ÑĞ°Ğ¹Ñ‚Ğ°)
    queries     â€“ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ²ÑĞµÑ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
    snippets    â€“ [(url, snippet)] Ğ¿Ğ¾ÑĞ»Ğµ rerank Ğ¸ dedup
    news_snips  â€“ ÑĞ½Ğ¸Ğ¿Ğ¿ĞµÑ‚Ñ‹ Ñ ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ½Ñ‹Ñ… ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²
    site_ctx    â€“ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ ÑĞ½Ğ¸Ğ¿Ğ¿ĞµÑ‚ Â«site:<Ğ´Ğ¾Ğ¼ĞµĞ½> â€¦Â»
    site_pass   â€“ Ğ¿Ğ°ÑĞ¿Ğ¾Ñ€Ñ‚ ÑĞ°Ğ¹Ñ‚Ğ°
    """
    def __init__(
        self,
        company: str,
        *,
        website: str = "",
        market: str = "",
        years=(2022, 2023, 2024),
        country: str = "Ğ Ğ¾ÑÑĞ¸Ñ",
        steps: int = 2,
        snips: int = 4,
        llm_model: str = "gpt-4o-mini",
        facts_model: str = "gpt-4o",         # Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ĞµĞµ ÑĞ¸Ğ»ÑŒĞ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ
        render_model: str = "gpt-4o-mini",
        company_info: dict | None = None,
        group: str = "",
    ):
        self.company   = company.strip()
        self.website   = website.strip()
        self.market    = market.strip()
        self.country   = country
        self.years     = years
        self.steps     = steps
        self.snips     = snips
        self.llm_model = llm_model
        self.facts_model = facts_model or llm_model
        self.render_model = render_model or llm_model
        self.company_info = company_info or {}
        self.group = group.strip()

    # â”€â”€ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Â«site:Ğ´Ğ¾Ğ¼ĞµĞ½Â»
    async def _site_ctx(self) -> str:
        dom = tldextract.extract(self.website).registered_domain if self.website else ""
        if not dom:
            return f"Ñ€Ñ‹Ğ½Ğ¾Ğº ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸ â€“ {self.market}" if self.market else ""
        async with aiohttp.ClientSession() as sess:
            snips = await _google(sess, f"site:{dom}", n=1)
            base = snips[0][1] if snips else ""
        if base and self.market:
            return f"{base}\nÑ€Ñ‹Ğ½Ğ¾Ğº ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸ â€“ {self.market}"
        return base or (f"Ñ€Ñ‹Ğ½Ğ¾Ğº ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸ â€“ {self.market}" if self.market else "")

    # â”€â”€ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² (Ğ´ĞµÑ‚ĞµÑ€Ğ¼Ğ¸Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğµ ÑĞ´Ñ€Ğ¾ + 10 LLM-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²)
    async def _queries(self) -> list[str]:
        dom = tldextract.extract(self.website).registered_domain if self.website else ""
        C = self.company

        seeds = BASE_TEMPLATES(C)
        if self.group:
            seeds += [tpl(C) for tpl in GROUP_QUERY_TEMPLATES.get(self.group, [])]
        seeds = unique(seeds)

        extras = SOCIAL_TEMPLATES(C, dom)
        ql = unique(seeds + extras)

        sys = (
            "Ğ”Ğ°Ğ¹ 10 Ğ´Ğ¾Ğ¿. Google-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ Ğ´Ğ»Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸ "
            f"Â«{C}Â» (ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ÑŒ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸), "
            "ÑƒĞ¿Ğ¾Ñ€ Ğ½Ğ° Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸/Ğ¾Ğ±ÑŠÑ‘Ğ¼Ñ‹/Ğ°Ğ´Ñ€ĞµÑĞ°/Ğ¸Ğ½Ğ²ĞµÑÑ‚Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñ‹. "
            "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹ site:, intitle:, inurl:, filetype:pdf, OR. "
            "Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚: QUERY: <ÑÑ‚Ñ€Ğ¾ĞºĞ°>. ĞĞ¸Ñ‡ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ."
        )
        raw = await _gpt(
            [{"role": "system", "content": sys},
             {"role": "user", "content": ""}],
            model=self.llm_model, T=0.05
        )
        llm_q = re.findall(r"QUERY:\s*(.+)", raw, flags=re.I)

        def add_market(q: str) -> str:
            if self.market and self.market.lower() not in q.lower():
                return f'{q} "{self.market}"'
            return q

        ql = unique([add_market(q) for q in (ql + llm_q)])
        return ql[:60]   # Ğ¶Ñ‘ÑÑ‚ĞºĞ¸Ğ¹ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚

    # â”€â”€ 2-ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´: Ñ„Ğ°ĞºÑ‚Ñ‹(JSON) â†’ Ñ€ĞµĞ½Ğ´ĞµÑ€ Ñ‚ĞµĞºÑÑ‚Ğ°
    async def _facts_json(self, ctx: str) -> str:
        hint = GROUP_SUMMARY_HINTS.get(self.group, "")
        sys = (
            "Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ¸ Ğ¡Ğ¢Ğ Ğ£ĞšĞ¢Ğ£Ğ Ğ˜Ğ ĞĞ’ĞĞĞĞ«Ğ• Ğ¤ĞĞšĞ¢Ğ« Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸ Ğ² JSON Ğ±ĞµĞ· Ğ»Ğ¸ÑˆĞ½ĞµĞ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°. "
            "Ğ¡Ñ…ĞµĞ¼Ğ°: {"
            '"description": str|null,'
            '"brands": [str],'
            '"headcount": {"value": number|null, "year": number|null, "sources":[string]},'
            '"capacity": [{"metric": str, "value": str, "year": number|null, "site": str|null, "sources":[string]}],'
            '"investments": [{"desc": str, "amount": str|null, "year": number|null, "sources":[string]}],'
            '"addresses": [{"type":"hq|plant", "value": str, "sources":[string]}],'
            '"socials": [{"type":"site|vk|fb|linkedin|youtube|ok", "url": str}],'
            '"history": [{"year": number|null, "event": str, "sources":[string]}],'
            '"production": [{"product": str, "volume": str, "period": str|null, "sources":[string]}],'
            '"competitors": [{"name": str, "site": str|null}],'
            '"mentions": [{"type":"forum|rating|news", "url": str}]'
            "}. "
            "ĞŸĞ¸ÑˆĞ¸ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğ¹ JSON. Ğ•ÑĞ»Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ½ĞµÑ‚ â€” null/[] ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾. "
            + (f"Ğ£Ñ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ğ¹ Ğ¾Ñ‚Ñ€Ğ°ÑĞ»ĞµĞ²Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚: {hint} " if hint else "")
        )
        return await _gpt(
            [{"role": "system", "content": sys},
             {"role": "user", "content": ctx[:18000]}],
            model=self.facts_model, T=0.05
        )

    async def _render_text_from_facts(self, facts_json: str) -> str:
        sys = (
            "Ğ’Ğ¾Ğ·ÑŒĞ¼Ğ¸ ÑÑ‚Ğ¾Ñ‚ JSON Ñ Ñ„Ğ°ĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¸ ÑĞ²ĞµÑ€ÑÑ‚Ğ°Ğ½Ğ¸ Ğ½ĞµĞ¿Ñ€Ğ¾Ñ‚Ğ¸Ğ²Ğ¾Ñ€ĞµÑ‡Ğ¸Ğ²Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ±ĞµĞ· Markdown "
            "ÑÑ‚Ñ€Ğ¾Ğ³Ğ¸Ğ¼ Ğ¿Ğ¾Ñ€ÑĞ´ĞºĞ¾Ğ¼ Ñ€Ğ°Ğ·Ğ´ĞµĞ»Ğ¾Ğ²: 1) ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ; 2) Ğ‘Ñ€ĞµĞ½Ğ´Ñ‹; 3) Ğ§Ğ¸ÑĞ»ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ; "
            "4) ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ñ‰Ğ½Ğ¾ÑÑ‚Ğ¸; 5) Ğ˜Ğ½Ğ²ĞµÑÑ‚Ğ¸Ñ†Ğ¸Ğ¸ Ğ¸ Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ñ; 6) ĞĞ´Ñ€ĞµÑĞ° HQ Ğ¸ Ğ¿Ğ»Ğ¾Ñ‰Ğ°Ğ´Ğ¾Ğº; "
            "7) Ğ¡Ğ¾Ñ†ÑĞµÑ‚Ğ¸; 8) Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ; 9) ĞŸÑ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ/Ğ¾Ğ±ÑŠÑ‘Ğ¼Ñ‹ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ†Ğ¸Ğ¸; 10) ĞšĞ¾Ğ½ĞºÑƒÑ€ĞµĞ½Ñ‚Ñ‹; 11) Ğ£Ğ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ñ. "
            "ĞŸĞ¾ÑĞ»Ğµ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ñ„Ğ°ĞºÑ‚Ğ° Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞ¹ ÑÑÑ‹Ğ»ĞºÑƒ Ğ² ĞºÑ€ÑƒĞ³Ğ»Ñ‹Ñ… ÑĞºĞ¾Ğ±ĞºĞ°Ñ… (Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ URL). "
            "ĞĞµ ÑƒĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ¹ Ğ²Ñ‹Ñ€ÑƒÑ‡ĞºÑƒ. ĞšĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¾ Ğ¸ Ğ¿Ğ¾ Ğ´ĞµĞ»Ñƒ."
        )
        return await _gpt(
            [{"role": "system", "content": sys},
             {"role": "user", "content": facts_json}],
            model=self.render_model, T=0.15
        )

    def _normalize_sections(self, summary: str) -> str:
        sections = summary.split("\n\n")
        norm = []
        for sec in sections:
            lines = [l.strip() for l in sec.splitlines() if l.strip()]
            seen, uniq = set(), []
            for line in lines:
                if line not in seen:
                    seen.add(line); uniq.append(line)
            norm.append("\n".join(uniq) if uniq else "Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾")
        return "\n\n".join(norm)

    async def _run_async(self):
        # Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ¾: site snippet Ğ¸ Ğ¿Ğ°ÑĞ¿Ğ¾Ñ€Ñ‚ ÑĞ°Ğ¹Ñ‚Ğ°
        site_ctx_task = asyncio.create_task(self._site_ctx())
        site_pass_task = (
            asyncio.create_task(asyncio.to_thread(_site_passport_sync, self.website))
            if self.website else None
        )

        queries, snippets = [], []
        news_snippets: list[tuple[str, str]] = []

        async with aiohttp.ClientSession() as s:
            # Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² 1 Ñ€Ğ°Ğ· (ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ĞµĞµ)
            ql = await self._queries()
            queries += ql

            # Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ÑĞ±Ğ¾Ñ€ ÑĞ½Ğ¸Ğ¿Ğ¿ĞµÑ‚Ğ¾Ğ²
            res = await asyncio.gather(*[_google(s, q, self.snips) for q in ql])
            snippets += sum(res, [])

            # ĞºÑ€ÑƒĞ¿Ğ½Ñ‹Ğµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸
            news_domains = ["rbc.ru","kommersant.ru","vedomosti.ru","tass.ru","forbes.ru"]
            news_queries = [f'site:{d} "{self.company}"' for d in news_domains]
            queries += news_queries
            res = await asyncio.gather(*[_google(s, q, self.snips) for q in news_queries])
            news_snippets = sum(res, [])
            snippets += news_snippets

        # rerank + dedup
        snippets = rerank_snippets(snippets)

        # ÑĞ¾Ñ†ÑĞµÑ‚Ğ¸ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾
        dom = tldextract.extract(self.website).registered_domain if self.website else ""
        social_domains = ["vk.com", "facebook.com", "linkedin.com", "youtube.com", "ok.ru"]
        if dom: social_domains.append(dom)
        social_snips = [(u, t) for u, t in snippets if any(sd in u.lower() or sd in t.lower() for sd in social_domains)]

        site_ctx  = await site_ctx_task
        site_pass = await site_pass_task if site_pass_task else ""

        # ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ LLM: Ğ±ĞµÑ€Ñ‘Ğ¼ Ñ‚Ğ¾Ğ¿ Ğ¿Ğ¾ÑĞ»Ğµ rerank (Ğ»Ğ¸Ğ¼Ğ¸Ñ‚ Ğ¿Ğ¾ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñƒ)
        ctx_parts: list[str] = []
        if site_ctx:  ctx_parts.append(f"SITE_SNIPPET:\n{site_ctx}")
        if site_pass: ctx_parts.append(f"SITE_PASSPORT:\n{site_pass}")

        # Ñ„Ğ°ĞºÑ‚Ñ‹ Ğ¸Ğ· company_info (ĞµÑĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½Ñ‹)
        company_doc_txt = ""
        if self.company_info:
            def _pair(k, v):
                if v in (None, "", []): return ""
                if isinstance(v, list): v = "; ".join(map(str, v[:10]))
                return f"* **{k}:** {v}"
            company_doc_txt = "\n".join(
                p for p in (_pair(k, v) for k, v in self.company_info.items()) if p
            )
            if company_doc_txt:
                ctx_parts.append(f"COMPANY_DOC:\n{company_doc_txt}")

        if social_snips:
            ctx_parts.append(
                "SOCIAL_SNIPPETS:\n" + "\n".join(f"URL:{u}\nTXT:{t}" for u, t in social_snips[:30])
            )

        # Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ Ğ¾Ğ±ÑŠÑ‘Ğ¼ ÑĞ½Ğ¸Ğ¿Ğ¿ĞµÑ‚Ğ¾Ğ² (Ğ¿Ğ¾ÑĞ»Ğµ rerank)
        main_snips_txt = []
        total_len = 0
        for u, t in snippets:
            line = f"URL:{u}\nTXT:{t}"
            if total_len + len(line) > 16000:
                break
            main_snips_txt.append(line)
            total_len += len(line)
        ctx_parts.append("\n".join(main_snips_txt))

        # Ğ´Ğ²ÑƒÑ…ÑˆĞ°Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ²Ñ‹Ğ²Ğ¾Ğ´
        facts = await self._facts_json("\n\n".join(ctx_parts))
        summary = await self._render_text_from_facts(facts)
        summary = self._normalize_sections(summary)

        return {
            "summary":       summary,
            "queries":       queries,
            "snippets":      snippets,
            "news_snippets": news_snippets,
            "site_ctx":      site_ctx,
            "site_pass":     site_pass,
            "company_doc":   company_doc_txt,
        }

    def run(self) -> dict:
        try:
            loop = asyncio.get_event_loop()
            if loop and loop.is_running():
                return loop.run_until_complete(self._run_async())
        except RuntimeError:
            pass
        return asyncio.run(self._run_async())











# â•­â”€ğŸŒ  Market RAG helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
async def google_snippets(query: str, num: int = 4):
    q = re.sub(r'[\"\'â€œâ€]', '', query)[:80]
    params = {
        "key": KEYS["GOOGLE_API_KEY"],
        "cx":  KEYS["GOOGLE_CX"],
        "q":   q, "num": num, "hl": "ru", "gl": "ru"
    }
    async with aiohttp.ClientSession() as sess:
        async with sess.get("https://www.googleapis.com/customsearch/v1",
                            params=params, headers=HEADERS, timeout=8) as r:
            if r.status != 200:
                logging.warning(f"[Google] {r.status}")
                return []
            js = await r.json()
            return [(it["link"], it.get("snippet", ""))
                    for it in js.get("items", []) if not _bad(it["link"])]

async def gpt_async(messages, T=0.2, model="gpt-4o-mini"):
    chat = await openai.ChatCompletion.acreate(
        model=model, temperature=T, messages=messages)
    return chat.choices[0].message.content.strip()

class FastMarketRAG:
    """Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ dict(summary, queries, snippets) Ğ·Ğ° ~10 Ñ."""
    def __init__(self, market, country="Ğ Ğ¾ÑÑĞ¸Ñ",
                 years=(2021, 2022, 2023, 2024),
                 steps=1, snips=6):
        self.market, self.country = market, country
        self.years, self.steps, self.snips = years, steps, snips



    
    async def _queries(self, hist=""):
        sys = (
            "Ğ¢Ğ« â€” ĞĞŸĞ«Ğ¢ĞĞ«Ğ™ Ğ˜Ğ¡Ğ¡Ğ›Ğ•Ğ”ĞĞ’ĞĞ¢Ğ•Ğ›Ğ¬ Ğ Ğ«ĞĞšĞĞ’ Ğ˜ Ğ”ĞĞĞĞ«Ğ¥. Ğ¡Ğ¤ĞĞ ĞœĞ£Ğ›Ğ˜Ğ Ğ£Ğ™ 10â€“12 Ğ¢ĞĞ§ĞĞ«Ğ¥ Ğ˜ Ğ­Ğ¤Ğ¤Ğ•ĞšĞ¢Ğ˜Ğ’ĞĞ«Ğ¥ GOOGLE-Ğ—ĞĞŸĞ ĞĞ¡ĞĞ’, "
            f"ĞĞĞŸĞ ĞĞ’Ğ›Ğ•ĞĞĞ«Ğ¥ ĞĞ Ğ¡Ğ‘ĞĞ  Ğ¡Ğ¢Ğ Ğ£ĞšĞ¢Ğ£Ğ Ğ˜Ğ ĞĞ’ĞĞĞĞĞ™ Ğ˜ĞĞ¤ĞĞ ĞœĞĞ¦Ğ˜Ğ˜ Ğ Ğ Ğ«ĞĞšĞ• Â«{self.market}Â» Ğ’ Ğ¡Ğ¢Ğ ĞĞĞ• {self.country.upper()} Ğ—Ğ ĞŸĞ•Ğ Ğ˜ĞĞ” {', '.join(map(str, self.years))}. "
            "ĞŸĞĞ˜Ğ¡ĞšĞĞ’Ğ«Ğ• Ğ—ĞĞŸĞ ĞĞ¡Ğ« Ğ”ĞĞ›Ğ–ĞĞ« ĞĞ¥Ğ’ĞĞ¢Ğ«Ğ’ĞĞ¢Ğ¬ Ğ¡Ğ›Ğ•Ğ”Ğ£Ğ®Ğ©Ğ˜Ğ• ĞĞ¡ĞŸĞ•ĞšĞ¢Ğ« Ğ Ğ«ĞĞšĞ: "
            "1) ĞĞ‘ĞªĞĞœ Ğ˜ Ğ”Ğ˜ĞĞĞœĞ˜ĞšĞ Ğ Ğ«ĞĞšĞ, "
            "2) Ğ¡Ğ¢Ğ Ğ£ĞšĞ¢Ğ£Ğ Ğ Ğ˜ Ğ¡Ğ•Ğ“ĞœĞ•ĞĞ¢ĞĞ¦Ğ˜Ğ¯, "
            "3) ĞĞ¡ĞĞĞ’ĞĞ«Ğ• Ğ˜Ğ“Ğ ĞĞšĞ˜ Ğ˜ Ğ˜Ğ¥ Ğ”ĞĞ›Ğ˜, "
            "4) Ğ¦Ğ•ĞĞ« Ğ˜ Ğ¦Ğ•ĞĞĞ’Ğ«Ğ• Ğ¢Ğ•ĞĞ”Ğ•ĞĞ¦Ğ˜Ğ˜, "
            "5) ĞšĞ›Ğ®Ğ§Ğ•Ğ’Ğ«Ğ• Ğ¢Ğ Ğ•ĞĞ”Ğ« Ğ˜ Ğ˜ĞĞĞ’ĞĞ¦Ğ˜Ğ˜, "
            "6) Ğ Ğ•Ğ“Ğ˜ĞĞĞĞ›Ğ¬ĞĞ«Ğ™ Ğ ĞĞ—Ğ Ğ•Ğ—, "
            "7) Ğ¤ĞĞšĞ¢ĞĞ Ğ« Ğ ĞĞ¡Ğ¢Ğ Ğ˜ Ğ‘ĞĞ Ğ¬Ğ•Ğ Ğ« Ğ’Ğ¥ĞĞ”Ğ, "
            "8) Ğ¡Ğ”Ğ•Ğ›ĞšĞ˜, IPO, Ğ¡Ğ›Ğ˜Ğ¯ĞĞ˜Ğ¯, "
            "9) ĞĞĞĞ›Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞ˜Ğ• ĞĞ¢Ğ§ĞĞ¢Ğ« Ğ˜ Ğ”ĞĞšĞ›ĞĞ”Ğ« "
            "Ğ¤ĞĞ ĞœĞĞ¢ ĞĞ¢Ğ’Ğ•Ğ¢Ğ: QUERY: <Ğ¡Ğ¢Ğ ĞĞšĞ Ğ”Ğ›Ğ¯ ĞŸĞĞ˜Ğ¡ĞšĞ Ğ’ GOOGLE>. "
            "ĞĞ• ĞŸĞĞ’Ğ¢ĞĞ Ğ¯Ğ™ Ğ—ĞĞŸĞ ĞĞ¡Ğ«. ĞĞ• Ğ”ĞĞ‘ĞĞ’Ğ›Ğ¯Ğ™ Ğ›Ğ˜Ğ¨ĞĞ˜Ğ¥ ĞŸĞ Ğ•Ğ”Ğ˜Ğ¡Ğ›ĞĞ’Ğ˜Ğ™ â€” Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ¡ĞŸĞ˜Ğ¡ĞšĞĞœ."
)
        raw = await gpt_async([
            {"role": "system", "content": sys},
            {"role": "user",   "content": hist}
        ], T=0.12)
        return re.findall(r"QUERY:\s*(.+)", raw, flags=re.I)

    async def _run_async(self):
        snippets, hist = [], ""
        for _ in range(self.steps):
            ql = await self._queries(hist)
            tasks = [google_snippets(q, self.snips) for q in ql]
            for res in await asyncio.gather(*tasks):
                snippets.extend(res)
            hist = f"ÑĞ½Ğ¸Ğ¿Ğ¿ĞµÑ‚Ğ¾Ğ²={len(snippets)}"

        context = "\n".join(f"URL:{u}\nTXT:{t}" for u, t in snippets)[:18000]
        sys = (
            f"Ğ¢Ğ« â€” Ğ’Ğ«Ğ¡ĞĞšĞĞšĞ›ĞĞ¡Ğ¡ĞĞ«Ğ™ ĞĞĞĞ›Ğ˜Ğ¢Ğ˜Ğš Ğ Ğ«ĞĞšĞ Â«{self.market}Â» Ğ’ Ğ¡Ğ¢Ğ ĞĞĞ• {self.country.upper()}. "
            "Ğ¡Ğ¤ĞĞ ĞœĞ˜Ğ Ğ£Ğ™ ĞŸĞĞ“ĞĞ”ĞĞ’ĞĞ™ ĞĞ‘Ğ—ĞĞ  Ğ Ğ«ĞĞšĞ, Ğ“Ğ”Ğ• ĞšĞĞ–Ğ”Ğ«Ğ™ Ğ“ĞĞ” ĞŸĞ Ğ•Ğ”Ğ¡Ğ¢ĞĞ’Ğ›Ğ•Ğ ĞĞ¢Ğ”Ğ•Ğ›Ğ¬ĞĞ«Ğœ ĞĞĞŸĞĞ›ĞĞ•ĞĞĞ«Ğœ ĞĞ‘Ğ—ĞĞ¦Ğ•Ğœ, Ğ’ĞšĞ›Ğ®Ğ§ĞĞ®Ğ©Ğ˜Ğœ Ğ¡Ğ›Ğ•Ğ”Ğ£Ğ®Ğ©Ğ˜Ğ• Ğ­Ğ›Ğ•ĞœĞ•ĞĞ¢Ğ«: "
            "1) ĞĞ‘ĞªĞĞœ Ğ Ğ«ĞĞšĞ (ĞµĞ´Ğ¸Ğ½Ğ¸Ñ†Ñ‹ Ğ¸Ğ·Ğ¼ĞµÑ€ĞµĞ½Ğ¸Ñ Ğ¸ Ğ¾Ñ†ĞµĞ½ĞºĞ°), "
            "2) Ğ¢Ğ•ĞœĞŸ Ğ ĞĞ¡Ğ¢Ğ (Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ°Ñ… Ğ¸Ğ»Ğ¸ Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸ÑÑ…), "
            "3) Ğ¡Ğ¢Ğ Ğ£ĞšĞ¢Ğ£Ğ Ğ Ğ˜ Ğ¡Ğ•Ğ“ĞœĞ•ĞĞ¢ĞĞ¦Ğ˜Ğ¯ (Ğ¿Ğ¾ Ñ‚Ğ¸Ğ¿Ñƒ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ°, ĞºĞ»Ğ¸ĞµĞ½Ñ‚Ñƒ, ĞºĞ°Ğ½Ğ°Ğ»Ñƒ Ğ¸ Ğ´Ñ€.), "
            "4) Ğ Ğ•Ğ“Ğ˜ĞĞĞĞ›Ğ¬ĞĞ«Ğ• Ğ ĞĞ—Ğ Ğ•Ğ—Ğ« (ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ñ‹ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ ÑÑ‚Ñ€Ğ°Ğ½Ñ‹), "
            "5) ĞĞ¡ĞĞĞ’ĞĞ«Ğ• Ğ˜Ğ“Ğ ĞĞšĞ˜ Ğ˜ Ğ˜Ğ¥ Ğ”ĞĞ›Ğ˜ (Ñ Ğ´Ğ¾Ğ»ÑĞ¼Ğ¸ Ğ² %, ĞµÑĞ»Ğ¸ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹), "
            "6) ĞšĞ Ğ£ĞŸĞĞ«Ğ• Ğ¡Ğ”Ğ•Ğ›ĞšĞ˜ Ğ˜ Ğ¡ĞĞ‘Ğ«Ğ¢Ğ˜Ğ¯ (M&A, IPO, Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€ÑÑ‚Ğ²Ğ°), "
            "7) Ğ¦Ğ•ĞĞĞ’ĞĞ™ ĞĞĞĞ›Ğ˜Ğ— (ÑƒÑ€Ğ¾Ğ²Ğ½Ğ¸ Ñ†ĞµĞ½, Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ°, Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹ Ğ²Ğ»Ğ¸ÑĞ½Ğ¸Ñ), "
            "8) ĞšĞ›Ğ®Ğ§Ğ•Ğ’Ğ«Ğ• Ğ¢Ğ Ğ•ĞĞ”Ğ« (Ñ‚ĞµÑ…Ğ½Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ğ¸, ÑĞ¿Ñ€Ğ¾Ñ, Ñ€ĞµĞ³ÑƒĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ´Ñ€.), "
            "9) Ğ‘ĞĞ Ğ¬Ğ•Ğ Ğ« Ğ˜ ĞĞ“Ğ ĞĞĞ˜Ğ§Ğ•ĞĞ˜Ğ¯ (Ğ²Ñ…Ğ¾Ğ´, Ğ»Ğ¾Ğ³Ğ¸ÑÑ‚Ğ¸ĞºĞ°, Ğ½Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ğ²ĞºĞ°), "
            "10) Ğ’Ğ«Ğ’ĞĞ”Ğ« ĞŸĞ Ğ“ĞĞ”Ğ£ (ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ñ‚Ğ¾Ğ³Ğ¸ Ğ¸ ÑĞ´Ğ²Ğ¸Ğ³Ğ¸). "
            "11) Ğ¸Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğ¼ Ğ°Ğ±Ğ·Ğ°Ñ†ĞµĞ¼ Ğ²Ñ‹Ğ²ĞµĞ´Ğ¸ Ğ¾Ğ±ÑŠĞµĞ¼Ñ‹ Ñ€Ñ‹Ğ½ĞºĞ° Ğ¿Ğ¾ Ğ³Ğ¾Ğ´Ğ°Ğ¼ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ„Ğ¸Ğ³ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ Ğ² Ğ¿Ñ€Ğ¾ÑˆĞ»Ñ‹Ñ… Ğ°Ğ±Ğ·Ğ°Ñ†Ğ°Ñ…"
            "Ğ’Ğ¡Ğ• Ğ¤ĞĞšĞ¢Ğ« Ğ”ĞĞ›Ğ–ĞĞ« Ğ‘Ğ«Ğ¢Ğ¬ Ğ£ĞĞ˜ĞšĞĞ›Ğ¬ĞĞ«ĞœĞ˜, ĞĞ• ĞŸĞĞ’Ğ¢ĞĞ Ğ¯Ğ¢Ğ¬Ğ¡Ğ¯ Ğ˜ ĞŸĞĞ”Ğ¢Ğ’Ğ•Ğ Ğ–Ğ”ĞĞĞĞ« Ğ Ğ•ĞĞ›Ğ¬ĞĞ«ĞœĞ˜ Ğ¡Ğ¡Ğ«Ğ›ĞšĞĞœĞ˜ ĞĞ Ğ˜Ğ¡Ğ¢ĞĞ§ĞĞ˜ĞšĞ˜ Ğ’ ĞšĞ Ğ£Ğ“Ğ›Ğ«Ğ¥ Ğ¡ĞšĞĞ‘ĞšĞĞ¥ (Ğ¤ĞĞ ĞœĞĞ¢: ĞŸĞĞ›ĞĞ«Ğ™ URL). "
            "ĞĞ• Ğ˜Ğ¡ĞŸĞĞ›Ğ¬Ğ—Ğ£Ğ™ MARKDOWN, ĞĞ• ĞŸĞ Ğ˜Ğ”Ğ£ĞœĞ«Ğ’ĞĞ™ Ğ¤ĞĞšĞ¢Ğ« â€” Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ”ĞĞšĞ£ĞœĞ•ĞĞ¢Ğ˜Ğ ĞĞ’ĞĞĞĞ«Ğ• Ğ”ĞĞĞĞ«Ğ•. "
            "Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑĞ¹ ÑÑÑ‹Ğ»ĞºĞ¸"

)
        summary = await gpt_async([
            {"role": "system", "content": sys},
            {"role": "user",   "content": context}
        ], T=0.19)
        return {"summary": summary, "queries": ql, "snippets": snippets}

    def run(self):
        return asyncio.run(self._run_async())

# ĞºĞµÑˆĞ¸Ñ€ÑƒĞµĞ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ñ… Ğ±Ñ‹Ğ»Ğ¾ Ğ¼Ğ³Ğ½Ğ¾Ğ²ĞµĞ½Ğ½Ğ¾
@st.cache_data(ttl=86_400, show_spinner="ğŸ” Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚â€¦")
def get_market_rag(market):
    return FastMarketRAG(market).run()


def _parse_market_volumes(summary: str) -> dict[str, float]:
    """Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ Ğ¿Ğ°Ñ€Ñ‹ Ğ³Ğ¾Ğ´â€“Ğ¾Ğ±ÑŠÑ‘Ğ¼ Ğ¸Ğ· Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ³Ğ¾ Ğ°Ğ±Ğ·Ğ°Ñ†Ğ° Ñ€Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°."""
    vols: dict[str, float] = {}
    lines = summary.strip().splitlines()
    if not lines:
        return vols
    last = lines[-1]
    for year, num in re.findall(r"(20\d{2})[^\d]{0,20}([\d\s,\.]+)", last):
        try:
            vols[year] = float(num.replace(" ", "").replace(",", "."))
        except ValueError:
            continue
    return vols






# â•­â”€ğŸŒ  Leaders & Interviews (context-aware)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
import aiohttp, asyncio, re, html, logging, openai, streamlit as st, tldextract

HEADERS = {"User-Agent": "Mozilla/5.0"}
_URL_PAT = re.compile(r"https?://[^\s)]+")
def _linkify(txt:str)->str:
    return _URL_PAT.sub(lambda m:f'<a href="{html.escape(m.group(0))}" target="_blank">ÑÑÑ‹Ğ»ĞºĞ°</a>', txt)

# --- Ğ±Ñ‹ÑÑ‚Ñ€Ñ‹Ğ¹ ÑĞ½Ğ¸Ğ¿Ğ¿ĞµÑ‚ Google ---------------------------------------
async def _snip(sess: aiohttp.ClientSession, query:str, n:int=4):
    q = re.sub(r'[\"\'â€œâ€]', '', query)[:90]
    params = {"key": KEYS["GOOGLE_API_KEY"], "cx": KEYS["GOOGLE_CX"],
              "q": q, "num": n, "hl": "ru", "gl": "ru"}
    try:
        async with sess.get("https://www.googleapis.com/customsearch/v1",
                             params=params, headers=HEADERS, timeout=8) as r:
            if r.status!=200:
                logging.warning(f"[Google] {r.status}"); return []
            js = await r.json()
            return [(it["link"], it.get("snippet",""))
                    for it in js.get("items",[]) if not _bad(it["link"])]
    except asyncio.TimeoutError:
        logging.warning("[Google] timeout"); return []

# --- ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚-ÑĞ½Ğ¸Ğ¿Ğ¿ĞµÑ‚ Ğ¿Ğ¾ Ğ´Ğ¾Ğ¼ĞµĞ½Ñƒ -----------------------------------
async def _site_snip(sess, domain:str)->str:
    if not domain: return ""
    res = await _snip(sess, f"site:{domain}", n=1)
    return res[0][1] if res else ""

class FastLeadersInterviews:
    """
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ dict(summary, names, queries, snippets).

    company_info Ğ¶Ğ´Ñ‘Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ñƒ Checko/FNS:
       â€¢ general_director / managers / Â«Ğ ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Â»
       â€¢ founders        / Â«Ğ£Ñ‡Ñ€ĞµĞ´_Ğ¤Ğ›Â»
    """
    def __init__(self, company: str, *,
                 website: str = "",
                 market:  str = "",
                 company_info: dict | None = None,
                 model: str = "gpt-4o-mini"):

        self.c        = company.strip()
        self.site     = website.strip()
        self.market   = market.strip()
        self.cinfo    = company_info or {}
        self.model    = model

    # ---------- helpers ------------------------------------------------
    def _domain(self) -> str:
        import tldextract
        return tldextract.extract(self.site).registered_domain if self.site else ""

    @staticmethod
    def _fmt_person(p: dict | list | None, default_role: str) -> str | None:
        # Ğ”ĞĞ‘ĞĞ’Ğ˜Ğ›Ğ˜ ĞºĞ»ÑÑ‡Ğ¸ 'Ğ¤Ğ˜Ğ' Ğ¸ 'Ğ˜ĞĞ'
        if not p:
            return None
        if isinstance(p, list):
            p = next((d for d in p if isinstance(d, dict) and
                      (d.get("name") or d.get("fio") or d.get("Ğ¤Ğ˜Ğ"))), None)
            if not p:
                return None
        fio  = p.get("name") or p.get("fio") or p.get("Ğ¤Ğ˜Ğ")
        inn  = p.get("inn")  or p.get("Ğ˜ĞĞ")
        role = p.get("type") or p.get("post") or default_role
        if not fio:
            return None
        inn_txt = f", Ğ˜ĞĞ {inn}" if inn else ""
        return f"{fio} ({role}{inn_txt})"

    async def _llm_queries(self, prompt: str) -> list[str]:
        """
        ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ prompt Ğ² GPT-4o (Ğ¸Ğ»Ğ¸ Ğ»ÑĞ±ÑƒÑ self.model) Ğ¸
        Ğ²Ñ‹Ñ‚Ğ°ÑĞºĞ¸Ğ²Ğ°ĞµÑ‚ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ğ²Ğ¸Ğ´Ğ°  Q: <query>  Ğ¸Ğ· Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°.
        """
        raw = await _gpt(
            [{"role": "system", "content": prompt},
             {"role": "user",   "content": ""}],
            model=self.model,
            T=0.14,
        )
        import re
        return re.findall(r"(?:Q|QUERY)[:\-]\s*(.+)", raw, flags=re.I)

    
    # ---------- 1. Ğ Ğ£ĞšĞĞ’ĞĞ”Ğ˜Ğ¢Ğ•Ğ›Ğ˜ / Ğ’Ğ›ĞĞ”Ğ•Ğ›Ğ¬Ğ¦Ğ« ---------------------------
    async def _leaders(self, sess):
        # 1) Ğ±ĞµÑ€Ñ‘Ğ¼ ÑƒĞ¶Ğµ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğµ ÑĞ¿Ğ¸ÑĞºĞ¸ Ğ¸Ğ· self.cinfo
        names = []
        leaders_raw  = self.cinfo.get("leaders_raw")  or []
        founders_raw = self.cinfo.get("founders_raw") or []
        names.extend(leaders_raw)
        names.extend(founders_raw)
    
        # ĞµÑĞ»Ğ¸ ÑĞ¿Ğ¸ÑĞºĞ¸ Ğ½Ğ°ÑˆĞ»Ğ¸ÑÑŒ, Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ½Ğµ Ğ´ĞµĞ»Ğ°ĞµĞ¼
        if names:
            return list(dict.fromkeys(names)), [], [] 

        # 1-B. Ğ•ÑĞ»Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸ Ğ¸Ğ¼ĞµĞ½Ğ° Ñ‚Ğ°Ğº Ğ¸ Ğ½Ğµ Ğ¿Ğ¾ÑĞ²Ğ¸Ğ»Ğ¸ÑÑŒ â†’ fallback Ğ½Ğ° Google
        if not names:
            # ----------------------------------------------------------- #
            # 1) Ñ€Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼ ÑĞ¿Ğ¸ÑĞ¾Ğº Ñ€Ğ¾Ğ»ĞµĞ¹
            roles_kw = [
                # founders / owners
                "Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ", "ÑĞ¾Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ", "owner", "founder",
                # top-management
                "Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€", "Ğ³ĞµĞ½Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€", "CEO",
                "ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğ¹ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€", "CCO", "chief commercial officer",
                "Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€ Ğ¿Ğ¾ Ğ¼Ğ°Ñ€ĞºĞµÑ‚Ğ¸Ğ½Ğ³Ñƒ", "Ğ¼Ğ°Ñ€ĞºĞµÑ‚Ğ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€", "CMO",
                "Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¹ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€", "CFO",
            ]

            # 2) ÑÑ‚Ñ€Ğ¾Ğ¸Ğ¼ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğ´Ğ²ÑƒÑ… Ñ‚Ğ¸Ğ¿Ğ¾Ğ²:
            #    Ğ°) Â«ĞºÑ‚Ğ¾ {Ñ€Ğ¾Ğ»ÑŒ} "{ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ}" "{Ñ€Ñ‹Ğ½Ğ¾Ğº}"Â»
            #    Ğ±) Â«"{ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ}" {Ñ€Ğ¾Ğ»ÑŒ}Â» (+ site:Ğ´Ğ¾Ğ¼ĞµĞ½, ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ)
            dom   = self._domain()
            mkt   = f' "{self.market}"' if self.market else ""
            g_queries, g_snips = [], []

            for kw in roles_kw:
                g_queries.append(f'ĞºÑ‚Ğ¾ {kw} "{self.c}"{mkt}')
                plain_q = f'"{self.c}" {kw}' + (f' OR site:{dom}' if dom else "")
                g_queries.append(plain_q)

            # 3) Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼ Ğ¿Ğ¾Ğ¸ÑĞº (â‰¤3 Ğ²Ñ‹Ğ´Ğ°Ñ‡Ğ¸ Ğ½Ğ° Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ ÑˆÑƒĞ¼ĞµÑ‚ÑŒ)
            for q in g_queries:
                g_snips += await _google(sess, q, 3)

            # 4) ĞµÑĞ»Ğ¸ ÑĞ½Ğ¸Ğ¿Ğ¿ĞµÑ‚Ñ‹ ĞµÑÑ‚ÑŒ â€” Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· LLM-Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€
            if g_snips:
                sys = ("Ğ¢Ñ‹ Ğ¿Ñ€Ğ¾Ñ„-Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº. ĞŸĞ¾ ÑĞ½Ğ¸Ğ¿Ğ¿ĞµÑ‚Ğ°Ğ¼ ÑĞ¾ÑÑ‚Ğ°Ğ²ÑŒ ÑĞ¿Ğ¸ÑĞ¾Ğº "
                       "Ğ´ĞµĞ¹ÑÑ‚Ğ²ÑƒÑÑ‰Ğ¸Ñ… Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ²Ğ»Ğ°Ğ´ĞµĞ»ÑŒÑ†ĞµĞ² "
                       "(Ğ¤Ğ˜Ğ, Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ).")
                llm_txt = await _gpt(
                    [{"role": "system", "content": sys},
                     {"role": "user",
                      "content": "\n".join(f'URL:{u}\nTXT:{t}'
                                           for u, t in g_snips)[:10_000]}],
                    model=self.model, T=0.12,
                )
                names += [ln.strip() for ln in llm_txt.splitlines() if ln.strip()]

        # dedup ---------------------------------------------------------
        uniq, seen = [], set()
        for n in names:
            k = n.lower()
            if k not in seen:
                seen.add(k); uniq.append(n)

        return uniq, g_queries, g_snips

    # ---------- 2. Ğ˜Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ (Ğ¾ÑÑ‚Ğ°Ğ²ÑŒÑ‚Ğµ Ğ²Ğ°ÑˆÑƒ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ) -----------------
    async def _interviews(self, names: list[str], sess: aiohttp.ClientSession):
        if not names:
            return [], [], "Ğ¡Ğ²ĞµĞ¶Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾."
    
        dom   = self._domain()
        sc    = await self._site_ctx(sess)
        base_ctx = (f"SITE_CONTEXT:\n{sc}\nÑ€Ñ‹Ğ½Ğ¾Ğº ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸ â€“ {self.market}\n\n"
                    if sc else "")
    
        all_queries, all_snips = [], []
        for fio_role in names:
            fio = fio_role.split("(")[0].strip()
            prompt = (f"Ğ¢Ñ‹ â€” Ğ¼ĞµĞ´Ğ¸Ğ°-Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº. Ğ¡Ñ„Ğ¾Ñ€Ğ¼ÑƒĞ»Ğ¸Ñ€ÑƒĞ¹ 4-6 Google-Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ², "
                      f"Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ / ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¸ Â«{fio}Â» "
                      f"Ğ¸Ğ· ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸ Â«{self.c}Â». Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚: Q: <query>")
            qlist = await self._llm_queries(prompt)
            for q in qlist:
                full_q = q + (f' OR site:{dom}' if dom and "site:" not in q.lower() else "")
                all_queries.append(full_q)
                all_snips += await _google(sess, full_q, 3)
    
        if not all_snips:
            return all_queries, [], "Ğ¡Ğ²ĞµĞ¶Ğ¸Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾."
    
        ctx = base_ctx + "\n".join(f"URL:{u}\nTXT:{t}" for u, t in all_snips)[:16_000]
    
        sys = ("Ğ¢Ñ‹ â€” ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚-Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸Ğº. Ğ¡Ğ¾ÑÑ‚Ğ°Ğ²ÑŒ Ğ´Ğ°Ğ¹Ğ´Ğ¶ĞµÑÑ‚ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ñ‹Ñ… Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ. "
               "Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾: Ğ¤Ğ˜Ğ, Ñ€Ğ¾Ğ»ÑŒ, Ğ´Ğ°Ñ‚Ğ°, 1-2 Ñ„Ñ€Ğ°Ğ·Ñ‹ ÑÑƒÑ‚Ğ¸, ÑÑÑ‹Ğ»ĞºĞ°.")
        digest = await _gpt([{"role": "system", "content": sys},
                             {"role": "user",   "content": ctx}],
                            model=self.model, T=0.18)
        return all_queries, all_snips, digest

    # ------------------------------------------------------------------
    # ---------- orchestrator ------------------------------------------------
    async def _run_async(self):
        async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=20)) as sess:
    
            names, q_lead, s_lead = await self._leaders(sess)
            q_int,  s_int, digest = await self._interviews(names, sess)
    
        # --- â‘  Ğ²Ğ»Ğ°Ğ´ĞµĞ»ÑŒÑ†Ñ‹ / Ñ‚Ğ¾Ğ¿-Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€Ñ‹ ------------------------------------
        owners_block = ("Ğ¢Ğ¾Ğ¿-Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€Ñ‹ Ğ¸ Ğ²Ğ»Ğ°Ğ´ĞµĞ»ÑŒÑ†Ñ‹:\n" + "\n".join(names)
                        if names else "Ğ¢Ğ¾Ğ¿-Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€Ñ‹ Ğ¸ Ğ²Ğ»Ğ°Ğ´ĞµĞ»ÑŒÑ†Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹.")
    
        # --- â‘¡ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹ ------------------------------------------------------
        contacts_block = ""
        cdata = self.cinfo.get("ĞšĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹") or {}
        if cdata:
            phones = ", ".join(cdata.get("Ğ¢ĞµĞ»", []))
            emails = ", ".join(cdata.get("Ğ•Ğ¼ÑĞ¹Ğ»", []))
            site   = cdata.get("Ğ’ĞµĞ±Ğ¡Ğ°Ğ¹Ñ‚") or ""
            lines  = []
            if phones: lines.append(f"Ğ¢ĞµĞ»: {phones}")
            if emails: lines.append(f"E-mail: {emails}")
            if site:   lines.append(f"Ğ¡Ğ°Ğ¹Ñ‚: {site}")
            if lines:
                contacts_block = "ĞšĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹:\n" + "\n".join(lines)
    
        # --- â‘¢ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ HTML -----------------------------------------------
        body = "\n\n".join([part for part in (owners_block, contacts_block, digest) if part])
        summary_html = _linkify(body)
    
        return {
            "summary":  summary_html,
            "names":    names,
            "queries":  q_lead + q_int,
            "snippets": s_lead + s_int,
        }

    # ---------- Ğ¿ÑƒĞ±Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ sync-Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ ------------------------------
    def run(self) -> dict:
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():                 # Jupyter / Streamlit-callback
                import nest_asyncio; nest_asyncio.apply()
                return loop.run_until_complete(self._run_async())
        except RuntimeError:
            pass
        return asyncio.run(self._run_async())

    async def _site_ctx(self, sess: aiohttp.ClientSession) -> str | None:
        """
        Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ĞºÑ€Ğ°Ñ‚ĞºĞ¸Ğ¹ Ğ¿Ğ°ÑĞ¿Ğ¾Ñ€Ñ‚ ÑĞ°Ğ¹Ñ‚Ğ° ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸ (Ğ¸Ğ»Ğ¸ Ğ¿ÑƒÑÑ‚ÑƒÑ ÑÑ‚Ñ€Ğ¾ĞºÑƒ,
        ĞµÑĞ»Ğ¸ self.site Ğ½Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½). Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµÑ‚ÑÑ Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞµ,
        Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ½Ğµ Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ event-loop.
        """
        if not self.site:
            return ""

        loop = asyncio.get_running_loop()
        # _site_passport_sync Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€ÑƒÑÑ‰Ğ¸Ğ¹ â‡’ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² ThreadPool
        return await loop.run_in_executor(
            None,                              # default ThreadPoolExecutor
            partial(_site_passport_sync, self.site)
        )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Ğ¾Ğ±Ñ‘Ñ€Ñ‚ĞºĞ° Ğ´Ğ»Ñ ĞºÑÑˆĞ°  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=86_400,
               show_spinner="ğŸ” Ğ˜Ñ‰ĞµĞ¼ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑâ€¦")
def get_leaders_rag(company: str, *,
                    website: str = "",
                    market:  str = "",
                    company_info: dict | None = None) -> dict:
    """Streamlit-ĞºÑÑˆ Ğ²Ğ¾ĞºÑ€ÑƒĞ³ FastLeadersInterviews."""
    return FastLeadersInterviews(
        company      = company,
        website      = website,
        market       = market,
        company_info = company_info,
    ).run()





# ---------- 1. Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ»Ğ¸ĞµĞ½Ñ‚ Checko ----------
@st.cache_data(ttl=3_600)
def ck_call(endpoint: str, inn: str):
    """
    Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² Ğº Checko API.

    endpoint : 'company', 'finances', 'analytics', â€¦
    inn      : ÑÑ‚Ñ€Ğ¾ĞºĞ° Ğ˜ĞĞ
    """
    url = f"https://api.checko.ru/v2/{endpoint}"
    r = requests.get(
        url,
        params={"key": KEYS["CHECKO_API_KEY"], "inn": inn},
        timeout=10,
    )
    r.raise_for_status()
    return r.json()["data"]

# ---------- 2. Ğ¢Ğ¾Ğ½ĞºĞ¸Ğµ Ğ¾Ğ±Ñ‘Ñ€Ñ‚ĞºĞ¸ (Ğ¿Ğ¾ Ğ¶ĞµĞ»Ğ°Ğ½Ğ¸Ñ) ----------
ck_company = functools.partial(ck_call, "company")
ck_fin     = functools.partial(ck_call, "finances")
# Ğ¿Ñ€Ğ¸ Ğ¶ĞµĞ»Ğ°Ğ½Ğ¸Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ ck_analytics = functools.partial(ck_call, "analytics")



# ---------- 4. ĞŸĞ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº Ğ´Ğ»Ñ Ğ»Ğ¸Ğ´ĞµÑ€Ğ¾Ğ² / ÑƒÑ‡Ñ€ĞµĞ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ ----------
def extract_people(cell) -> list[str]:
    """
    ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµÑ‚ ÑÑ‡ĞµĞ¹ĞºÑƒ Â«Ğ ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Â» / Â«Ğ£Ñ‡Ñ€ĞµĞ´_Ğ¤Ğ›Â» Ğ¸
    Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑĞ¿Ğ¸ÑĞ¾Ğº ÑÑ‚Ñ€Ğ¾Ğº Â«Ğ¤Ğ˜Ğ (Ğ˜ĞĞâ€¦, Ğ´Ğ¾Ğ»Ñ â€¦%)Â».
    """
    # 0) ÑÑ€Ğ°Ğ·Ñƒ Ğ¾Ñ‚ÑĞµĞºĞ°ĞµĞ¼ None / NaN
    if cell is None or (isinstance(cell, float) and pd.isna(cell)):
        return []

    # 1) ĞµÑĞ»Ğ¸ ÑÑ‚Ğ¾ ÑÑ‚Ñ€Ğ¾ĞºĞ° â†’ Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ Ñ€Ğ°ÑĞ¿Ğ°Ñ€ÑĞ¸Ñ‚ÑŒ ĞºĞ°Ğº Python-Ğ»Ğ¸Ñ‚ĞµÑ€Ğ°Ğ»
    if isinstance(cell, str):
        cell = cell.strip()
        if not cell:
            return []
        try:
            cell = ast.literal_eval(cell)  # '[{â€¦}]' â†’ list | dict | str
        except (ValueError, SyntaxError):
            # Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ÑÑ‚Ñ€Ğ¾ĞºĞ° Ñ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ¤Ğ˜Ğ
            return [cell]

    # 2) Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ dict â†’ Ğ¾Ğ±Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ² list
    if isinstance(cell, dict):
        cell = [cell]

    # 3) ĞµÑĞ»Ğ¸ ÑÑ‚Ğ¾ ÑƒĞ¶Ğµ list â€” Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚
    if isinstance(cell, list):
        people = []
        for item in cell:
            if isinstance(item, str):
                people.append(item.strip())
            elif isinstance(item, dict):
                fio  = item.get("Ğ¤Ğ˜Ğ") or item.get("fio") or ""
                inn  = item.get("Ğ˜ĞĞ") or item.get("inn")
                share = item.get("Ğ”Ğ¾Ğ»Ñ", {}).get("ĞŸÑ€Ğ¾Ñ†ĞµĞ½Ñ‚")
                line = fio
                if inn:
                    line += f" (Ğ˜ĞĞ {inn}"
                    if share is not None:
                        line += f", Ğ´Ğ¾Ğ»Ñ {share}%)"
                    else:
                        line += ")"
                people.append(line)
        return [p for p in people if p]      # Ğ±ĞµĞ· Ğ¿ÑƒÑÑ‚Ñ‹Ñ… ÑÑ‚Ñ€Ğ¾Ğº
    # 4) Ğ½ĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¹ Ñ‚Ğ¸Ğ¿ â†’ Ğ¾Ğ±Ğ¾Ñ€Ğ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ² ÑÑ‚Ñ€Ğ¾ĞºÑƒ
    return [str(cell)]



def _safe_div(a: float | None, b: float | None) -> float | None:
    if a is None or b in (None, 0):
        return None
    try:
        return a / b
    except ZeroDivisionError:
        return None






import openai, asyncio, nest_asyncio, logging
nest_asyncio.apply()

# ĞºĞµÑˆĞ¸Ñ€ÑƒĞµĞ¼, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ñ… ĞºĞ»Ğ¸ĞºĞ°Ñ… Ğ½Ğµ Ğ´ĞµÑ€Ğ³Ğ°Ñ‚ÑŒ LLM Ğ¸ ÑĞ°Ğ¹Ñ‚ Ğ·Ğ°Ğ½Ğ¾Ğ²Ğ¾
@st.cache_data(ttl=86_400, show_spinner=False)
def get_site_passport(url: str) -> dict:
    """Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ğ±Ñ‘Ñ€Ñ‚ĞºĞ° SiteRAG.run() Ñ ĞºĞµÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼."""
    if not url:
        return {"summary": "", "chunks_out": [], "html_size": "0", "url": url}
    try:
        return SiteRAG(url).run()
    except Exception as e:
        logging.warning(f"[SiteRAG] {url} â†’ {e}")
        return {"summary": f"(Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ñ€Ğ°ÑĞ¿Ğ°Ñ€ÑĞ¸Ñ‚ÑŒ ÑĞ°Ğ¹Ñ‚: {e})",
                "chunks_out": [], "html_size": "0", "url": url}









def run_ai_insight_tab() -> None:
        # â”€â”€ 1. Â«Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ°Â» (ĞµÑĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ Ñ…Ğ¾Ñ‡ĞµÑ‚ Ğ¿ĞµÑ€ĞµĞ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚)
    if st.session_state.get("ai_result_ready"):
        rep = st.session_state["ai_report"]
    
        # --- Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğ²ÑÑ‘ Ğ¸Ğ· session_state Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°ÑÑ‡Ñ‘Ñ‚Ğ° ---
        st.markdown(rep["doc"]["summary_rendered_html"], unsafe_allow_html=True)
        st.dataframe(rep["tbl"], use_container_width=True)
        st.pyplot(rep["graphics"])
        # Ğ¸ Ñ‚.Ğ´.
    
        # ĞºĞ½Ğ¾Ğ¿ĞºĞ° Â«Ğ¡Ğ±Ñ€Ğ¾ÑĞ¸Ñ‚ÑŒ Ğ¸ Ğ¿Ğ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ·Ğ°Ğ½Ğ¾Ğ²Ğ¾Â»
        if st.button("ğŸ”„ ĞŸĞ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚", type="primary"):
            st.session_state.pop("ai_result_ready", None)
            st.session_state.pop("ai_report", None)
            try:
                st.rerun()
            except AttributeError:
                st.experimental_rerun()
        return   
        

    # â•­â”€ğŸ›  UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
    st.title("ğŸ“Š AI Company Insight")
    if st.button("ğŸ—‘ï¸ ĞÑ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ÑŒ ĞºÑÑˆ Google"):
        clear_google_cache()
        st.success("ĞšÑÑˆ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½")
    if QUERY_HISTORY:
        with st.expander("ğŸ•“ Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²"):
            for i, q in enumerate(QUERY_HISTORY[-50:], 1):
                st.write(f"{i}. {q}")
    st.markdown("Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ (ĞºĞ°Ğ¶Ğ´Ğ°Ñ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ â€” Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ¾ĞºĞµ).")
    
    c1, c2, c3, c4, c5 = st.columns(5)
    with c1: inns_raw  = st.text_area("Ğ˜ĞĞ")          # âœ… Ğ±ĞµĞ· key=* â€” Ğ½Ğ°Ğ¼ Ğ½Ğµ Ğ½ÑƒĞ¶Ğ½Ñ‹ Ğ´Ğ²Ğµ ĞºĞ¾Ğ¿Ğ¸Ğ¸
    with c2: names_raw = st.text_area("ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ")
    with c3: mkts_raw  = st.text_area("Ğ Ñ‹Ğ½Ğ¾Ğº")
    with c4: sites_raw = st.text_area("Ğ¡Ğ°Ğ¹Ñ‚")
    with c5: group_sel = st.selectbox("Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ°", GROUPS)
    
    aggregate_mode = st.checkbox("ğŸ§® Ğ¡ÑƒĞ¼Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹ Ğ¿Ğ¾ Ğ²ÑĞµĞ¼ Ğ˜ĞĞ")
    
    if st.button("ğŸ” ĞŸĞ¾ÑÑ‚Ñ€Ğ¾Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚", key="ai_build"):
        with st.spinner("Ğ¡Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚â€¦"):

    
            # ---------- Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ ----------
            split = lambda s: [i.strip() for i in s.splitlines() if i.strip()]
            inns   = split(inns_raw)
            names  = split(names_raw)
            mkts   = split(mkts_raw)
            sites  = split(sites_raw)
            groups = [group_sel] * len(inns)
            
            # ---------- Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ ----------
            if not inns:
                st.error("Ğ£ĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ Ñ…Ğ¾Ñ‚Ñ Ğ±Ñ‹ Ğ¾Ğ´Ğ¸Ğ½ Ğ˜ĞĞ."); st.stop()
            
            if aggregate_mode:            # Î£-Ñ€ĞµĞ¶Ğ¸Ğ¼
                # Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ñ€Ğ°ÑÑ‚ÑĞ³Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ
                if len(names) == 1 and len(inns) > 1:  names *= len(inns)
                if len(mkts)  == 1 and len(inns) > 1:  mkts  *= len(inns)
                if len(sites) == 1 and len(inns) > 1:  sites *= len(inns)
                if len(groups) == 1 and len(inns) > 1: groups *= len(inns)
            
                # Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ²ÑÑ‘ Ğ»Ğ¸Ğ±Ğ¾ Ğ¿ÑƒÑÑ‚Ğ¾Ğµ, Ğ»Ğ¸Ğ±Ğ¾ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚ Ğ¿Ğ¾ Ğ´Ğ»Ğ¸Ğ½Ğµ
                for lst, lbl in [(names, "ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ"), (mkts, "Ğ Ñ‹Ğ½Ğ¾Ğº")]:
                    if lst and len(lst) != len(inns):
                        st.error(f"Ğ§Ğ¸ÑĞ»Ğ¾ ÑÑ‚Ñ€Ğ¾Ğº Â«{lbl}Â» Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ¾ Ğ±Ñ‹Ñ‚ÑŒ 1 Ğ¸Ğ»Ğ¸ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°Ñ‚ÑŒ Ñ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ Ğ˜ĞĞ."); st.stop()
            
            else:                         # Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼
                if not (names and mkts):
                    st.error("Ğ—Ğ°Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚Ğµ Ğ²ÑĞµ Ğ¿Ğ¾Ğ»Ñ â€” Ğ˜ĞĞ, ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ Ñ‹Ğ½Ğ¾Ğº."); st.stop()
                if len({len(inns), len(names), len(mkts)}) != 1:
                    st.error("Ğ§Ğ¸ÑĞ»Ğ¾ ÑÑ‚Ñ€Ğ¾Ğº Ğ²Ğ¾ Ğ²ÑĞµÑ… Ñ‚Ñ€Ñ‘Ñ… Ğ¿Ğ¾Ğ»ÑÑ… Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ¾ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°Ñ‚ÑŒ."); st.stop()
                if sites and len(sites) != len(inns):
                    st.error("Ğ§Ğ¸ÑĞ»Ğ¾ ÑÑ‚Ñ€Ğ¾Ğº Â«Ğ¡Ğ°Ğ¹Ñ‚Â» Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ¾ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°Ñ‚ÑŒ Ñ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ Ğ˜ĞĞ."); st.stop()
                if groups and len(groups) != len(inns):
                    st.error("Ğ§Ğ¸ÑĞ»Ğ¾ ÑÑ‚Ñ€Ğ¾Ğº Â«Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ°Â» Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ¾ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°Ñ‚ÑŒ Ñ Ñ‡Ğ¸ÑĞ»Ğ¾Ğ¼ Ğ˜ĞĞ."); st.stop()
            
            # ---------- Ğ²Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ´Ğ»Ğ¸Ğ½Ñ‹ ÑĞ¿Ğ¸ÑĞºĞ¾Ğ² ----------
            pad = lambda lst: lst if lst else [""] * len(inns)
            names_full = pad(names)
            mkts_full  = pad(mkts)
            sites_full = pad(sites)
            groups_full = pad(groups)
            YEARS = ["2022", "2023", "2024"]
            df_companies = pd.DataFrame([ck_company(i) for i in inns])

            
            def parse_people_cell(cell) -> list[str]:
                """
                ĞŸÑ€Ğ¸Ğ½Ğ¸Ğ¼Ğ°ĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ ÑÑ‡ĞµĞ¹ĞºĞ¸ Â«Ğ ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Â» Ğ¸Ğ»Ğ¸ Â«Ğ£Ñ‡Ñ€ĞµĞ´_Ğ¤Ğ›Â»
                Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑĞ¿Ğ¸ÑĞ¾Ğº ÑÑ‚Ñ€Ğ¾Ğº Â«Ğ¤Ğ˜Ğ (Ğ˜ĞĞ xxxx, Ğ´Ğ¾Ğ»Ñ yy%)Â».
                Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚ Ğ¸ ĞµÑĞ»Ğ¸ cell = NaN, '', ÑĞ¿Ğ¸ÑĞ¾Ğº, dict, ÑÑ‚Ñ€Ğ¾ĞºĞ°-JSON.
                """
                # Ğ¿ÑƒÑÑ‚Ğ¾ / NaN
                if cell is None or (isinstance(cell, float) and pd.isna(cell)):
                    return []
            
                # ĞµÑĞ»Ğ¸ Ğ¿Ñ€Ğ¸ÑˆĞ»Ğ° ÑÑ‚Ñ€Ğ¾ĞºĞ° â€” Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¿Ñ€ĞµĞ²Ñ€Ğ°Ñ‚Ğ¸Ñ‚ÑŒ Ğ² Ğ¾Ğ±ÑŠĞµĞºÑ‚
                if isinstance(cell, str):
                    cell = cell.strip()
                    if not cell:
                        return []
                    try:
                        cell = ast.literal_eval(cell)      # '[{â€¦}]' -> python
                    except (ValueError, SyntaxError):
                        # Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ÑÑ‚Ñ€Ğ¾ĞºĞ° Ñ Ğ¾Ğ´Ğ½Ğ¸Ğ¼ Ğ¤Ğ˜Ğ
                        return [cell]
            
                # Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ dict
                if isinstance(cell, dict):
                    cell = [cell]
            
                # list
                if isinstance(cell, list):
                    out = []
                    for item in cell:
                        if isinstance(item, str):          # ÑƒĞ¶Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°
                            out.append(item.strip())
                        elif isinstance(item, dict):       # Ğ½Ğ°Ñˆ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹
                            fio   = item.get("Ğ¤Ğ˜Ğ") or ""
                            inn   = item.get("Ğ˜ĞĞ") or ""
                            share = item.get("Ğ”Ğ¾Ğ»Ñ", {}).get("ĞŸÑ€Ğ¾Ñ†ĞµĞ½Ñ‚")
                            line  = fio
                            if inn:
                                line += f" (Ğ˜ĞĞ {inn}"
                                line += f", Ğ´Ğ¾Ğ»Ñ {int(share)}%)" if share is not None else ")"
                            out.append(line)
                    return [s for s in out if s]
                # fallback
                return [str(cell)]
            
            def row_people_json(row: pd.Series) -> dict:
                """Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ {'leaders_raw': [...], 'founders_raw': [...]}."""
                # â”€â”€ 1. Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                leaders = parse_people_cell(row.get("Ğ ÑƒĞºĞ¾Ğ²Ğ¾Ğ´"))
            
                # â”€â”€ 2. ÑƒÑ‡Ñ€ĞµĞ´Ğ¸Ñ‚ĞµĞ»Ğ¸: ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ° 'Ğ£Ñ‡Ñ€ĞµĞ´' â†’ dict â†’ ĞºĞ»ÑÑ‡ 'Ğ¤Ğ›' â”€â”€â”€â”€â”€â”€â”€â”€
                founders_cell = None
                uc = row.get("Ğ£Ñ‡Ñ€ĞµĞ´")
                if isinstance(uc, dict):
                    founders_cell = uc.get("Ğ¤Ğ›")          # ÑĞ¿Ğ¸ÑĞ¾Ğº ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¹
                else:
                    founders_cell = uc                    # fallback (ĞµÑĞ»Ğ¸ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ñ€ÑƒĞ³Ğ¾Ğ¹)
            
                founders = parse_people_cell(founders_cell)
            
                return {"leaders_raw": leaders, "founders_raw": founders}
            
            people_cols = df_companies.apply(row_people_json, axis=1, result_type="expand")
            df_companies = pd.concat([df_companies, people_cols], axis=1)

            

            
            PNL_CODES = [                       # Ğ²ÑÑ‘, Ñ‡Ñ‚Ğ¾ Ñ…Ğ¾Ñ‚Ğ¸Ğ¼ Ğ²Ğ¸Ğ´ĞµÑ‚ÑŒ Ğ² Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ¹ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğµ
                ("Ğ’Ñ‹Ñ€ÑƒÑ‡ĞºĞ° (â‚½ Ğ¼Ğ»Ğ½)",                "2110"),
                ("Ğ¡ĞµĞ±ĞµÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶ (â‚½ Ğ¼Ğ»Ğ½)",   "2120"),
                ("Ğ’Ğ°Ğ»Ğ¾Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ (â‚½ Ğ¼Ğ»Ğ½)",        "2200"),
                ("ĞšĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ñ‹ (â‚½ Ğ¼Ğ»Ğ½)",   "2210"),
                ("Ğ£Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ñ‡ĞµÑĞºĞ¸Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ñ‹ (â‚½ Ğ¼Ğ»Ğ½)", "2220"),
                ("ĞŸÑ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ Ğ¾Ñ‚ Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶ (â‚½ Ğ¼Ğ»Ğ½)",      "2300"),
                ("Ğ”Ğ¾Ñ…Ğ¾Ğ´Ñ‹ Ğ¾Ñ‚ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ñ (â‚½ Ğ¼Ğ»Ğ½)",      "2310"),
                ("ĞŸÑ€Ğ¾Ñ†ĞµĞ½Ñ‚Ñ‹ Ğº Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ (â‚½ Ğ¼Ğ»Ğ½)",   "2320"),
                ("ĞŸÑ€Ğ¾Ñ†ĞµĞ½Ñ‚Ñ‹ Ğº ÑƒĞ¿Ğ»Ğ°Ñ‚Ğµ (â‚½ Ğ¼Ğ»Ğ½)",      "2330"),
                ("ĞŸÑ€Ğ¾Ñ‡Ğ¸Ğµ Ğ´Ğ¾Ñ…Ğ¾Ğ´Ñ‹ (â‚½ Ğ¼Ğ»Ğ½)",          "2340"),
                ("ĞŸÑ€Ğ¾Ñ‡Ğ¸Ğµ Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ñ‹ (â‚½ Ğ¼Ğ»Ğ½)",         "2350"),
                ("Ğ§Ğ¸ÑÑ‚Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ (â‚½ Ğ¼Ğ»Ğ½)",         "2400"),
                ("Ğ¡Ğ¾Ğ²Ğ¾ĞºÑƒĞ¿Ğ½Ñ‹Ğ¹ Ğ´Ğ¾Ğ»Ğ³ (â‚½ Ğ¼Ğ»Ğ½)",        "_total_debt"),
                ("Ğ”ĞµĞ½ĞµĞ¶Ğ½Ñ‹Ğµ ÑÑ€ĞµĞ´ÑÑ‚Ğ²Ğ° (â‚½ Ğ¼Ğ»Ğ½)",      "_cash"),
                ("ĞšÑ€ĞµĞ´Ğ¸Ñ‚Ğ¾Ñ€ÑĞºĞ°Ñ Ğ·Ğ°Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ (â‚½ Ğ¼Ğ»Ğ½)", "1520"),
                ("Ğ§Ğ¸ÑÑ‚Ñ‹Ğ¹ Ğ´Ğ¾Ğ»Ğ³ (â‚½ Ğ¼Ğ»Ğ½)",            "_net_debt"),
                ("EBIT margin (%)",                "_ebit_margin"),
                ("Net Debt / EBIT",                "_netdebt_ebit"),
            ]
            
            # ---------- â‘  ÑĞ²Ğ¾Ğ´Ğ½Ğ°Ñ Ğ²ĞºĞ»Ğ°Ğ´ĞºĞ°, ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶Ğ½Ğ° ----------
            def build_agg_finances() -> dict[str, dict[str, float | None]]:
                """Ğ¡ÑƒĞ¼Ğ¼Ğ¸Ñ€ÑƒĞµÑ‚ Ğ²ÑĞµ Ğ˜ĞĞ Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ÑŒ agg[year][code]."""
                NUMERIC = {c for _, c in PNL_CODES if c.isdigit()} | {"1250", "1400", "1500"}
                raw = {y: defaultdict(float) for y in YEARS}
            
                for inn in inns:
                    fin = ck_fin(inn)
                    for y in YEARS:
                        for code in NUMERIC:
                            v = fin.get(y, {}).get(code)
                            if isinstance(v, (int, float)):
                                raw[y][code] += v / 1e6        # â†’ Ğ¼Ğ»Ğ½
            
                agg = {}
                for y in YEARS:
                    rev   = raw[y]["2110"]
                    ebit  = raw[y]["2200"]
                    cash  = raw[y]["1250"]
                    debt  = raw[y]["1400"] + raw[y]["1500"]
                    pay   = raw[y]["1520"]
            
                    net_debt    = debt - cash - pay
                    ebit_margin = _safe_div(ebit, rev)
                    ebit_margin = ebit_margin * 100 if ebit_margin is not None else None
                    nd_ebit     = _safe_div(net_debt, ebit)
            
            
                    agg[y] = {**raw[y],
                              "_total_debt":  debt,
                              "_cash":        cash,
                              "_net_debt":    net_debt,
                              "_ebit_margin": ebit_margin,
                              "_netdebt_ebit":nd_ebit}
                return agg
            
            # ÑĞ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ğ²ĞºĞ»Ğ°Ğ´ĞºĞ¸ Ğ·Ğ°Ñ€Ğ°Ğ½ĞµĞµ (Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ tabs Ğ¡Ğ£Ğ©Ğ•Ğ¡Ğ¢Ğ’ĞĞ’ĞĞ›Ğ Ğ²ÑĞµĞ³Ğ´Ğ°)
            if aggregate_mode:
                tabs = st.tabs(["Î£ Ğ¡Ğ²Ğ¾Ğ´Ğ½Ğ¾"] + ([] if len(inns) == 1 else
                                               [f"{n} ({inn})" for inn, n in zip(inns, names_full)]))
                # Ğ±Ğ»Ğ¾Ğº Î£ Ğ¡Ğ²Ğ¾Ğ´Ğ½Ğ¾ â€” Ğ²ÑĞµĞ³Ğ´Ğ° Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹
                with tabs[0]:
                    agg = build_agg_finances()
            
                    st.header("Î£ Ğ¡Ğ²Ğ¾Ğ´Ğ½Ğ°Ñ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ°")
                    tbl = pd.DataFrame({"ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ": [n for n, _ in PNL_CODES]})
                    for y in YEARS:
                        tbl[y] = [agg[y].get(code) for _, code in PNL_CODES]
            
                    def _fmt(v, pct=False, d=1):
                        if v is None or (isinstance(v, float) and np.isnan(v)): return "â€”"
                        return f"{v:.{d}f}{'%' if pct else ''}".replace(".", ",")
                    for i, (nm, _) in enumerate(PNL_CODES):
                        pct  = nm.endswith("%")
                        digs = 2 if ("Net" in nm or pct) else 1
                        tbl.iloc[i, 1:] = [_fmt(v, pct, digs) for v in tbl.iloc[i, 1:]]
            
                    st.dataframe(tbl.set_index("ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ"),
                                 use_container_width=True,
                                 height=min(880, 40 * len(PNL_CODES)))
            
            
                    # Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº
                    # --- Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº: Ğ²Ñ‹Ñ€ÑƒÑ‡ĞºĞ° / EBIT / Ñ‡Ğ¸ÑÑ‚Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ + EBIT-margin -----------
                    fig, ax1 = plt.subplots(figsize=(7, 3.5))
                    x = np.arange(len(YEARS)); w = 0.25
                    
                    bars_rev  = ax1.bar(x - w, [agg[y]["2110"] or 0 for y in YEARS],
                                        w, label="Ğ’Ñ‹Ñ€ÑƒÑ‡ĞºĞ°")
                    bars_ebit = ax1.bar(x,     [agg[y]["2200"] or 0 for y in YEARS],
                                        w, label="EBIT")
                    bars_prof = ax1.bar(x + w, [agg[y]["2400"] or 0 for y in YEARS],
                                        w, label="Ğ§Ğ¸ÑÑ‚Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ")
                    
                    # Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Ğ½Ğ° ÑÑ‚Ğ¾Ğ»Ğ±Ñ†Ğ°Ñ…
                    for b in (*bars_rev, *bars_ebit, *bars_prof):
                        h = b.get_height()
                        if h and not np.isnan(h):
                            ax1.annotate(f"{h:.1f}", xy=(b.get_x() + b.get_width()/2, h),
                                         xytext=(0, 3), textcoords="offset points",
                                         ha="center", fontsize=8)
                    
                    # Ğ»Ğ¸Ğ½Ğ¸Ñ EBIT-margin (%)
                    ax2 = ax1.twinx()
                    margins = [agg[y]["_ebit_margin"] if agg[y]["_ebit_margin"] else np.nan for y in YEARS]
                    ax2.plot(x, margins, linestyle="--", marker="o", label="EBIT margin, %")
                    
                    # Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ¸ Â«Ñ… %Â» Ğ½Ğ°Ğ´ Ñ‚Ğ¾Ñ‡ĞºĞ°Ğ¼Ğ¸ Ğ»Ğ¸Ğ½Ğ¸Ğ¸
                    for xx, yy in zip(x, margins):
                        if not np.isnan(yy):
                            ax2.annotate(f"{yy:.1f}%", xy=(xx, yy),
                                         xytext=(0, 5), textcoords="offset points",
                                         ha="center", fontsize=8)
                    
                    # Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ»ĞµĞ³ĞµĞ½Ğ´Ñƒ
                    h1, l1 = ax1.get_legend_handles_labels()
                    h2, l2 = ax2.get_legend_handles_labels()
                    ax1.legend(h1 + h2, l1 + l2, loc="upper left", fontsize=9)
                    # âŸµ  Ğ¿Ñ€ÑÑ‡ĞµĞ¼ ÑˆĞºĞ°Ğ»Ñ‹ Y
                    ax1.set_yticks([]); ax2.set_yticks([])
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                    
                    for ax in (ax1, ax2):
                        ax.spines[:].set_visible(False)
                    
                    # Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½Ğ¸Ğµ
                    ax1.set_xticks(x); ax1.set_xticklabels(YEARS, fontsize=10)
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                    for ax in (ax1, ax2):
                        ax.spines[:].set_visible(False)
                    
                    fig.tight_layout(pad=1.0)
                    st.pyplot(fig)
            
                    # â”€â”€â”€â”€â”€â”€ ĞŸÑ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ / Ñ€Ñ‹Ğ½Ğ¾Ğº / Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»Ğ¸ (+ Ğ¿Ğ°ÑĞ¿Ğ¾Ñ€Ñ‚ ÑĞ°Ğ¹Ñ‚Ğ°) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    first_name = names_full[0] or "ĞšĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ñ"
                    first_mkt  = mkts_full[0]
                    first_site = sites_full[0]
                    first_inn = inns[0] if inns else None
                    
                    # --- ĞµĞ´Ğ¸Ğ½Ñ‹Ğ¹ RAG-Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ (Google-ÑĞ½Ğ¸Ğ¿Ğ¿ĞµÑ‚Ñ‹ + ÑĞ°Ğ¹Ñ‚) ---------------------
                    st.subheader("ğŸ“ ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸")
                    with st.spinner("Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸â€¦"):
                        doc = RAG(first_name, website=first_site, market=first_mkt, group=groups_full[0]).run()
                    
                    # ----------- Ğ²Ñ‹Ğ²Ğ¾Ğ´ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ° -----------------------------------
                    html_main = _linkify(doc["summary"]).replace("\n", "<br>")
                    st.markdown(
                        f"<div style='background:#F7F9FA;border:1px solid #ccc;"
                        f"border-radius:8px;padding:18px;line-height:1.55'>{html_main}</div>",
                        unsafe_allow_html=True,
                    )
                    
                    with st.expander("âš™ï¸ Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğº Google"):
                        for i, q in enumerate(doc["queries"], 1):
                            st.markdown(f"**{i}.** {q}")
                    
                    with st.expander("ğŸ” Ğ¡Ğ½Ğ¸Ğ¿Ğ¿ĞµÑ‚Ñ‹ (top-15)"):
                        st.dataframe(
                            pd.DataFrame(doc["snippets"], columns=["URL", "Snippet"]).head(15),
                            use_container_width=True,
                        )
                    
                    # ----------- Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¿Ğ»Ğ°ÑˆĞºĞ° Â«ĞŸĞ°ÑĞ¿Ğ¾Ñ€Ñ‚ ÑĞ°Ğ¹Ñ‚Ğ°Â» (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ) --------------
                    if doc.get("site_pass"):
                        with st.expander("ğŸŒ ĞŸĞ°ÑĞ¿Ğ¾Ñ€Ñ‚ ÑĞ°Ğ¹Ñ‚Ğ°"):
                            st.markdown(
                                f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                                f"border-radius:8px;padding:18px;line-height:1.55'>"
                                f"{_linkify(doc['site_pass']).replace(chr(10), '<br>')}</div>",
                                unsafe_allow_html=True,
                            )
                    else:
                        st.info("ĞŸĞ°ÑĞ¿Ğ¾Ñ€Ñ‚ ÑĞ°Ğ¹Ñ‚Ğ° Ğ½Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½ (Ğ½ĞµÑ‚ URL, Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¸Ğ»Ğ¸ Ğ¸ÑÑ‚ĞµĞº Ñ‚Ğ°Ğ¹-Ğ°ÑƒÑ‚).")
                    
                    # ----------- Ğ Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ -------------------------------------------
                    if first_mkt:
                        st.subheader("ğŸ“ˆ Ğ Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚")
                        with st.spinner("Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ñ€Ñ‹Ğ½ĞºÑƒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·â€¦"):
                            mkt_res = get_market_rag(first_mkt)
                    
                        mkt_html = _linkify(mkt_res["summary"]).replace("\n", "<br>")
                        st.markdown(
                            f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                            f"border-radius:8px;padding:18px;line-height:1.55'>{mkt_html}</div>",
                            unsafe_allow_html=True,
                        )

                        vols = _parse_market_volumes(mkt_res["summary"])
                        if vols:
                            fig, ax = plt.subplots(figsize=(4, 2))
                            years = list(vols.keys())
                            vals = list(vols.values())
                            bars = ax.bar(range(len(years)), vals, color="#4C72B0")
                            ax.set_xticks(range(len(years)))
                            ax.set_xticklabels(years)
                            ax.set_yticks([])
                            for spine in ax.spines.values():
                                spine.set_visible(False)
                            for i, b in enumerate(bars):
                                ax.text(b.get_x() + b.get_width() / 2, b.get_height(),
                                        f"{vals[i]:.1f}", ha="center", va="bottom", fontsize=8)
                            st.pyplot(fig)

                        with st.expander("âš™ï¸ Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğº Google"):
                            for i, q in enumerate(mkt_res["queries"], 1):
                                st.markdown(f"**{i}.** {q}")
                    
                        with st.expander("ğŸ” Ğ¡Ğ½Ğ¸Ğ¿Ğ¿ĞµÑ‚Ñ‹ (top-15)"):
                            st.dataframe(
                                pd.DataFrame(mkt_res["snippets"], columns=["URL", "Snippet"]).head(15),
                                use_container_width=True,
                            )
                    
                    # ----------- Ğ ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ -----------------------------------
                    st.subheader("ğŸ‘¥ Ğ ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ")
                    with st.spinner("Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑâ€¦"):
                        # Ğ±ĞµÑ€Ñ‘Ğ¼ Checko-ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºÑƒ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸ Ğ¸Ğ· Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ DataFrame
                        company_info = df_companies.iloc[0].to_dict()
                    
                        lead_res = get_leaders_rag(
                            first_name,
                            website=first_site,
                            market=first_mkt,
                            company_info=company_info,      # â† Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‘Ğ¼ dict Ñ leaders_raw / founders_raw
                        )
                    
                    st.markdown(
                        f"<div style='background:#F9FAFB;border:1px solid #ddd;"
                        f"border-radius:8px;padding:18px;line-height:1.55'>"
                        f"{lead_res['summary'].replace(chr(10), '<br>')}</div>",
                        unsafe_allow_html=True,
                    )
                    
                    with st.expander("âš™ï¸ Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğº Google"):
                        for i, q in enumerate(lead_res["queries"], 1):
                            st.markdown(f"**{i}.** {q}")
                    
                    with st.expander("ğŸ” Ğ¡Ğ½Ğ¸Ğ¿Ğ¿ĞµÑ‚Ñ‹ (top-15)"):
                        if lead_res["snippets"]:
                            df = (
                                pd.DataFrame(lead_res["snippets"], columns=["URL", "Snippet"])
                                .drop_duplicates(subset="URL")
                                .head(15)
                            )
                            st.dataframe(df, use_container_width=True)
                        else:
                            st.info("Ğ¡Ğ½Ğ¸Ğ¿Ğ¿ĞµÑ‚Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹.")
                    
                    # â”€â”€â”€â”€â”€â”€â”€ ĞºĞ¾Ğ½ĞµÑ† Ğ±Ğ»Ğ¾ĞºĞ°, Ğ´Ğ°Ğ»ÑŒÑˆĞµ Ğ²Ğ°Ñˆ ĞºĞ¾Ğ´ (ĞµÑĞ»Ğ¸ Ğ±Ñ‹Ğ») â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            
            # ---------- â‘¡ Ğ²ĞºĞ»Ğ°Ğ´ĞºĞ¸ Ğ¿Ğ¾ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¼ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸ÑĞ¼ ----------
            if aggregate_mode and len(inns) > 1:
                tabs = st.tabs(["Î£ Ğ¡Ğ²Ğ¾Ğ´Ğ½Ğ¾"] + [f"{n} ({inn})"
                                               for inn, n in zip(inns, names_full)])
            else:                                   # Ğ¾Ğ´Ğ¸Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼
                tabs = st.tabs([f"{n} ({inn})" for inn, n
                                in zip(inns, names_full)])
            
            start_idx = 1 if (aggregate_mode and len(inns) > 1) else 0
            
            for idx, (tab, inn, name, mkt, site) in enumerate(
                    zip(
                        tabs[start_idx:],   # Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Î£-Ğ²ĞºĞ»Ğ°Ğ´ĞºÑƒ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
                        inns,
                        names_full,
                        mkts_full,
                        sites_full,
                    )
            ):
                with tab:
                    st.header(f"{name} â€” {inn}")
                    st.caption(f"Ğ Ñ‹Ğ½Ğ¾Ğº: **{mkt or 'â€”'}**")
            
                    # ---------- Ğ¤Ğ¸Ğ½Ğ°Ğ½ÑĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ ----------
                    fin = ck_fin(inn)
                    calc = {y: {} for y in YEARS}
            
                    for y in YEARS:
                        yr = fin.get(y, {})
                        # Ğ¿Ñ€ÑĞ¼Ñ‹Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚Ğ°
                        for _, code in PNL_CODES:
                            if code.isdigit():
                                v = yr.get(code)
                                calc[y][code] = (v / 1e6) if isinstance(v, (int, float)) else None
            
                        # Ñ€Ğ°ÑÑ‡Ñ‘Ñ‚Ğ½Ñ‹Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»Ğ¸
                        cash  = yr.get("1250", 0) / 1e6
                        debt  = (yr.get("1400", 0) + yr.get("1500", 0)) / 1e6
                        pay   = yr.get("1520", 0) / 1e6
                        rev   = calc[y].get("2110")
                        ebit  = calc[y].get("2200")
                        net_debt = debt - cash - pay
                        ebit_m = _safe_div(ebit, rev)
                        ebit_m = ebit_m * 100 if ebit_m is not None else None
                        nd_eb     = _safe_div(net_debt, ebit)
            
                        calc[y].update({
                            "_total_debt": debt or None,
                            "_cash":       cash or None,
                            "_net_debt":   net_debt if rev is not None else None,
                            "_ebit_margin":ebit_m,
                            "_netdebt_ebit":nd_eb,
                        })
            
                    # --- Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ°Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° ---
                    tbl = pd.DataFrame({"ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ": [n for n, _ in PNL_CODES]})
                    for y in YEARS:
                        tbl[y] = [calc[y].get(code) for _, code in PNL_CODES]
            
                    def fmt(v, pct=False, d=1):
                        if v is None or (isinstance(v, float) and np.isnan(v)): return "â€”"
                        return f"{v:.{d}f}{'%' if pct else ''}".replace(".", ",")
            
                    for i, (nm, _) in enumerate(PNL_CODES):
                        pct  = nm.endswith("%")
                        digs = 2 if ("Net" in nm or pct) else 1
                        tbl.iloc[i, 1:] = [fmt(v, pct, digs) for v in tbl.iloc[i, 1:]]
            
                    st.dataframe(tbl.set_index("ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒ"),
                                 use_container_width=True,
                                 height=min(880, 40 * len(tbl)))
            
                    # --- Ğ³Ñ€Ğ°Ñ„Ğ¸Ğº: Ğ²Ñ‹Ñ€ÑƒÑ‡ĞºĞ° / EBIT / Ñ‡Ğ¸ÑÑ‚Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ + EBIT-margin ---
                    fig, ax1 = plt.subplots(figsize=(7, 3.5))
                    x = np.arange(len(YEARS)); w = 0.25
                    bars_r  = ax1.bar(x - w, [calc[y]["2110"] or 0 for y in YEARS], w, label="Ğ’Ñ‹Ñ€ÑƒÑ‡ĞºĞ°")
                    bars_e  = ax1.bar(x,     [calc[y]["2200"] or 0 for y in YEARS], w, label="EBIT")
                    bars_p  = ax1.bar(x + w, [calc[y]["2400"] or 0 for y in YEARS], w, label="Ğ§Ğ¸ÑÑ‚Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ")
            
                    for b in (*bars_r, *bars_e, *bars_p):
                        h = b.get_height()
                        if h and not np.isnan(h):
                            ax1.annotate(f"{h:.1f}", xy=(b.get_x()+b.get_width()/2, h),
                                         xytext=(0,3), textcoords="offset points",
                                         ha="center", fontsize=8)
            
                    ax2 = ax1.twinx()
                    m_vals = [calc[y]["_ebit_margin"] if calc[y]["_ebit_margin"] else np.nan for y in YEARS]
                    ax2.plot(x, m_vals, linestyle="--", marker="o", label="EBIT margin, %")
                    # ----- ĞµĞ´Ğ¸Ğ½Ğ°Ñ Ğ»ĞµĞ³ĞµĞ½Ğ´Ğ° -----
                    h1, l1 = ax1.get_legend_handles_labels()   # bars
                    h2, l2 = ax2.get_legend_handles_labels()   # Ğ»Ğ¸Ğ½Ğ¸Ñ margin
                    ax1.legend(h1 + h2, l1 + l2, loc='upper left', fontsize=9)
            
            
                    
                    for xx, yy in zip(x, m_vals):
                        if yy and not np.isnan(yy):
                            ax2.annotate(f"{yy:.1f}%", xy=(xx, yy),
                                         xytext=(0,5), textcoords="offset points",
                                         ha="center", fontsize=8)
            
                    ax1.set_xticks(x); ax1.set_xticklabels(YEARS, fontsize=10)
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                     # âŸµ  Ğ¿Ñ€ÑÑ‡ĞµĞ¼ ÑˆĞºĞ°Ğ»Ñ‹ Y
                    ax1.set_yticks([]); ax2.set_yticks([])
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                    
                    for ax in (ax1, ax2):
                        ax.spines[:].set_visible(False)
                    for ax in (ax1, ax2): ax.spines[:].set_visible(False)
                    ax1.legend(loc="upper left", fontsize=9)
                    fig.tight_layout(pad=1.0)
                    st.pyplot(fig)
            
                    
                    
                    # â”€â”€â”€â”€â”€â”€ ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸ (Google + ÑĞ°Ğ¹Ñ‚) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    st.subheader("ğŸ“ ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸")
                    with st.spinner("Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸â€¦"):
                        doc = RAG(name, website=site, market=mkt, group=groups_full[idx]).run()     # â† Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ
                    
                    # Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚
                    main_html = _linkify(doc["summary"]).replace("\n", "<br>")
                    st.markdown(
                        f"<div style='background:#F7F9FA;border:1px solid #ccc;"
                        f"border-radius:8px;padding:18px;line-height:1.55'>{main_html}</div>",
                        unsafe_allow_html=True
                    )
                    
                    with st.expander("âš™ï¸ Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğº Google"):
                        for i, q in enumerate(doc["queries"], 1):
                            st.markdown(f"**{i}.** {q}")
                    
                    with st.expander("ğŸ” Ğ¡Ğ½Ğ¸Ğ¿Ğ¿ĞµÑ‚Ñ‹ (top-15)"):
                        st.dataframe(
                            pd.DataFrame(doc["snippets"], columns=["URL", "Snippet"]).head(15),
                            use_container_width=True,
                        )
                    
                    # ğŸŒ ĞŸĞ°ÑĞ¿Ğ¾Ñ€Ñ‚ ÑĞ°Ğ¹Ñ‚Ğ° (ĞµÑĞ»Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»ÑÑ)
                    if doc.get("site_pass"):
                        with st.expander("ğŸŒ ĞŸĞ°ÑĞ¿Ğ¾Ñ€Ñ‚ ÑĞ°Ğ¹Ñ‚Ğ°"):
                            st.markdown(
                                f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                                f"border-radius:8px;padding:18px;line-height:1.55'>"
                                f"{_linkify(doc['site_pass']).replace(chr(10), '<br>')}</div>",
                                unsafe_allow_html=True,
                            )
                    else:
                        st.info("ĞŸĞ°ÑĞ¿Ğ¾Ñ€Ñ‚ ÑĞ°Ğ¹Ñ‚Ğ° Ğ½Ğµ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½ (Ğ½ĞµÑ‚ URL, Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¸Ğ»Ğ¸ Ğ¸ÑÑ‚ĞµĞº Ñ‚Ğ°Ğ¹-Ğ°ÑƒÑ‚).")
                    
                    # â”€â”€â”€â”€â”€â”€ Ğ Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    if mkt:
                        st.subheader("ğŸ“ˆ Ğ Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚")
                        with st.spinner("Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¿Ğ¾ Ñ€Ñ‹Ğ½ĞºÑƒ Ğ¸ Ğ³ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·â€¦"):
                            mkt_res = get_market_rag(mkt)

                        mkt_html = _linkify(mkt_res["summary"]).replace("\n", "<br>")
                        st.markdown(
                            f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                            f"border-radius:8px;padding:18px;line-height:1.55'>{mkt_html}</div>",
                            unsafe_allow_html=True,
                        )

                        vols = _parse_market_volumes(mkt_res["summary"])
                        if vols:
                            fig, ax = plt.subplots(figsize=(4, 2))
                            years = list(vols.keys())
                            vals = list(vols.values())
                            bars = ax.bar(range(len(years)), vals, color="#4C72B0")
                            ax.set_xticks(range(len(years)))
                            ax.set_xticklabels(years)
                            ax.set_yticks([])
                            for spine in ax.spines.values():
                                spine.set_visible(False)
                            for i, b in enumerate(bars):
                                ax.text(b.get_x() + b.get_width() / 2, b.get_height(),
                                        f"{vals[i]:.1f}", ha="center", va="bottom", fontsize=8)
                            st.pyplot(fig)

                        with st.expander("âš™ï¸ Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğº Google"):
                            for i, q in enumerate(mkt_res["queries"], 1):
                                st.markdown(f"**{i}.** {q}")
                    
                        with st.expander("ğŸ” Ğ¡Ğ½Ğ¸Ğ¿Ğ¿ĞµÑ‚Ñ‹ (top-15)"):
                            st.dataframe(
                                pd.DataFrame(mkt_res["snippets"], columns=["URL", "Snippet"]).head(15),
                                use_container_width=True,
                            )
                    
                    # â”€â”€â”€â”€â”€â”€ Ğ ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    st.subheader("ğŸ‘¥ Ğ ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»Ğ¸ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ")
                    with st.spinner("Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑâ€¦"):
                    
                        # â‘  Ğ±ĞµÑ€Ñ‘Ğ¼ ÑÑ‹Ñ€Ñ‹Ğµ ÑĞ¿Ğ¸ÑĞºĞ¸ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ / ÑƒÑ‡Ñ€ĞµĞ´Ğ¸Ñ‚ĞµĞ»ĞµĞ¹ Ğ¸Ğ· Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ df_companies
                        company_info = {
                            "leaders_raw":  df_companies.loc[idx, "leaders_raw"]  or [],
                            "founders_raw": df_companies.loc[idx, "founders_raw"] or [],
                        }
                    
                        # â‘¡ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½
                        lead_res = get_leaders_rag(
                            name,
                            website=site,
                            market=mkt,
                            company_info=company_info,   # â† Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğµ ĞºĞ»ÑÑ‡Ğ¸
                        )
                    
                    # Ğ²Ñ‹Ğ²Ğ¾Ğ´
                    st.markdown(
                        f"<div style='background:#F9FAFB;border:1px solid #ddd;"
                        f"border-radius:8px;padding:18px;line-height:1.55'>"
                        f"{lead_res['summary'].replace(chr(10), '<br>')}</div>",
                        unsafe_allow_html=True,
                    )
                    
                    with st.expander("âš™ï¸ Ğ—Ğ°Ğ¿Ñ€Ğ¾ÑÑ‹ Ğº Google"):
                        for i, q in enumerate(lead_res["queries"], 1):
                            st.markdown(f"**{i}.** {q}")
                    
                    with st.expander("ğŸ” Ğ¡Ğ½Ğ¸Ğ¿Ğ¿ĞµÑ‚Ñ‹ (top-15)"):
                        if lead_res["snippets"]:
                            df = (
                                pd.DataFrame(lead_res["snippets"], columns=["URL", "Snippet"])
                                .drop_duplicates(subset="URL")
                                .head(15)
                            )
                            st.dataframe(df, use_container_width=True)
                        else:
                            st.info("Ğ¡Ğ½Ğ¸Ğ¿Ğ¿ĞµÑ‚Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹.")

        st.session_state["ai_report"] = {
            "doc":          doc,          # Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸
            "mkt_res":      mkt_res,      # Ñ€Ñ‹Ğ½Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚
            "lead_res":     lead_res,     # Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»Ğ¸/Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²ÑŒÑ
            "tbl":          tbl,          # Ñ„Ğ¸Ğ½. Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° DataFrame
            "graphics":     fig,          # Ğ¾Ğ±ÑŠĞµĞºÑ‚ matplotlib (ĞµÑĞ»Ğ¸ Ğ½ÑƒĞ¶ĞµĞ½ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ½Ğ´ĞµÑ€)
            # â€¦ Ñ‡Ñ‚Ğ¾-ÑƒĞ³Ğ¾Ğ´Ğ½Ğ¾ ĞµÑ‰Ñ‘
        }
        st.session_state["ai_result_ready"] = True

def long_job(total_sec: int = 180, key_prog: str = "ai_prog"):
    """Ğ¤Ğ¾Ğ½Ğ¾Ğ²Ğ°Ñ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°, ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ 1 Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµÑ‚ progress Ğ² session_state."""
    for i in range(total_sec + 1):
        time.sleep(1)
        st.session_state[key_prog] = i / total_sec     # 0 â€¦ 1
    st.session_state["ai_done"] = True                 # Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ Ğ³Ğ¾Ñ‚Ğ¾Ğ²

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. UI-Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ´Ğ²ÑƒÑ… Ğ²ĞºĞ»Ğ°Ğ´Ğ¾Ğº
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def run_advance_eye_tab() -> None:
    st.header("ğŸ‘ï¸ Advance Eye")

    user_query = st.text_input("Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ˜ĞĞ Ğ¸Ğ»Ğ¸ Ğ¤Ğ˜Ğ")
    if st.button("ğŸ” ĞĞ°Ğ¹Ñ‚Ğ¸ ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹") and user_query:
        with st.spinner("Ğ—Ğ°Ğ¿Ñ€Ğ°ÑˆĞ¸Ğ²Ğ°ĞµĞ¼ Dyxlessâ€¦"):
            res = dyxless_query(user_query, token=DYXLESS_TOKEN, max_rows=20_000)

        if res.get("status"):
            st.success(f"ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ğ½Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹: **{res['counts']}**")
            st.json(res["data"] or {"note": "ĞĞ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾"})
        else:
            st.warning(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°: {res.get('error', 'Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚')}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ (Ğ¾Ğ´Ğ¸Ğ½ Ñ€Ğ°Ğ· Ğ·Ğ° ÑĞµÑÑĞ¸Ñ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.session_state.setdefault("ai_prog", None)   # float 0â€¦1 Ğ¸Ğ»Ğ¸ None
st.session_state.setdefault("ai_done", False)  # Ğ¾Ñ‚Ñ‡Ñ‘Ñ‚ Ğ³Ğ¾Ñ‚Ğ¾Ğ²?

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Ğ”Ğ²Ğµ Ğ²ĞºĞ»Ğ°Ğ´ĞºĞ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
tab_ai, tab_eye = st.tabs(["ğŸ“Š AI-Insight", "ğŸ‘ï¸ Advance Eye"])

# === Ğ²ĞºĞ»Ğ°Ğ´ĞºĞ° 1: AI-Insight =========================================

with tab_ai:
    run_ai_insight_tab()       # Ğ²ÑÑ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸

with tab_eye:
    run_advance_eye_tab()      # Ğ¿Ğ¾Ğ¸ÑĞº Dyxless


# In[6]:





# In[14]:





# In[13]:





# In[ ]:




