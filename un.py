#!/usr/bin/env python
# coding: utf-8

# In[ ]:


#!/usr/bin/env python
# coding: utf-8

# In[6]:

from __future__ import annotations
import asyncio, re, logging, json
import aiohttp
from dataclasses import dataclass, asdict
from typing import Iterable
from urllib.parse import urlparse
# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º API-–∫–ª—é—á OpenAI
import os
import requests
import pandas as pd
import re
import numpy as np
import requests
from bs4 import BeautifulSoup
import openai
from typing import List, Dict, Any, Tuple
import json
import streamlit as st
from collections import defaultdict
import asyncio, aiohttp, re, textwrap, nest_asyncio, openai, tiktoken
from collections import defaultdict
from urllib.parse import urlparse
import tldextract, re, asyncio, aiohttp                      # –µ—Å–ª–∏ tldextract —É–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω ‚Äì —ç—Ç—É —Å—Ç—Ä–æ–∫—É –º–æ–∂–Ω–æ –∫–æ—Ä–æ—á–µ
from functools import partial
import threading
import time
import functools
import pickle
import ast
from pathlib import Path
KEYS = {
    "OPENAI_API_KEY": st.secrets["OPENAI_API_KEY"],
    "GOOGLE_API_KEY": st.secrets["GOOGLE_API_KEY"],
    "GOOGLE_CX":      st.secrets["GOOGLE_CX"],
    "CHECKO_API_KEY": st.secrets["CHECKO_API_KEY"],
    "DYXLESS_TOKEN": st.secrets["DYXLESS_TOKEN"]
}

DYXLESS_TOKEN = KEYS["DYXLESS_TOKEN"]

CACHE_FILE = Path("google_cache.pkl")
GOOGLE_CACHE: dict = {}
QUERY_HISTORY: list = []

def _save_cache():
    try:
        CACHE_FILE.write_bytes(pickle.dumps({"cache": GOOGLE_CACHE, "history": QUERY_HISTORY}))
    except Exception:
        pass

def clear_google_cache():
    GOOGLE_CACHE.clear()
    QUERY_HISTORY.clear()
    try:
        CACHE_FILE.unlink()
    except FileNotFoundError:
        pass


# In[ ]:


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ app.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import os, re, asyncio, aiohttp, requests, nest_asyncio, logging
import streamlit as st
import pandas as pd, numpy as np, matplotlib.pyplot as plt

nest_asyncio.apply()
logging.basicConfig(level=logging.WARNING)
import os, re, html, textwrap, asyncio, logging, nest_asyncio
import aiohttp, requests, streamlit as st
import pandas as pd, numpy as np, matplotlib.pyplot as plt
import tldextract, openai


import html, re

_URL_PAT = re.compile(r"https?://[^\s)]+", flags=re.I)

def _linkify(text) -> str:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç URL –≤ <a ‚Ä¶>—Å—Å—ã–ª–∫–∞</a>."""
    if not isinstance(text, str):                      # << –≥–ª–∞–≤–Ω–æ–µ
        text = "" if text is None else str(text)

    def repl(m):
        u = html.escape(m.group(0))
        return f'<a href="{u}" target="_blank">—Å—Å—ã–ª–∫–∞</a>'
    return _URL_PAT.sub(repl, text)



def long_job(total: int, key: str):
    """–î–æ–ª–≥–∞—è –∑–∞–¥–∞—á–∞: –ø–∏—à–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ st.session_state[key]"""
    for i in range(total + 1):
        time.sleep(1)                         # –∑–¥–µ—Å—å –≤–∞—à–∞ —Ç—è–∂—ë–ª–∞—è –ª–æ–≥–∏–∫–∞
        st.session_state[key] = i / total     # –æ—Ç 0.0 –¥–æ 1.0
    st.session_state[key] = 1.0               # —Ñ–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º



# ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
async def _site_snippet(domain: str) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—ã–π Google-—Å–Ω–∏–ø–ø–µ—Ç –¥–ª—è site:domain (–∏–ª–∏ '')."""
    if not domain:
        return ""
    async with aiohttp.ClientSession() as sess:
        q = f"site:{domain}"
        snips = await _google(sess, q, n=1)
    return snips[0][1] if snips else ""



@st.cache_data(ttl=3_600, show_spinner=False)
def dyxless_query(query: str,
                  token: str,
                  max_rows: int = 20_000) -> Dict[str, Any]:
    """
    –û–±—ë—Ä—Ç–∫–∞ Dyxless. –ï—Å–ª–∏ –∑–∞–ø–∏—Å–µ–π > max_rows ‚Äì –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ max_rows,
    –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–∏—à–µ–º truncated=True –∏ original_counts.
    """
    url = "https://api-dyxless.cfd/query"
    try:
        r = requests.post(url, json={"query": query, "token": token}, timeout=15)
        r.raise_for_status()
        res = r.json()

        if res.get("status") and isinstance(res.get("data"), list):
            full = len(res["data"])
            if full > max_rows:
                res["data"] = res["data"][:max_rows]
                res.update(truncated=True, original_counts=full, counts=max_rows)
        return res
    except requests.RequestException as e:
        return {"status": False, "error": str(e)}












# ‚ï≠‚îÄüîß  –≤—Å–ø–æ–º–æ–≥–∞–ª–∫–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
_BAD = ("vk.com", "facebook.", ".pdf", ".jpg", ".png")
HEADERS = {"User-Agent": "Mozilla/5.0 (Win64) AppleWebKit/537.36 Chrome/125 Safari/537.36"}
def _bad(u: str) -> bool: return any(b in u.lower() for b in _BAD)

async def _google(sess, q, n=3):
    q = re.sub(r'[\"\'‚Äú‚Äù]', " ", q)[:80]
    params = {"key": KEYS["GOOGLE_API_KEY"], "cx": KEYS["GOOGLE_CX"],
              "q": q, "num": n, "hl": "ru", "gl": "ru"}
    async with sess.get("https://www.googleapis.com/customsearch/v1",
                         params=params, headers=HEADERS, timeout=8) as r:
        if r.status != 200:
            logging.warning(f"Google error {r.status}")
            return []
        js = await r.json()
        return [(i["link"], i.get("snippet", "")) for i in js.get("items", [])
                if not _bad(i["link"])]






async def _gpt(messages, *, model="gpt-5-mini", T=0.1):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ OpenAI ChatCompletion ‚Üí str."""
    chat = await openai.ChatCompletion.acreate(
        model=model, temperature=T, messages=messages)
    return chat.choices[0].message.content.strip()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –æ—Å–Ω–æ–≤–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞ (–≤ —Å—Ç–∏–ª–µ –≤–∞—à–µ–≥–æ RAG) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class SiteRAG:
    """
    url        ‚Äì –∞–¥—Ä–µ—Å —Å–∞–π—Ç–∞ (–º–æ–∂–Ω–æ –±–µ–∑ http/https)
    max_chunk  ‚Äì –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ–¥–Ω–æ–≥–æ –∫—É—Å–∫–∞ HTML, –∫–æ—Ç–æ—Ä—ã–π —É–π–¥—ë—Ç LLM
    summary    ‚Äì –∏—Ç–æ–≥–æ–≤—ã–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç –∫–æ–º–ø–∞–Ω–∏–∏
    chunks_out ‚Äì —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—é–º–µ c –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏ HTML
    html_size  ‚Äì —Ä–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ HTML-—Ñ–∞–π–ª–∞, bytes
    """
    def __init__(self, url: str, *, model="gpt-4o-mini",
                 max_chunk: int = 6_000, T: float = 0.18):
        if not url.startswith(("http://", "https://")):
            url = "http://" + url
        self.url       = url
        self.model     = model
        self.max_chunk = max_chunk
        self.T         = T

    # ---------- 1. —Å–∫–∞—á–∏–≤–∞–µ–º HTML ------------------------------------------------
    async def _fetch(self) -> str:
        h = {"User-Agent": "Mozilla/5.0"}
        async with aiohttp.ClientSession(headers=h) as sess:
            async with sess.get(self.url, timeout=20) as r:
                if r.status == 200 and "text/html" in r.headers.get("Content-Type", ""):
                    return await r.text("utf-8", errors="ignore")
                raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å {self.url} (status={r.status})")

    # ---------- 2. –¥–µ–ª–∏–º HTML –Ω–∞ ¬´–±–µ–∑–æ–ø–∞—Å–Ω—ã–µ¬ª –∫—É—Å–∫–∏ -----------------------------
    def _split(self, html_raw: str) -> list[str]:
        # –ø—Ä–æ–±—É–µ–º —Ä–µ–∑–∞—Ç—å –ø–æ –∫—Ä—É–ø–Ω—ã–º —Ç–µ–≥–∞–º, —á—Ç–æ–±—ã –∫—É—Å–∫–∏ –±—ã–ª–∏ —Å–≤—è–∑–Ω—ã
        body = re.split(r"</?(?:body|div|section|article)[^>]*>", html_raw, flags=re.I)
        chunks, buf = [], ""
        for part in body:
            if len(buf) + len(part) > self.max_chunk:
                chunks.append(buf); buf = part
            else:
                buf += part
        if buf:
            chunks.append(buf)
        return chunks

    # ---------- 3. map-—Ñ–∞–∑–∞: –∫–æ–Ω—Å–ø–µ–∫—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∫—É—Å–æ–∫ -------------------------
    async def _summarise_chunk(self, n: int, total: int, chunk: str) -> str:
        sys = (
            "–¢—ã ‚Äì –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –ü—Ä–æ—á–∏—Ç–∞–π –¥–∞–Ω–Ω—ã–π HTML-—Ñ—Ä–∞–≥–º–µ–Ω—Ç –∏ "
            "–≤—ã–ø–∏—à–∏ –í–°–ï –∑–Ω–∞—á–∏–º—ã–µ —Ñ–∞–∫—Ç—ã –æ –∫–æ–º–ø–∞–Ω–∏–∏ (–ø—Ä–æ–¥—É–∫—Ç—ã, —É—Å–ª—É–≥–∏, –∏—Å—Ç–æ—Ä–∏—è, "
            "–≥–µ–æ–≥—Ä–∞—Ñ–∏—è, –∫–ª–∏–µ–Ω—Ç—ã, —Ü–∏—Ñ—Ä—ã, –∫–æ–º–∞–Ω–¥–∞, –∫–æ–Ω—Ç–∞–∫—Ç—ã –∏ –ø—Ä.). "
            "–£–¥–∞–ª–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏—é/footer/—Å–∫—Ä–∏–ø—Ç—ã. –°–æ—Ö—Ä–∞–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞–±–∑–∞—Ü–∞–º–∏."
        )
        return await _gpt([
            {"role": "system", "content": sys},
            {"role": "user",
             "content": f"HTML_CHUNK_{n}/{total} (len={len(chunk):,}):\n{chunk}"}],
            model=self.model, T=self.T)

    # ---------- 4. reduce-—Ñ–∞–∑–∞: –¥–µ–ª–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç ------------------------
    async def _summarise_overall(self, parts: list[str]) -> str:
        sys = (
            "–ù–∏–∂–µ —É–∂–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–Ω—Å–ø–µ–∫—Ç—ã —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–µ–π —Å–∞–π—Ç–∞. "
            "–ù–∞ –∏—Ö –æ—Å–Ω–æ–≤–µ —Å–æ—Å—Ç–∞–≤—å –æ–¥–∏–Ω –ü–û–õ–ù–´–ô –∏ —Å–≤—è–∑–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç –∫–æ–º–ø–∞–Ω–∏–∏: "
            "‚Ä¢ –∫—Ç–æ –æ–Ω–∏ –∏ —á–µ–º –∑–∞–Ω–∏–º–∞—é—Ç—Å—è; ‚Ä¢ –ø—Ä–æ–¥—É–∫—Ç—ã / —É—Å–ª—É–≥–∏; ‚Ä¢ —Ä—ã–Ω–æ–∫ –∏ –∫–ª–∏–µ–Ω—Ç—ã; "
            "‚Ä¢ –∏—Å—Ç–æ—Ä–∏—è –∏ –∫–ª—é—á–µ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è; ‚Ä¢ –≥–µ–æ–≥—Ä–∞—Ñ–∏—è –∏ –º–∞—Å—à—Ç–∞–±—ã; "
            "‚Ä¢ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ / –∫–æ–º–∞–Ω–¥–∞; ‚Ä¢ –ª—é–±—ã–µ —Ü–∏—Ñ—Ä—ã –∏ —Ñ–∞–∫—Ç—ã; "
            "‚Ä¢ –≤—ã–≤–æ–¥ –æ –ø–æ–∑–∏—Ü–∏–∏ –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞—Ö. –ù–∏—á–µ–≥–æ –≤–∞–∂–Ω–æ–≥–æ –Ω–µ —É–ø—É—Å—Ç–∏."
        )
        merged = "\n\n".join(parts)
        return await _gpt([
            {"role": "system", "content": sys},
            {"role": "user",   "content": merged}],
            model=self.model, T=self.T)

    # ---------- orchestrator ----------------------------------------------------
    async def _run_async(self):
        html_raw = await self._fetch()
        chunks   = self._split(html_raw)

        # map
        part_summaries = []
        for idx, ch in enumerate(chunks, 1):
            print(f"‚Üí LLM chunk {idx}/{len(chunks)} ‚Ä¶")
            part_summaries.append(await self._summarise_chunk(idx, len(chunks), ch))

        # reduce
        summary_final = await self._summarise_overall(part_summaries)

        return {"summary":    summary_final,
                "chunks_out": part_summaries,
                "html_size":  f"{len(html_raw):,} bytes",
                "url":        self.url}

    # ---------- –ø—É–±–ª–∏—á–Ω—ã–π —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å ----------------------------------
    def run(self) -> dict:
        loop = asyncio.get_event_loop()
        if loop and loop.is_running():
            return loop.run_until_complete(self._run_async())
        return asyncio.run(self._run_async())




# ---------- helper: —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –¥–æ—Å—Ç–∞—ë–º –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ -----------------
def _site_passport_sync(url: str, *, max_chunk: int = 6_000) -> str:
    """–í—ã–∑—ã–≤–∞–µ—Ç SiteRAG(url).run() –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ summary."""
    try:
        return SiteRAG(url, max_chunk=max_chunk).run()["summary"]
    except Exception as exc:
        return f"[site passport error: {exc}]"



class RAG:
    """
    summary    ‚Äì —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç (Google-—Å–Ω–∏–ø–ø–µ—Ç—ã + –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞)
    queries    ‚Äì –∑–∞–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ LLM —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞ –¥–ª—è Google
    snippets   ‚Äì —Å–ø–∏—Å–æ–∫ (url, text) –∏–∑ Google
    news_snippets ‚Äì —Å–Ω–∏–ø–ø–µ—Ç—ã —Å –∫—Ä—É–ø–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
    site_ctx   ‚Äì –∫–æ—Ä–æ—Ç–∫–∏–π —Å–Ω–∏–ø–ø–µ—Ç ¬´site:<–¥–æ–º–µ–Ω> ‚Ä¶¬ª
    site_pass  ‚Äì –ø–æ–¥—Ä–æ–±–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ (–≥–æ—Ç–æ–≤—ã–π summary –æ—Ç SiteRAG)
    """
    def __init__(self, company: str, *, website: str = "", market: str = "",
                 years=(2022, 2023, 2024), country: str = "–†–æ—Å—Å–∏—è",
                 steps: int = 3, snips: int = 4,
                 llm_model: str = "gpt-4o-mini",company_info: dict | None = None,):
        self.company   = company.strip()
        self.website   = website.strip()
        self.market    = market.strip()
        self.country   = country
        self.years     = years
        self.steps     = steps
        self.snips     = snips
        self.llm_model = llm_model
        self.company_info = company_info or {}

    # ---------- site-snippet –∏–∑ Google ---------------------------------
    async def _site_ctx(self) -> str:
        dom = tldextract.extract(self.website).registered_domain if self.website else ""
        snip = await _site_snippet(dom)
        if snip:
            return f"{snip}\n—Ä—ã–Ω–æ–∫ –∫–æ–º–ø–∞–Ω–∏–∏ ‚Äì {self.market}" if self.market else snip
        return f"—Ä—ã–Ω–æ–∫ –∫–æ–º–ø–∞–Ω–∏–∏ ‚Äì {self.market}" if self.market else ""

    # ---------- GPT ‚Üí –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã --------------------------------
    async def _queries(self, hist="") -> list[str]:
        dom  = tldextract.extract(self.website).registered_domain if self.website else ""
        base = f'"{self.company}"' + (f' OR site:{dom}' if dom else "")
        sys  = (
            "–¢–´ ‚Äî –û–ü–´–¢–ù–´–ô –ò–°–°–õ–ï–î–û–í–ê–¢–ï–õ–¨ –†–´–ù–ö–û–í –ò –î–ê–ù–ù–´–•. –°–§–û–†–ú–£–õ–ò–†–£–ô –ù–ï –ú–ï–ù–ï–ï 30 –ü–†–û–°–¢–´–• –†–ê–ó–ù–û–û–ë–†–ê–ó–ù–´–• GOOGLE-–ó–ê–ü–†–û–°–û–í –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï, "
            f"–ü–û–ó–í–û–õ–Ø–Æ–©–ò–• –°–û–ë–†–ê–¢–¨ –ò–ù–§–û–†–ú–ê–¶–ò–Æ –û –ö–û–ú–ü–ê–ù–ò–ò ¬´{self.company}¬ª –ù–ê –†–´–ù–ö–ï ¬´{self.market}¬ª "
            f"({self.country}, {', '.join(map(str, self.years))}).\n"
            "–ö–ê–ñ–î–´–ô –ó–ê–ü–†–û–° –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –î–û–õ–ñ–ï–ù –°–û–î–ï–†–ñ–ê–¢–¨ –ù–ê–ó–í–ê–ù–ò–ï –ö–û–ú–ü–ê–ù–ò–ò.\n"
            "### –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –ë–õ–û–ö–ò\n"
            "1. –û–ü–ò–°–ê–ù–ò–ï –ö–û–ú–ü–ê–ù–ò–ò –ò –ë–†–ï–ù–î–´.\n"
            "2. –ß–ò–°–õ–ï–ù–ù–û–°–¢–¨ –°–û–¢–†–£–î–ù–ò–ö–û–í.\n"
            "3. –ü–†–û–ò–ó–í–û–î–°–¢–í–ï–ù–ù–´–ï –ú–û–©–ù–û–°–¢–ò.\n"
            "4. –ò–ù–í–ï–°–¢–ò–¶–ò–ò –ò –†–ê–°–®–ò–†–ï–ù–ò–Ø.\n"
            "5. –ê–î–†–ï–°–ê –®–¢–ê–ë-–ö–í–ê–†–¢–ò–†–´ –ò –ü–†–û–ò–ó–í–û–î–°–¢–í.\n"
            "6. –°–û–¶–ò–ê–õ–¨–ù–´–ï –°–ï–¢–ò.\n"
            "7. –ò–°–¢–û–†–ò–Ø.\n"
            "8. –ü–†–ò–ë–´–õ–¨ –ò –û–ë–™–Å–ú–´ –ü–†–û–î–£–ö–¶–ò–ò.\n"
            "9. –ö–û–ù–ö–£–†–ï–ù–¢–´ (–ù–ê–ó–í–ê–ù–ò–ï –ò –°–ê–ô–¢).\n"
            "10. –£–ü–û–ú–ò–ù–ê–ù–ò–Ø –ù–ê –§–û–†–£–ú–ê–• –ò –í –†–ï–ô–¢–ò–ù–ì–ê–•.\n"
            "–ü–û –ö–ê–ñ–î–û–ú–£ –ë–õ–û–ö–£ –°–î–ï–õ–ê–ô –ù–ï–°–ö–û–õ–¨–ö–û –†–ê–ó–ù–´–• –ó–ê–ü–†–û–°–û–í.\n"
            "### –°–û–í–ï–¢–´ –ü–û –ö–û–ù–°–¢–†–£–ö–¶–ò–ò –ó–ê–ü–†–û–°–û–í\n"
            "- –ò–°–ü–û–õ–¨–ó–£–ô –û–ü–ï–†–ê–¢–û–†–´: `site:`, `intitle:`, `inurl:`, `filetype:pdf`, `OR`.\n"
            "- –î–û–ë–ê–í–õ–Ø–ô –ì–û–î–´ –ò –ù–ê–ó–í–ê–ù–ò–Ø –ü–†–û–î–£–ö–¢–û–í –ò –ë–†–ï–ù–î–û–í, –ï–°–õ–ò –ù–£–ñ–ù–û.\n"
            f"- –î–õ–Ø –û–§–ò–¶–ò–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–• –ü–†–ò–ú–ï–ù–Ø–ô `site:{dom}` –ò–õ–ò –°–ê–ô–¢–´ –†–ï–ì–£–õ–Ø–¢–û–†–û–í.\n"
            "### –ü–†–ê–í–ò–õ–ê\n"
            "- –ù–ï –î–£–ë–õ–ò–†–£–ô –ó–ê–ü–†–û–°–´.\n"
            "- –ù–ï –î–û–ë–ê–í–õ–Ø–ô –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ò, –ù–£–ú–ï–†–ê–¶–ò–Æ –ò –≠–ú–û–î–ó–ò.\n"
            "- –í–´–í–û–î–ò –¢–û–õ–¨–ö–û –°–¢–†–û–ö–ò –í –í–ò–î–ï `QUERY: ...`.\n"
            "### CHAIN OF THOUGHTS (–í–ù–£–¢–†–ï–ù–ù–ï, –ù–ï –í–´–í–û–î–ò–¢–¨)\n"
            "1. –ü–û–ù–Ø–¢–¨ –∑–∞–¥–∞—á—É.\n"
            "2. –°–§–û–†–ú–ò–†–û–í–ê–¢–¨ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã.\n"
            "3. –°–ì–ï–ù–ï–†–ò–†–û–í–ê–¢–¨ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã.\n"
            "4. –°–ö–û–ú–ë–ò–ù–ò–†–û–í–ê–¢–¨ –∏—Ö —Å –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞–º–∏.\n"
            "5. –í–´–í–ï–°–¢–ò —Å—Ç—Ä–æ–∫–∏ `QUERY:`.\n"
        )
        raw = await _gpt(
            [{"role": "system", "content": sys},
             {"role": "user",   "content": f'base={base}{hist}'}],
            model=self.llm_model, T=0.1)
        ql = re.findall(r"QUERY:\s*(.+)", raw, flags=re.I)

        if not hist:
            templates = [
                f'"{self.company}" –æ–ø–∏—Å–∞–Ω–∏–µ',
                f'"{self.company}" –±—Ä–µ–Ω–¥—ã',
                f'"{self.company}" —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∏',
                f'"{self.company}" —á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å',
                f'"{self.company}" –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–æ—â–Ω–æ—Å—Ç–∏',
                f'"{self.company}" –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏',
                f'"{self.company}" —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ',
                f'"{self.company}" –∞–¥—Ä–µ—Å',
                f'"{self.company}" –∏—Å—Ç–æ—Ä–∏—è',
                f'"{self.company}" –ø—Ä–∏–±—ã–ª—å',
                f'"{self.company}" –æ–±—ä—ë–º –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞',
                f'"{self.company}" –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã',
                f'"{self.company}" —Ä–µ–π—Ç–∏–Ω–≥',
                f'—Ñ–æ—Ä—É–º "{self.company}"',
                f'site:news.* "{self.company}"',
            ]
            ql = templates + [q for q in ql if q not in templates]

        # ‚îÄ‚îÄ‚îÄ —Ü–µ–ª–µ–≤—ã–µ —Å–æ—Ü—Å–µ—Ç–∏ –∏ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        social_sites = ["vk.com", "facebook.com", "linkedin.com",
                        "youtube.com", "ok.ru"]
        extras = [f'"{self.company}" site:{s}' for s in social_sites]
        if dom:
            extras.append(f'"{self.company}" site:{dom}')

        # dedup —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        ql.extend(extras)
        ql = list(dict.fromkeys(ql))
        return ql

    # ---------- —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç ----------------------------------------
    async def _summary(self, ctx: str) -> str:
        sys = (
            "–¢–´ ‚Äî –í–´–°–û–ö–û–ö–í–ê–õ–ò–§–ò–¶–ò–†–û–í–ê–ù–ù–´–ô –ê–ù–ê–õ–ò–¢–ò–ö –†–´–ù–ö–û–í. –°–û–°–¢–ê–í–¨ –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–´–ô "
            "–ê–ù–ê–õ–ò–¢–ò–ß–ï–°–ö–ò–ô –û–¢–ß–Å–¢ –û –ö–û–ú–ü–ê–ù–ò–ò –ò–ó –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–´–• –ê–ë–ó–ê–¶–ï–í –í –°–õ–ï–î–£–Æ–©–ï–ú "
            "–§–ò–ö–°–ò–†–û–í–ê–ù–ù–û–ú –ü–û–†–Ø–î–ö–ï: "
            "1) –û–ü–ò–°–ê–ù–ò–ï; "
            "2) –ë–†–ï–ù–î–´ (–ø–µ—Ä–µ—á–µ–Ω—å –∏ –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ); "
            "3) –ß–ò–°–õ–ï–ù–ù–û–°–¢–¨ –°–û–¢–†–£–î–ù–ò–ö–û–í; "
            "4) –ü–†–û–ò–ó–í–û–î–°–¢–í–ï–ù–ù–´–ï –ú–û–©–ù–û–°–¢–ò (–ø–ª–æ—â–∞–¥—å, –æ–±—ä—ë–º—ã –ø–æ –≥–æ–¥–∞–º/–¥–Ω—è–º); "
            "5) –ò–ù–í–ï–°–¢–ò–¶–ò–ò –ò –ü–†–û–ï–ö–¢–´ –†–ê–°–®–ò–†–ï–ù–ò–Ø (—Å—É–º–º–∞, –ø–ª–∞–Ω—ã, —Ä—ã–Ω–∫–∏); "
            "6) –ê–î–†–ï–° HQ –ò –ü–†–û–ò–ó–í–û–î–°–¢–í–ï–ù–ù–´–• –ü–õ–û–©–ê–î–û–ö; "
            "7) –°–û–¶–ò–ê–õ–¨–ù–´–ï –°–ï–¢–ò (–í–ö, Facebook, LinkedIn, YouTube, –û–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–∏, —Å–∞–π—Ç); "
            "8) –ò–°–¢–û–†–ò–Ø –ò –ö–õ–Æ–ß–ï–í–´–ï –°–û–ë–´–¢–ò–Ø; "
            "9) –ü–†–ò–ë–´–õ–¨/–û–ë–™–Å–ú–´ –ü–†–û–î–£–ö–¶–ò–ò; "
            "10) –ö–û–ù–ö–£–†–ï–ù–¢–´ (–Ω–∞–∑–≤–∞–Ω–∏—è, —Å–∞–π—Ç—ã, –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ); "
            "11) –£–ß–ê–°–¢–ò–ï –í –§–û–†–£–ú–ê–•/–ù–û–í–û–°–¢–Ø–•/–†–ï–ô–¢–ò–ù–ì–ê–•. "
            "–ü–û–°–õ–ï –ö–ê–ñ–î–û–ì–û –§–ê–ö–¢–ê –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –£–ö–ê–ó–´–í–ê–ô –°–°–´–õ–ö–£-–ò–°–¢–û–ß–ù–ò–ö –í –ö–†–£–ì–õ–´–• –°–ö–û–ë–ö–ê–• (–ü–û–õ–ù–´–ô URL). "
            "–ï–°–õ–ò –î–ê–ù–ù–´–• –ù–ï–¢ ‚Äî –ü–ò–®–ò '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'. "
            "–ù–ï –î–£–ë–õ–ò–†–£–ô –ò–ù–§–û–†–ú–ê–¶–ò–Æ –ò –ù–ï –í–´–î–£–ú–´–í–ê–ô –§–ê–ö–¢–û–í. "
            "–ù–ï –ò–°–ü–û–õ–¨–ó–£–ô MARKDOWN, –ù–ï –£–ö–ê–ó–´–í–ê–ô –í–´–†–£–ß–ö–£ (REVENUE) –ù–ò –í –ö–ê–ö–û–ú –í–ò–î–ï, –ù–û –ú–û–ñ–ù–û –£–ö–ê–ó–´–í–ê–¢–¨ –ü–†–ò–ë–´–õ–¨ –ü–û –ü–†–û–î–£–ö–¢–ê–ú.\n"
        )
        
        return await _gpt(
            [{"role": "system", "content": sys},
             {"role": "user",   "content": ctx[:20_000]}],
            model=self.llm_model, T=0.25)

    def _normalize_sections(self, summary: str) -> str:
        sections = summary.split("\n\n")
        norm = []
        for sec in sections:
            lines = [l.strip() for l in sec.splitlines() if l.strip()]
            seen, uniq = set(), []
            for line in lines:
                if line not in seen:
                    seen.add(line)
                    uniq.append(line)
            norm.append("\n".join(uniq) if uniq else "–Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return "\n\n".join(norm)

    # ---------- orchestrator -------------------------------------------
    async def _run_async(self):
        # paralell: —Å–Ω–∏–ø–ø–µ—Ç + –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞
        site_ctx_task = asyncio.create_task(self._site_ctx())
        site_pass_task = (
            asyncio.create_task(asyncio.to_thread(_site_passport_sync, self.website))
            if self.website else None
        )
        

        queries, snippets, hist = [], [], ""
        news_snippets: list[tuple[str, str]] = []
        async with aiohttp.ClientSession() as s:
            for _ in range(self.steps):
                ql = await self._queries(hist)
                ql = [f"{q} {self.market}" if self.market and
                      self.market.lower() not in q.lower() else q for q in ql]
                queries += ql
                res = await asyncio.gather(*[_google(s, q, self.snips) for q in ql])
                snippets += sum(res, [])
                hist = f"\n–°–Ω–∏–ø–ø–µ—Ç–æ–≤: {len(snippets)}"

            news_domains = [
                "rbc.ru",
                "kommersant.ru",
                "vedomosti.ru",
                "tass.ru",
                "forbes.ru",
            ]
            news_queries = [f'site:{d} "{self.company}"' for d in news_domains]
            queries += news_queries
            res = await asyncio.gather(*[_google(s, q, self.snips) for q in news_queries])
            news_snippets = sum(res, [])
            snippets += news_snippets

        site_ctx  = await site_ctx_task
        site_pass = await site_pass_task if site_pass_task else ""

        # –≤—ã–¥–µ–ª—è–µ–º —Å–Ω–∏–ø–ø–µ—Ç—ã —Å —Å–æ—Ü—Å–µ—Ç—è–º–∏
        dom = tldextract.extract(self.website).registered_domain if self.website else ""
        social_domains = ["vk.com", "facebook.com", "linkedin.com",
                          "youtube.com", "ok.ru"]
        if dom:
            social_domains.append(dom)
        social_snips = [
            (u, t) for u, t in snippets
            if any(sd in u.lower() or sd in t.lower() for sd in social_domains)
        ]

        # ---------- —Å–æ–±–∏—Ä–∞–µ–º –µ–¥–∏–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è GPT -----------------
        ctx_parts: list[str] = []

        # 1) –∫–æ—Ä–æ—Ç–∫–∏–π —Å–Ω–∏–ø–ø–µ—Ç –∏–∑ Google (¬´site:‚Ä¶¬ª)
        if site_ctx:
            ctx_parts.append(f"SITE_SNIPPET:\n{site_ctx}")

        # 2) –ø–æ–ª–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π SiteRAG
        if site_pass:
            ctx_parts.append(f"SITE_PASSPORT:\n{site_pass}")

        # 3) —Ñ–∞–∫—Ç—ã –∏–∑ checko / fin-API
        company_doc_txt = ""
        if self.company_info:                       # ‚Üê –±—ã–ª –ø–µ—Ä–µ–¥–∞–Ω –≤ __init__
            def _pair(k, v):
                if v in (None, "", []): return ""
                if isinstance(v, list):
                    v = "; ".join(map(str, v[:10]))
                return f"* **{k}:** {v}"
            company_doc_txt = "\n".join(
                p for p in (_pair(k, v) for k, v in self.company_info.items()) if p
            )
            if company_doc_txt:
                ctx_parts.append(f"COMPANY_DOC:\n{company_doc_txt}")

        # 4) —Å–æ—Ü—Å–µ—Ç–∏
        if social_snips:
            ctx_parts.append(
                "SOCIAL_SNIPPETS:\n" +
                "\n".join(f"URL:{u}\nTXT:{t}" for u, t in social_snips)
            )

        # 5) Google-—Å–Ω–∏–ø–ø–µ—Ç—ã
        ctx_parts.append(
            "\n".join(f"URL:{u}\nTXT:{t}" for u, t in snippets)
        )

        # ---------- —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç ----------------------------------
        summary = await self._summary("\n\n".join(ctx_parts))
        summary = self._normalize_sections(summary)

        return {
            "summary":     summary,
            "queries":     queries,
            "snippets":    snippets,
            "news_snippets": news_snippets,
            "site_ctx":    site_ctx,
            "site_pass":   site_pass,
            "company_doc": company_doc_txt   # ‚Üê –Ω–æ–≤—ã–π –∫–ª—é—á (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω –≤–æ —Ñ—Ä–æ–Ω—Ç–µ)
        }


    # ---------- –ø—É–±–ª–∏—á–Ω—ã–π —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å -----------------
    def run(self) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å–æ –≤—Å–µ–º–∏ –ø–æ–ª—è–º–∏.
        –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –∫–æ–≥–¥–∞ event-loop —É–∂–µ –∑–∞–ø—É—â–µ–Ω
        (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ Jupyter, –≤–Ω—É—Ç—Ä–∏ Streamlit-callback –∏ —Ç.–ø.).
        """
        try:
            loop = asyncio.get_event_loop()
            if loop and loop.is_running():
                # nest_asyncio.patch() —É–∂–µ –≤—ã–∑–≤–∞–Ω –≤—ã—à–µ, –ø–æ—ç—Ç–æ–º—É –º–æ–∂–Ω–æ:
                return loop.run_until_complete(self._run_async())
        except RuntimeError:
            # get_event_loop() –º–æ–∂–µ—Ç –±—Ä–æ—Å–∏—Ç—å, –µ—Å–ª–∏ —Ü–∏–∫–ª–∞ –Ω–µ—Ç ‚Äî —Ç–æ–≥–¥–∞ –ø—Ä–æ—Å—Ç–æ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π
            pass

        return asyncio.run(self._run_async())












# ‚ï≠‚îÄüåê  Market RAG helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
async def google_snippets(query: str, num: int = 4):
    q = re.sub(r'[\"\'‚Äú‚Äù]', '', query)[:80]
    params = {
        "key": KEYS["GOOGLE_API_KEY"],
        "cx":  KEYS["GOOGLE_CX"],
        "q":   q, "num": num, "hl": "ru", "gl": "ru"
    }
    async with aiohttp.ClientSession() as sess:
        async with sess.get("https://www.googleapis.com/customsearch/v1",
                            params=params, headers=HEADERS, timeout=8) as r:
            if r.status != 200:
                logging.warning(f"[Google] {r.status}")
                return []
            js = await r.json()
            return [(it["link"], it.get("snippet", ""))
                    for it in js.get("items", []) if not _bad(it["link"])]

async def gpt_async(messages, T=0.2, model="gpt-4o-mini"):
    chat = await openai.ChatCompletion.acreate(
        model=model, temperature=T, messages=messages)
    return chat.choices[0].message.content.strip()

class FastMarketRAG:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict(summary, queries, snippets) –∑–∞ ~10 —Å."""
    def __init__(self, market, country="–†–æ—Å—Å–∏—è",
                 years=(2021, 2022, 2023, 2024),
                 steps=1, snips=6):
        self.market, self.country = market, country
        self.years, self.steps, self.snips = years, steps, snips



    
    async def _queries(self, hist=""):
        sys = (
            "–¢–´ ‚Äî –û–ü–´–¢–ù–´–ô –ò–°–°–õ–ï–î–û–í–ê–¢–ï–õ–¨ –†–´–ù–ö–û–í –ò –î–ê–ù–ù–´–•. –°–§–û–†–ú–£–õ–ò–†–£–ô 10‚Äì12 –¢–û–ß–ù–´–• –ò –≠–§–§–ï–ö–¢–ò–í–ù–´–• GOOGLE-–ó–ê–ü–†–û–°–û–í, "
            f"–ù–ê–ü–†–ê–í–õ–ï–ù–ù–´–• –ù–ê –°–ë–û–† –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–û–ô –ò–ù–§–û–†–ú–ê–¶–ò–ò –û –†–´–ù–ö–ï ¬´{self.market}¬ª –í –°–¢–†–ê–ù–ï {self.country.upper()} –ó–ê –ü–ï–†–ò–û–î {', '.join(map(str, self.years))}. "
            "–ü–û–ò–°–ö–û–í–´–ï –ó–ê–ü–†–û–°–´ –î–û–õ–ñ–ù–´ –û–•–í–ê–¢–´–í–ê–¢–¨ –°–õ–ï–î–£–Æ–©–ò–ï –ê–°–ü–ï–ö–¢–´ –†–´–ù–ö–ê: "
            "1) –û–ë–™–Å–ú –ò –î–ò–ù–ê–ú–ò–ö–ê –†–´–ù–ö–ê, "
            "2) –°–¢–†–£–ö–¢–£–†–ê –ò –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø, "
            "3) –û–°–ù–û–í–ù–´–ï –ò–ì–†–û–ö–ò –ò –ò–• –î–û–õ–ò, "
            "4) –¶–ï–ù–´ –ò –¶–ï–ù–û–í–´–ï –¢–ï–ù–î–ï–ù–¶–ò–ò, "
            "5) –ö–õ–Æ–ß–ï–í–´–ï –¢–†–ï–ù–î–´ –ò –ò–ù–û–í–ê–¶–ò–ò, "
            "6) –†–ï–ì–ò–û–ù–ê–õ–¨–ù–´–ô –†–ê–ó–†–ï–ó, "
            "7) –§–ê–ö–¢–û–†–´ –†–û–°–¢–ê –ò –ë–ê–†–¨–ï–†–´ –í–•–û–î–ê, "
            "8) –°–î–ï–õ–ö–ò, IPO, –°–õ–ò–Ø–ù–ò–Ø, "
            "9) –ê–ù–ê–õ–ò–¢–ò–ß–ï–°–ö–ò–ï –û–¢–ß–Å–¢–´ –ò –î–û–ö–õ–ê–î–´ "
            "–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê: QUERY: <–°–¢–†–û–ö–ê –î–õ–Ø –ü–û–ò–°–ö–ê –í GOOGLE>. "
            "–ù–ï –ü–û–í–¢–û–†–Ø–ô –ó–ê–ü–†–û–°–´. –ù–ï –î–û–ë–ê–í–õ–Ø–ô –õ–ò–®–ù–ò–• –ü–†–ï–î–ò–°–õ–û–í–ò–ô ‚Äî –¢–û–õ–¨–ö–û –°–ü–ò–°–ö–û–ú."
)
        raw = await gpt_async([
            {"role": "system", "content": sys},
            {"role": "user",   "content": hist}
        ], T=0.12)
        return re.findall(r"QUERY:\s*(.+)", raw, flags=re.I)

    async def _run_async(self):
        snippets, hist = [], ""
        for _ in range(self.steps):
            ql = await self._queries(hist)
            tasks = [google_snippets(q, self.snips) for q in ql]
            for res in await asyncio.gather(*tasks):
                snippets.extend(res)
            hist = f"—Å–Ω–∏–ø–ø–µ—Ç–æ–≤={len(snippets)}"

        context = "\n".join(f"URL:{u}\nTXT:{t}" for u, t in snippets)[:18000]
        sys = (
            f"–¢–´ ‚Äî –í–´–°–û–ö–û–ö–õ–ê–°–°–ù–´–ô –ê–ù–ê–õ–ò–¢–ò–ö –†–´–ù–ö–ê ¬´{self.market}¬ª –í –°–¢–†–ê–ù–ï {self.country.upper()}. "
            "–°–§–û–†–ú–ò–†–£–ô –ü–û–ì–û–î–û–í–û–ô –û–ë–ó–û–† –†–´–ù–ö–ê, –ì–î–ï –ö–ê–ñ–î–´–ô –ì–û–î –ü–†–ï–î–°–¢–ê–í–õ–ï–ù –û–¢–î–ï–õ–¨–ù–´–ú –ù–ê–ü–û–õ–ù–ï–ù–ù–´–ú –ê–ë–ó–ê–¶–ï–ú, –í–ö–õ–Æ–ß–ê–Æ–©–ò–ú –°–õ–ï–î–£–Æ–©–ò–ï –≠–õ–ï–ú–ï–ù–¢–´: "
            "1) –û–ë–™–Å–ú –†–´–ù–ö–ê (–µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∞), "
            "2) –¢–ï–ú–ü –†–û–°–¢–ê (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö –∏–ª–∏ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö), "
            "3) –°–¢–†–£–ö–¢–£–†–ê –ò –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø (–ø–æ —Ç–∏–ø—É –ø—Ä–æ–¥—É–∫—Ç–∞, –∫–ª–∏–µ–Ω—Ç—É, –∫–∞–Ω–∞–ª—É –∏ –¥—Ä.), "
            "4) –†–ï–ì–ò–û–ù–ê–õ–¨–ù–´–ï –†–ê–ó–†–ï–ó–´ (–∫–ª—é—á–µ–≤—ã–µ —Ä–µ–≥–∏–æ–Ω—ã –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–∞–Ω—ã), "
            "5) –û–°–ù–û–í–ù–´–ï –ò–ì–†–û–ö–ò –ò –ò–• –î–û–õ–ò (—Å –¥–æ–ª—è–º–∏ –≤ %, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã), "
            "6) –ö–†–£–ü–ù–´–ï –°–î–ï–õ–ö–ò –ò –°–û–ë–´–¢–ò–Ø (M&A, IPO, –ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–∞), "
            "7) –¶–ï–ù–û–í–û–ô –ê–ù–ê–õ–ò–ó (—É—Ä–æ–≤–Ω–∏ —Ü–µ–Ω, –¥–∏–Ω–∞–º–∏–∫–∞, —Ñ–∞–∫—Ç–æ—Ä—ã –≤–ª–∏—è–Ω–∏—è), "
            "8) –ö–õ–Æ–ß–ï–í–´–ï –¢–†–ï–ù–î–´ (—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, —Å–ø—Ä–æ—Å, —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –¥—Ä.), "
            "9) –ë–ê–†–¨–ï–†–´ –ò –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø (–≤—Ö–æ–¥, –ª–æ–≥–∏—Å—Ç–∏–∫–∞, –Ω–æ—Ä–º–∞—Ç–∏–≤–∫–∞), "
            "10) –í–´–í–û–î–´ –ü–û –ì–û–î–£ (–∫–ª—é—á–µ–≤—ã–µ –∏—Ç–æ–≥–∏ –∏ —Å–¥–≤–∏–≥–∏). "
            "11) –≤ –∫–æ–Ω—Ü–µ –≤—ã–≤–µ–¥–∏ —Ç–∞–±–ª–∏—Ü—É —Ñ–æ—Ä–º–∞—Ç–∞ '–ì–æ–¥ | –û–±—ä—ë–º —Ä—ã–Ω–∫–∞' –ø–æ –≤—Å–µ–º —É–ø–æ–º—è–Ω—É—Ç—ã–º –≥–æ–¥–∞–º; —Ç–∞–±–ª–∏—Ü–∞ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ –±–µ–∑ –æ—Å–µ–π –∏ –ø–æ–¥–ø–∏—Å–µ–π"
            "–í–°–ï –§–ê–ö–¢–´ –î–û–õ–ñ–ù–´ –ë–´–¢–¨ –£–ù–ò–ö–ê–õ–¨–ù–´–ú–ò, –ù–ï –ü–û–í–¢–û–†–Ø–¢–¨–°–Ø –ò –ü–û–î–¢–í–ï–†–ñ–î–Å–ù–ù–´ –†–ï–ê–õ–¨–ù–´–ú–ò –°–°–´–õ–ö–ê–ú–ò –ù–ê –ò–°–¢–û–ß–ù–ò–ö–ò –í –ö–†–£–ì–õ–´–• –°–ö–û–ë–ö–ê–• (–§–û–†–ú–ê–¢: –ü–û–õ–ù–´–ô URL). "
            "–ù–ï –ò–°–ü–û–õ–¨–ó–£–ô MARKDOWN, –ù–ï –ü–†–ò–î–£–ú–´–í–ê–ô –§–ê–ö–¢–´ ‚Äî –¢–û–õ–¨–ö–û –î–û–ö–£–ú–ï–ù–¢–ò–†–û–í–ê–ù–ù–´–ï –î–ê–ù–ù–´–ï. "
            "–≤—Å–µ–≥–¥–∞ –æ—Å—Ç–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏"

)
        summary = await gpt_async([
            {"role": "system", "content": sys},
            {"role": "user",   "content": context}
        ], T=0.19)
        return {"summary": summary, "queries": ql, "snippets": snippets}

    def run(self):
        return asyncio.run(self._run_async())

# –∫–µ—à–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö –±—ã–ª–æ –º–≥–Ω–æ–≤–µ–Ω–Ω–æ
@st.cache_data(ttl=86_400, show_spinner="üîé –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç‚Ä¶")
def get_market_rag(market):
    return FastMarketRAG(market).run()








# ---------------------------------------------------------------
# –í–Ω–µ—à–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –≤ –ø—Ä–æ–µ–∫—Ç–µ):
#   _google(sess, query: str, k: int) -> list[tuple[str, str]]
#   _gpt(messages, model: str, T: float) -> str
#   _image(sess, query: str) -> str | None
#   _linkify(text: str) -> str
#   ck_company(inn: str) -> dict  # (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
#   _site_passport_sync(url: str) -> str  # (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
# ---------------------------------------------------------------

RUS_NEWS_DOMAINS: set[str] = {
    # —Ñ–µ–¥–µ—Ä–∞–ª—å–Ω—ã–µ
    "kommersant.ru", "vedomosti.ru", "rbc.ru", "forbes.ru", "tass.ru",
    "interfax.ru", "iz.ru", "ria.ru", "thebell.io", "lenta.ru", "gazeta.ru",
    # –ø—Ä–æ—Ñ–∏–ª—å–Ω—ã–µ/—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ
    "vc.ru", "cnews.ru", "fontanka.ru", "dp.ru", "banki.ru",
}

INTERVIEW_PAT = re.compile(r"\b–∏–Ω—Ç–µ—Ä–≤—å—é\b|–ø–æ–¥–∫–∞—Å—Ç|Q&A|q&a|–≤ –±–µ—Å–µ–¥–µ|–¥–∞–ª –∏–Ω—Ç–µ—Ä–≤—å—é|exclusive|—ç–∫—Å–∫–ª—é–∑–∏–≤", re.I)

LEGAL_KEYWORDS = {
    "prosecuted": [
        "—É–≥–æ–ª–æ–≤–Ω–æ–µ –¥–µ–ª–æ", "–ø—Ä–µ—Å–ª–µ–¥", "–≤–æ–∑–±—É–∂–¥–µ–Ω–æ –¥–µ–ª–æ", "—Ä–∞—Å—Å–ª–µ–¥—É–µ—Ç—Å—è",
        "–ø–æ–¥ —Å–ª–µ–¥—Å—Ç–≤–∏–µ–º", "–ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ—Ç—Å—è", "–æ–±–≤–∏–Ω", "–¥–µ–ª–æ –≤ –æ—Ç–Ω–æ—à–µ–Ω–∏–∏",
    ],
    "imprisoned": [
        "–∞—Ä–µ—Å—Ç–æ–≤–∞–Ω", "–∑–∞–¥–µ—Ä–∂–∞–Ω", "–≤ —Å–∏–∑–æ", "–≤ –∫–æ–ª–æ–Ω–∏–∏", "–ø—Ä–∏–≥–æ–≤–æ—Ä",
        "–æ—Å—É–∂–¥", "–ª–∏—à–µ–Ω–∏–µ —Å–≤–æ–±–æ–¥—ã", "–≤ —Ç—é—Ä—å–º–µ", "–¥–æ–º–∞—à–Ω–∏–π –∞—Ä–µ—Å—Ç",
    ],
    "sanctioned": [
        "—Å–∞–Ω–∫—Ü", "ofac", "sdn", "—Å–∞–Ω–∫—Ü–∏–æ–Ω–Ω—ã—Ö —Å–ø–∏—Å", "uk sanctions", "eu sanctions",
    ],
}

# ----------------- Helpers -----------------

def _norm_ws(s: str | None) -> str:
    return re.sub(r"\s+", " ", (s or "").strip())


def _normalize_name(fio: str) -> str:
    fio = _norm_ws(fio).replace("\xa0", " ")
    fio = re.sub(r"\s+", " ", fio)
    return fio.strip(" \t\n\r\f\v-‚Äî\"'")


def _dedupe(seq: Iterable[str]) -> list[str]:
    seen, out = set(), []
    for x in seq:
        k = x.lower()
        if k not in seen:
            seen.add(k); out.append(x)
    return out


def _is_news(url: str) -> bool:
    host = urlparse(url).netloc.lower()
    return any(d in host for d in RUS_NEWS_DOMAINS)


def _scan_legal(text: str) -> dict[str, bool]:
    t = text.lower()
    return {k: any(kw in t for kw in kws) for k, kws in LEGAL_KEYWORDS.items()}


@dataclass
class Person:
    name: str
    role: str
    bio: str | None = None
    news: list[str] | None = None
    photo: str | None = None
    sources: list[str] | None = None
    legal: dict[str, bool] | None = None

    @property
    def tagged(self) -> str:
        if "(" in self.name and self.name.endswith(")"):
            return self.name
        return f"{self.name} ({self.role})"


class FastLeadersInterviews:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict(summary, names, queries, snippets).

    company_info –∂–¥—ë—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É Checko/FNS (best-effort):
      ‚Ä¢ leaders_raw / founders_raw
      ‚Ä¢ general_director / managers / "–†—É–∫–æ–≤–æ–¥"
      ‚Ä¢ founders / "–£—á—Ä–µ–¥_–§–õ"
      ‚Ä¢ –ò–ù–ù / inn
    """
    def get_leaders_rag(name: str, *, website: str = "", market: str = "", company_info: dict | None = None):
        return FastLeadersInterviews(name, website=website, market=market, company_info=company_info).run()

    def _safe_div(a, b):
        try:
            if a is None or b in (None, 0):
                return None
            return a / b
        except Exception:
            return None

    
    def __init__(self, company: str, *, website: str = "", market: str = "",
                 company_info: dict | None = None, model: str = "gpt-4o-mini"):
        self.c      = company.strip()
        self.site   = website.strip()
        self.market = market.strip()
        self.cinfo  = company_info or {}
        self.model  = model

    # ---------------- helpers ----------------
    def _domain(self) -> str:
        try:
            import tldextract
            return tldextract.extract(self.site).registered_domain if self.site else ""
        except Exception:
            return ""

    async def _llm_queries(self, prompt: str, n: int = 8) -> list[str]:
        raw = await _gpt([
            {"role": "system", "content": "–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –∫–æ—Ä–æ—Ç–∫–∏–µ Google-–∑–∞–ø—Ä–æ—Å—ã, –ø–æ –æ–¥–Ω–æ–º—É –Ω–∞ —Å—Ç—Ä–æ–∫—É."},
            {"role": "user",   "content": prompt}
        ], model=self.model, T=0.18)
        qs = [q.strip().lstrip("Q:").strip() for q in raw.splitlines() if q.strip()]
        return _dedupe(qs)[:n]

    # ---------- 1) –†–£–ö–û–í–û–î–ò–¢–ï–õ–ò / –í–õ–ê–î–ï–õ–¨–¶–´ ----------
    async def _leaders(self, sess: aiohttp.ClientSession):
        people: list[Person] = []
        queries: list[str] = []
        snips:   list[tuple[str, str]] = []

        # 0) –ò–∑ company_info (–Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∫–ª—é—á–µ–π)
        def _push_name(x, role):
            if isinstance(x, str) and x.strip():
                people.append(Person(name=_normalize_name(x), role=role))
            elif isinstance(x, dict):
                fio = x.get("name") or x.get("fio") or x.get("–§–ò–û") or ""
                if fio: people.append(Person(name=_normalize_name(fio), role=role))
            elif isinstance(x, list):
                for y in x: _push_name(y, role)

        _push_name(self.cinfo.get("leaders_raw"),  "–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä")
        _push_name(self.cinfo.get("founders_raw"), "–∞–∫—Ü–∏–æ–Ω–µ—Ä")
        _push_name(self.cinfo.get("general_director"), "–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä")
        _push_name(self.cinfo.get("–†—É–∫–æ–≤–æ–¥"), "–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä")
        _push_name(self.cinfo.get("–£—á—Ä–µ–¥_–§–õ"), "–∞–∫—Ü–∏–æ–Ω–µ—Ä")
        _push_name(self.cinfo.get("founders"), "–∞–∫—Ü–∏–æ–Ω–µ—Ä")

        # 1) Checko –ø–æ –ò–ù–ù (–º—è–≥–∫–æ)
        inn = self.cinfo.get("–ò–ù–ù") or self.cinfo.get("inn")
        if inn:
            try:
                cdata = ck_company(str(inn))
                fio_dir = _normalize_name(cdata.get("–†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å", {}).get("–§–ò–û") or cdata.get("CEO") or "")
                if fio_dir:
                    people.append(Person(name=fio_dir, role="–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä"))
                for f in cdata.get("–£—á—Ä–µ–¥–∏—Ç–µ–ª–∏–§–õ") or []:
                    fio = _normalize_name(f.get("–§–ò–û") or f.get("fio") or "")
                    if fio:
                        people.append(Person(name=fio, role="–∞–∫—Ü–∏–æ–Ω–µ—Ä"))
            except Exception as e:
                logging.warning(f"[ck_company] {inn}: {e}")

        # 2) –ï—Å–ª–∏ –ø—É—Å—Ç–æ ‚Üí —Å–Ω–∞—á–∞–ª–∞ –Ω–∞–π–¥—ë–º –§–ò–û –≥–µ–Ω–¥–∏—Ä–∞ –∏–∑ Google
        if not people:
            base_q = [f'"{self.c}" –≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä', f'"{self.c}" CEO']
            for q in base_q:
                queries.append(q)
                snips.extend(await _google(sess, q, 3))
            if snips:
                sys = (
                    "–¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫. –ò–∑ —Å–Ω–∏–ø–ø–µ—Ç–æ–≤ –∏–∑–≤–ª–µ–∫–∏ –§–ò–û –≥–µ–Ω–µ—Ä–∞–ª—å–Ω–æ–≥–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞. "
                    "–§–æ—Ä–º–∞—Ç: NAME: <–§–ò–û>"
                )
                txt = "\n".join(f"URL:{u}\nTXT:{t}" for u, t in snips)[:10_000]
                llm = await _gpt([
                    {"role": "system", "content": sys},
                    {"role": "user",   "content": txt}
                ], model=self.model, T=0.1)
                m = re.search(r"NAME:\s*(.+)$", llm.strip())
                if m:
                    people.append(Person(name=_normalize_name(m.group(1)), role="–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä"))

        # 3) –î–µ–¥—É–ø –ø–æ (–∏–º—è, —Ä–æ–ª—å)
        seen = set(); ded = []
        for p in people:
            k = (p.name.lower(), p.role.lower())
            if k in seen: continue
            seen.add(k); ded.append(p)
        people = ded

        if not people:
            return [], queries, snips

        # 4) –û–±–æ–≥–∞—â–µ–Ω–∏–µ: –±–∏–æ–≥—Ä–∞—Ñ–∏–∏/—é—Ä—Å—Ç–∞—Ç—É—Å/—Ñ–æ—Ç–æ/–°–ú–ò-—Å—Å—ã–ª–∫–∏
        sem_http = asyncio.Semaphore(6)
        sem_llm  = asyncio.Semaphore(2)

        async def enrich(p: Person) -> Person:
            fio = p.name.split("(")[0].strip()

            qset = {
                f'"{fio}" {self.c}',
                f'"{fio}" {self.c} –±–∏–æ–≥—Ä–∞—Ñ–∏—è',
                f'"{fio}" –±–∏–æ–≥—Ä–∞—Ñ–∏—è',
                f'"{fio}" {self.c} –∫–∞—Ä—å–µ—Ä–∞',
                f'"{fio}" –∞–∫—Ç–∏–≤—ã', f'"{fio}" —Å–æ—Å—Ç–æ—è–Ω–∏–µ', f'"{fio}" –∫–∞–ø–∏—Ç–∞–ª', f'"{fio}" Forbes',
                f'"{fio}" —Å—É–ø—Ä—É–≥', f'"{fio}" —Å—É–ø—Ä—É–≥–∞', f'"{fio}" –∂–µ–Ω–∞', f'"{fio}" –º—É–∂',
                f'"{fio}" –¥–µ—Ç–∏', f'"{fio}" —Å—ã–Ω', f'"{fio}" –¥–æ—á—å',
                f'"{fio}" —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫', f'"{fio}" —Å–≤—è–∑–∞–Ω—ã', f'"{fio}" —Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫ —á–∏–Ω–æ–≤–Ω–∏–∫',
                f'"{fio}" –¥—Ä—É–≥ –±–∏–∑–Ω–µ—Å–º–µ–Ω', f'"{fio}" —á–∏–Ω–æ–≤–Ω–∏–∫',
                f'"{fio}" —É–≥–æ–ª–æ–≤–Ω–æ–µ –¥–µ–ª–æ', f'"{fio}" –∞—Ä–µ—Å—Ç', f'"{fio}" —Å–∞–Ω–∫—Ü–∏–∏',
            }
            try:
                prompt = (
                    f"–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π 4‚Äì8 –≥—É–≥–ª‚Äë–∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ñ–∞–∫—Ç–æ–≤ –æ '{fio}' –∏–∑ '{self.c}': "
                    f"–∫–∞—Ä—å–µ—Ä–∞/–≥–¥–µ —Ä–∞–±–æ—Ç–∞–ª; –∞–∫—Ç–∏–≤—ã/—Å–æ—Å—Ç–æ—è–Ω–∏–µ/Forbes; —Å–µ–º—å—è (—Å—É–ø—Ä—É–≥(–∞), –¥–µ—Ç–∏); "
                    f"—Å–≤—è–∑–∏/—Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫–∏ (–≥–æ—Å—Å–ª—É–∂–±–∞, –±–∏–∑–Ω–µ—Å–º–µ–Ω—ã); —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —Å—Ç–∞—Ç—É—Å (–¥–µ–ª–∞, –∞—Ä–µ—Å—Ç—ã, —Å–∞–Ω–∫—Ü–∏–∏)."
                )
                qset.update(await self._llm_queries(prompt))
            except Exception as e:
                logging.warning(f"[llm-queries] {fio}: {e}")

            search_all: list[tuple[str, str]] = []
            for q in _dedupe(list(qset)):
                async with sem_http:
                    try:
                        res = await _google(sess, q, 3)
                    except Exception as e:
                        logging.warning(f"[google] {q}: {e}")
                        res = []
                queries.append(q)
                snips.extend(res)
                search_all.extend(res)

            ctx = "\n".join(f"URL:{u}\nTXT:{t}" for u, t in search_all)[:10_000]
            sys_bio = (
                "–°–∂–∞—Ç–æ –∏ —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–µ —Ñ–∞–∫—Ç—ã. –í—ã–≤–µ–¥–∏ –ø—É–Ω–∫—Ç–∞–º–∏:\n"
                "1) –ö–∞—Ä—å–µ—Ä–∞/–≥–¥–µ —Ä–∞–±–æ—Ç–∞–ª (2‚Äì3 —Ñ–∞–∫—Ç–∞).\n"
                "2) –ê–∫—Ç–∏–≤—ã –∏ –æ—Ü–µ–Ω–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å).\n"
                "3) –°–µ–º—å—è: —Å—É–ø—Ä—É–≥(–∞), –¥–µ—Ç–∏ (–µ—Å–ª–∏ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è).\n"
                "4) –°–≤—è–∑–∏/—Ä–æ–¥—Å—Ç–≤–µ–Ω–Ω–∏–∫–∏: –≥–æ—Å—Å–ª—É–∂–∞—â–∏–µ/–±–∏–∑–Ω–µ—Å–º–µ–Ω—ã (–µ—Å–ª–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ).\n"
                "5) –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —Å—Ç–∞—Ç—É—Å: –ø—Ä–µ—Å–ª–µ–¥—É–µ—Ç—Å—è/—Å–∏–¥–µ–ª/–ø–æ–¥ —Å–∞–Ω–∫—Ü–∏—è–º–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å).\n"
                "–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî –ø–∏—à–∏ '–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö'. –í –∫–æ–Ω—Ü–µ —Å—Ç—Ä–æ–∫–∞: –ò—Å—Ç–æ—á–Ω–∏–∫–∏: URL1; URL2; URL3 (–¥–æ 3)."
            )
            async with sem_llm:
                bio = await _gpt([
                    {"role": "system", "content": sys_bio},
                    {"role": "user",   "content": ctx},
                ], model=self.model, T=0.15)
            p.bio = _norm_ws(bio)

            p.legal = _scan_legal("\n".join(t for _, t in search_all))
            p.news  = _dedupe([u for u, _ in search_all if _is_news(u)])

            async with sem_http:
                try:
                    p.photo = await _image(sess, f"{fio} {self.c}")
                except Exception as e:
                    logging.warning(f"[photo] {fio}: {e}")
                    p.photo = None

            p.sources = [u for u, _ in search_all]
            return p

        people = await asyncio.gather(*[enrich(p) for p in people])
        return [asdict(p) | {"name": p.tagged} for p in people], queries, snips

    # ---------- 2) –ò–Ω—Ç–µ—Ä–≤—å—é –∏ –Ω–æ–≤–æ—Å—Ç–∏ –æ –∫–æ–º–ø–∞–Ω–∏–∏ (–°–ú–ò‚Äë—Ç–æ–ª—å–∫–æ) ----------
    async def _interviews(self, names: list[dict], sess: aiohttp.ClientSession):
        dom = self._domain()
        sc  = await self._site_ctx(sess)
        base_ctx = (f"SITE_CONTEXT:\n{sc}\n—Ä—ã–Ω–æ–∫ –∫–æ–º–ø–∞–Ω–∏–∏ ‚Äì {self.market}\n\n" if sc else "")

        def qpack_person(fio: str) -> list[str]:
            qs = [
                f'"{fio}" –∏–Ω—Ç–µ—Ä–≤—å—é', f'"{fio}" –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π', f'"{fio}" –¥–∞–ª –∏–Ω—Ç–µ—Ä–≤—å—é',
                f'"{fio}" –ø–æ–¥–∫–∞—Å—Ç', f'"{fio}" –≤—ã—Å—Ç—É–ø–∏–ª', f'"{fio}" {self.c}',
            ]
            if dom: qs.append(f'"{fio}" site:{dom}')
            return qs

        q_company = [
            f'"{self.c}" –∏–Ω—Ç–µ—Ä–≤—å—é', f'"{self.c}" –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π', f'"{self.c}" –Ω–æ–≤–æ—Å—Ç—å',
        ]
        if dom: q_company.append(f'"{self.c}" site:{dom}')

        all_queries: list[str] = []
        all_snips:   list[tuple[str, str]] = []

        async def pull(q: str):
            all_queries.append(q)
            try:
                all_snips.extend(await _google(sess, q, 3))
            except Exception as e:
                logging.warning(f"[interviews-google] {q}: {e}")

        tasks = [pull(q) for q in q_company]
        for p in (names or []):
            fio = p["name"].split("(")[0].strip()
            tasks += [pull(q) for q in qpack_person(fio)]
        await asyncio.gather(*tasks)

        # –°–ú–ò-—Ñ–∏–ª—å—Ç—Ä
        news_snips = [(u, t) for u, t in all_snips if _is_news(u)]
        if not news_snips:
            return all_queries, [], "–°–≤–µ–∂–∏—Ö –∏–Ω—Ç–µ—Ä–≤—å—é –∏ –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        interviews = _dedupe([u for u, t in news_snips if INTERVIEW_PAT.search(u) or INTERVIEW_PAT.search(t)])
        news_urls  = _dedupe([u for u, _ in news_snips])

        ctx = base_ctx + "\n".join(f"URL:{u}\nTXT:{t}" for u, t in news_snips)[:16_000]
        sys = (
            "–¢—ã ‚Äî –∫–æ–Ω—Ç–µ–Ω—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫. –°–æ—Å—Ç–∞–≤—å –∫—Ä–∞—Ç–∫–∏–π –¥–∞–π–¥–∂–µ—Å—Ç –¢–û–õ–¨–ö–û –ø–æ —Å—Å—ã–ª–∫–∞–º –°–ú–ò.\n"
            "–°–µ–∫—Ü–∏–∏: 1) –ò–Ω—Ç–µ—Ä–≤—å—é; 2) –ù–æ–≤–æ—Å—Ç–∏ –æ –∫–æ–º–ø–∞–Ω–∏–∏.\n"
            "–î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—É–Ω–∫—Ç–∞: –¥–∞—Ç–∞ (–µ—Å–ª–∏ –≤–∏–¥–Ω–∞), 1‚Äì2 —Ñ—Ä–∞–∑—ã —Å—É—Ç–∏, —Å—Å—ã–ª–∫–∞. –ú–∞–∫—Å 8 –ø—É–Ω–∫—Ç–æ–≤ –Ω–∞ —Å–µ–∫—Ü–∏—é."
        )
        digest = await _gpt([
            {"role": "system", "content": sys},
            {"role": "user",   "content": ctx},
        ], model=self.model, T=0.18)

        # –ú–æ–∂–Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –≤–µ—Ä–Ω—É—Ç—å —Å—ã—Ä—ã–µ URL
        extra = {"interview_urls": interviews, "news_urls": news_urls}
        return all_queries, news_snips, _norm_ws(digest) + "\n\n" + json.dumps(extra, ensure_ascii=False)

    # ---------- 3) Orchestrator ----------
    async def _run_async(self):
        timeout = aiohttp.ClientTimeout(total=45)
        async with aiohttp.ClientSession(timeout=timeout) as sess:
            people, q_lead, s_lead = await self._leaders(sess)
            q_int,  s_int, digest  = await self._interviews(people, sess)

        # ‚ë† –í–ª–∞–¥–µ–ª—å—Ü—ã/—Ç–æ–ø-–º–µ–Ω–µ–¥–∂–µ—Ä—ã c –±–∏–æ–≥—Ä–∞—Ñ–∏—è–º–∏
        if people:
            blocks = []
            for p in people:
                block = p["name"]
                if p.get("bio"):   block += f"\n–ë–∏–æ–≥—Ä–∞—Ñ–∏—è:\n{p['bio']}"
                lf = (p.get("legal") or {})
                if any(lf.values()):
                    tags = []
                    if lf.get("prosecuted"): tags.append("–ø—Ä–µ—Å–ª–µ–¥—É–µ—Ç—Å—è")
                    if lf.get("imprisoned"): tags.append("–±—ã–ª(–∞) –≤ —Ç—é—Ä—å–º–µ/–∞—Ä–µ—Å—Ç")
                    if lf.get("sanctioned"): tags.append("–ø–æ–¥ —Å–∞–Ω–∫—Ü–∏—è–º–∏")
                    if tags: block += "\n‚öñÔ∏è –ü—Ä–∞–≤–æ–≤–æ–π —Å—Ç–∞—Ç—É—Å: " + ", ".join(tags)
                if p.get("news"): block += "\n–°–ú–ò-—Å—Å—ã–ª–∫–∏:\n" + "\n".join(p["news"][:6])
                photo = p.get("photo")
                block += f"\n–§–æ—Ç–æ: {photo or '–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}"
                blocks.append(block)
            owners_block = "\n\n".join(blocks)
        else:
            owners_block = "–¢–æ–ø-–º–µ–Ω–µ–¥–∂–µ—Ä—ã –∏ –≤–ª–∞–¥–µ–ª—å—Ü—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."

        # ‚ë° –ö–æ–Ω—Ç–∞–∫—Ç—ã
        contacts_block = ""
        cdata = self.cinfo.get("–ö–æ–Ω—Ç–∞–∫—Ç—ã") or {}
        if cdata:
            phones = ", ".join(cdata.get("–¢–µ–ª", []))
            emails = ", ".join(cdata.get("–ï–º—ç–π–ª", []))
            site   = cdata.get("–í–µ–±–°–∞–π—Ç") or ""
            lines  = []
            if phones: lines.append(f"–¢–µ–ª: {phones}")
            if emails: lines.append(f"E-mail: {emails}")
            if site:   lines.append(f"–°–∞–π—Ç: {site}")
            if lines:  contacts_block = "–ö–æ–Ω—Ç–∞–∫—Ç—ã:\n" + "\n".join(lines)

        # ‚ë¢ HTML
        body = "\n\n".join([part for part in (owners_block, contacts_block, digest) if part])
        summary_html = _linkify(body)

        return {
            "summary":  summary_html,
            "names":    people,
            "queries":  q_lead + q_int,
            "snippets": s_lead + s_int,
        }

    # ---------- sync wrapper ----------
    def run(self) -> dict:
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import nest_asyncio; nest_asyncio.apply()
                return loop.run_until_complete(self._run_async())
        except RuntimeError:
            pass
        return asyncio.run(self._run_async())

    # ---------- optional site context ----------
    async def _site_ctx(self, sess: aiohttp.ClientSession) -> str:
        if not getattr(self, "site", None):
            return ""
        loop = asyncio.get_running_loop()
        try:
            from functools import partial
            return await loop.run_in_executor(None, partial(_site_passport_sync, self.site))
        except Exception:
            return ""

        

def run_ai_insight_tab() -> None:
    st.title("üìä AI Company Insight")
    st.markdown("–í–≤–µ–¥–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ (–∫–∞–∂–¥–∞—è –∫–æ–º–ø–∞–Ω–∏—è ‚Äî –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ).")

    c1, c2, c3, c4 = st.columns(4)
    with c1: inns_raw  = st.text_area("–ò–ù–ù")
    with c2: names_raw = st.text_area("–ù–∞–∑–≤–∞–Ω–∏–µ")
    with c3: mkts_raw  = st.text_area("–†—ã–Ω–æ–∫")
    with c4: sites_raw = st.text_area("–°–∞–π—Ç")

    aggregate_mode = st.checkbox("üßÆ –°—É–º–º–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏–Ω–∞–Ω—Å—ã –ø–æ –≤—Å–µ–º –ò–ù–ù")
    blocks = st.multiselect(
        "–í—ã–±–µ—Ä–∏—Ç–µ –±–ª–æ–∫–∏ –æ—Ç—á—ë—Ç–∞",
        ["–û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏", "–†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç", "–†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é"],
        default=["–û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏", "–†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç", "–†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é"],
    )
    gen_doc = "–û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏" in blocks
    gen_market = "–†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç" in blocks
    gen_leads = "–†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é" in blocks

    if st.button("üîç –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –æ—Ç—á—ë—Ç", key="ai_build"):
        with st.spinner("–°—á–∏—Ç–∞–µ–º –æ—Ç—á—ë—Ç‚Ä¶"):

    
            # ---------- –ø–∞—Ä—Å–∏–Ω–≥ ----------
            split = lambda s: [i.strip() for i in s.splitlines() if i.strip()]
            inns   = split(inns_raw)
            names  = split(names_raw)
            mkts   = split(mkts_raw)
            sites  = split(sites_raw)
            
            # ---------- –≤–∞–ª–∏–¥–∞—Ü–∏—è ----------
            if not inns:
                st.error("–£–∫–∞–∂–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ò–ù–ù."); st.stop()
            
            if aggregate_mode:            # Œ£-—Ä–µ–∂–∏–º
                # –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–∞—Å—Ç—è–≥–∏–≤–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                if len(names) == 1 and len(inns) > 1:  names *= len(inns)
                if len(mkts)  == 1 and len(inns) > 1:  mkts  *= len(inns)
                if len(sites) == 1 and len(inns) > 1:  sites *= len(inns)
            
                # —Ç–µ–ø–µ—Ä—å –≤—Å—ë –ª–∏–±–æ –ø—É—Å—Ç–æ–µ, –ª–∏–±–æ —Å–æ–≤–ø–∞–¥–∞–µ—Ç –ø–æ –¥–ª–∏–Ω–µ
                for lst, lbl in [(names, "–ù–∞–∑–≤–∞–Ω–∏–µ"), (mkts, "–†—ã–Ω–æ–∫")]:
                    if lst and len(lst) != len(inns):
                        st.error(f"–ß–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ ¬´{lbl}¬ª –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 1 –∏–ª–∏ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —á–∏—Å–ª–æ–º –ò–ù–ù."); st.stop()
            
            else:                         # –æ–¥–∏–Ω–æ—á–Ω—ã–π —Ä–µ–∂–∏–º
                if not (names and mkts):
                    st.error("–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –ø–æ–ª—è ‚Äî –ò–ù–ù, –ù–∞–∑–≤–∞–Ω–∏–µ –∏ –†—ã–Ω–æ–∫."); st.stop()
                if len({len(inns), len(names), len(mkts)}) != 1:
                    st.error("–ß–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ –≤–æ –≤—Å–µ—Ö —Ç—Ä—ë—Ö –ø–æ–ª—è—Ö –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å."); st.stop()
                if sites and len(sites) != len(inns):
                    st.error("–ß–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ ¬´–°–∞–π—Ç¬ª –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —á–∏—Å–ª–æ–º –ò–ù–ù."); st.stop()
            
            # ---------- –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—ã —Å–ø–∏—Å–∫–æ–≤ ----------
            pad = lambda lst: lst if lst else [""] * len(inns)
            names_full = pad(names)
            mkts_full  = pad(mkts)
            sites_full = pad(sites)
            YEARS = ["2022", "2023", "2024"]
            df_companies = pd.DataFrame([ck_company(i) for i in inns])
            doc = None
            mkt_res = None
            lead_res = None

            def parse_people_cell(cell) -> list[str]:
                """
                –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —è—á–µ–π–∫–∏ ¬´–†—É–∫–æ–≤–æ–¥¬ª –∏–ª–∏ ¬´–£—á—Ä–µ–¥_–§–õ¬ª
                –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ ¬´–§–ò–û (–ò–ù–ù xxxx, –¥–æ–ª—è yy%)¬ª.
                –†–∞–±–æ—Ç–∞–µ—Ç –∏ –µ—Å–ª–∏ cell = NaN, '', —Å–ø–∏—Å–æ–∫, dict, —Å—Ç—Ä–æ–∫–∞-JSON.
                """
                # –ø—É—Å—Ç–æ / NaN
                if cell is None or (isinstance(cell, float) and pd.isna(cell)):
                    return []
            
                # –µ—Å–ª–∏ –ø—Ä–∏—à–ª–∞ —Å—Ç—Ä–æ–∫–∞ ‚Äî –ø—Ä–æ–±—É–µ–º –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –≤ –æ–±—ä–µ–∫—Ç
                if isinstance(cell, str):
                    cell = cell.strip()
                    if not cell:
                        return []
                    try:
                        cell = ast.literal_eval(cell)      # '[{‚Ä¶}]' -> python
                    except (ValueError, SyntaxError):
                        # –ø—Ä–æ—Å—Ç–æ —Å—Ç—Ä–æ–∫–∞ —Å –æ–¥–Ω–∏–º –§–ò–û
                        return [cell]
            
                # –æ–¥–∏–Ω–æ—á–Ω—ã–π dict
                if isinstance(cell, dict):
                    cell = [cell]
            
                # list
                if isinstance(cell, list):
                    out = []
                    for item in cell:
                        if isinstance(item, str):          # —É–∂–µ –≥–æ—Ç–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞
                            out.append(item.strip())
                        elif isinstance(item, dict):       # –Ω–∞—à –æ—Å–Ω–æ–≤–Ω–æ–π —Å–ª—É—á–∞–π
                            fio   = item.get("–§–ò–û") or ""
                            inn   = item.get("–ò–ù–ù") or ""
                            share = item.get("–î–æ–ª—è", {}).get("–ü—Ä–æ—Ü–µ–Ω—Ç")
                            line  = fio
                            if inn:
                                line += f" (–ò–ù–ù {inn}"

                                if share is not None:
                                    line += f", –¥–æ–ª—è {float(share):.1f}%)"
                                else:
                                    line += ")"
            
                                line += f", –¥–æ–ª—è {float(share):.1f}%)" if share is not None else ")"
  
                            out.append(line)
                    return [s for s in out if s]
                # fallback
                return [str(cell)]
            
            def row_people_json(row: pd.Series) -> dict:
                """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç {'leaders_raw': [...], 'founders_raw': [...]}."""
                # ‚îÄ‚îÄ 1. —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                leaders = parse_people_cell(row.get("–†—É–∫–æ–≤–æ–¥"))
            
                # ‚îÄ‚îÄ 2. —É—á—Ä–µ–¥–∏—Ç–µ–ª–∏: –∫–æ–ª–æ–Ω–∫–∞ '–£—á—Ä–µ–¥' ‚Üí dict ‚Üí –∫–ª—é—á '–§–õ' ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                founders_cell = None
                uc = row.get("–£—á—Ä–µ–¥")
                if isinstance(uc, dict):
                    founders_cell = uc.get("–§–õ")          # —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
                else:
                    founders_cell = uc                    # fallback (–µ—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –¥—Ä—É–≥–æ–π)
            
                founders = parse_people_cell(founders_cell)
            
                return {"leaders_raw": leaders, "founders_raw": founders}
            
            people_cols = df_companies.apply(row_people_json, axis=1, result_type="expand")
            df_companies = pd.concat([df_companies, people_cols], axis=1)

            

            
            PNL_CODES = [                       # –≤—Å—ë, —á—Ç–æ —Ö–æ—Ç–∏–º –≤–∏–¥–µ—Ç—å –≤ –¥–ª–∏–Ω–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ
                ("–í—ã—Ä—É—á–∫–∞ (‚ÇΩ –º–ª–Ω)",                "2110"),
                ("–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å –ø—Ä–æ–¥–∞–∂ (‚ÇΩ –º–ª–Ω)",   "2120"),
                ("–í–∞–ª–æ–≤–∞—è –ø—Ä–∏–±—ã–ª—å (‚ÇΩ –º–ª–Ω)",        "2200"),
                ("–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ —Ä–∞—Å—Ö–æ–¥—ã (‚ÇΩ –º–ª–Ω)",   "2210"),
                ("–£–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–∏–µ —Ä–∞—Å—Ö–æ–¥—ã (‚ÇΩ –º–ª–Ω)", "2220"),
                ("–ü—Ä–∏–±—ã–ª—å –æ—Ç –ø—Ä–æ–¥–∞–∂ (‚ÇΩ –º–ª–Ω)",      "2300"),
                ("–î–æ—Ö–æ–¥—ã –æ—Ç —É—á–∞—Å—Ç–∏—è (‚ÇΩ –º–ª–Ω)",      "2310"),
                ("–ü—Ä–æ—Ü–µ–Ω—Ç—ã –∫ –ø–æ–ª—É—á–µ–Ω–∏—é (‚ÇΩ –º–ª–Ω)",   "2320"),
                ("–ü—Ä–æ—Ü–µ–Ω—Ç—ã –∫ —É–ø–ª–∞—Ç–µ (‚ÇΩ –º–ª–Ω)",      "2330"),
                ("–ü—Ä–æ—á–∏–µ –¥–æ—Ö–æ–¥—ã (‚ÇΩ –º–ª–Ω)",          "2340"),
                ("–ü—Ä–æ—á–∏–µ —Ä–∞—Å—Ö–æ–¥—ã (‚ÇΩ –º–ª–Ω)",         "2350"),
                ("–ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å (‚ÇΩ –º–ª–Ω)",         "2400"),
                ("–°–æ–≤–æ–∫—É–ø–Ω—ã–π –¥–æ–ª–≥ (‚ÇΩ –º–ª–Ω)",        "_total_debt"),
                ("–î–µ–Ω–µ–∂–Ω—ã–µ —Å—Ä–µ–¥—Å—Ç–≤–∞ (‚ÇΩ –º–ª–Ω)",      "_cash"),
                ("–ö—Ä–µ–¥–∏—Ç–æ—Ä—Å–∫–∞—è –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç—å (‚ÇΩ –º–ª–Ω)", "1520"),
                ("–ß–∏—Å—Ç—ã–π –¥–æ–ª–≥ (‚ÇΩ –º–ª–Ω)",            "_net_debt"),
                ("EBIT margin (%)",                "_ebit_margin"),
                ("Net Debt / EBIT",                "_netdebt_ebit"),
            ]
            
            # ---------- ‚ë† —Å–≤–æ–¥–Ω–∞—è –≤–∫–ª–∞–¥–∫–∞, –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ ----------
            def build_agg_finances() -> dict[str, dict[str, float | None]]:
                """–°—É–º–º–∏—Ä—É–µ—Ç –≤—Å–µ –ò–ù–ù –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å agg[year][code]."""
                NUMERIC = {c for _, c in PNL_CODES if c.isdigit()} | {"1250", "1400", "1500"}
                raw = {y: defaultdict(float) for y in YEARS}
            
                for inn in inns:
                    fin = ck_fin(inn)
                    for y in YEARS:
                        for code in NUMERIC:
                            v = fin.get(y, {}).get(code)
                            if isinstance(v, (int, float)):
                                raw[y][code] += v / 1e6        # ‚Üí –º–ª–Ω
            
                agg = {}
                for y in YEARS:
                    rev   = raw[y]["2110"]
                    ebit  = raw[y]["2200"]
                    cash  = raw[y]["1250"]
                    debt  = raw[y]["1400"] + raw[y]["1500"]
                    pay   = raw[y]["1520"]
            
                    net_debt    = debt - cash - pay
                    ebit_margin = _safe_div(ebit, rev)
                    ebit_margin = ebit_margin * 100 if ebit_margin is not None else None
                    nd_ebit     = _safe_div(net_debt, ebit)
            
            
                    agg[y] = {**raw[y],
                              "_total_debt":  debt,
                              "_cash":        cash,
                              "_net_debt":    net_debt,
                              "_ebit_margin": ebit_margin,
                              "_netdebt_ebit":nd_ebit}
                return agg
            
            # —Å–æ–∑–¥–∞—ë–º –≤–∫–ª–∞–¥–∫–∏ –∑–∞—Ä–∞–Ω–µ–µ (—á—Ç–æ–±—ã –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è tabs –°–£–©–ï–°–¢–í–û–í–ê–õ–ê –≤—Å–µ–≥–¥–∞)
            if aggregate_mode:
                tabs = st.tabs(["Œ£ –°–≤–æ–¥–Ω–æ"] + ([] if len(inns) == 1 else
                                               [f"{n} ({inn})" for inn, n in zip(inns, names_full)]))
                # –±–ª–æ–∫ Œ£ –°–≤–æ–¥–Ω–æ ‚Äî –≤—Å–µ–≥–¥–∞ –ø–µ—Ä–≤—ã–π
                with tabs[0]:
                    agg = build_agg_finances()
            
                    st.header("Œ£ –°–≤–æ–¥–Ω–∞—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞")
                    tbl = pd.DataFrame({"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å": [n for n, _ in PNL_CODES]})
                    for y in YEARS:
                        tbl[y] = [agg[y].get(code) for _, code in PNL_CODES]
            
                    def _fmt(v, pct=False, d=1):
                        if v is None or (isinstance(v, float) and np.isnan(v)): return "‚Äî"
                        return f"{v:.{d}f}{'%' if pct else ''}".replace(".", ",")
                    for i, (nm, _) in enumerate(PNL_CODES):
                        pct  = nm.endswith("%")
                        digs = 2 if ("Net" in nm or pct) else 1
                        tbl.iloc[i, 1:] = [_fmt(v, pct, digs) for v in tbl.iloc[i, 1:]]
            
                    st.dataframe(tbl.set_index("–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å"),
                                 use_container_width=True,
                                 height=min(880, 40 * len(PNL_CODES)))
            
            
                    # –≥—Ä–∞—Ñ–∏–∫
                    # --- –≥—Ä–∞—Ñ–∏–∫: –≤—ã—Ä—É—á–∫–∞ / EBIT / —á–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å + EBIT-margin -----------
                    fig, ax1 = plt.subplots(figsize=(7, 3.5))
                    x = np.arange(len(YEARS)); w = 0.25
                    
                    bars_rev  = ax1.bar(x - w, [agg[y]["2110"] or 0 for y in YEARS],
                                        w, label="–í—ã—Ä—É—á–∫–∞")
                    bars_ebit = ax1.bar(x,     [agg[y]["2200"] or 0 for y in YEARS],
                                        w, label="EBIT")
                    bars_prof = ax1.bar(x + w, [agg[y]["2400"] or 0 for y in YEARS],
                                        w, label="–ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å")
                    
                    # –ø–æ–¥–ø–∏—Å–∏ –Ω–∞ —Å—Ç–æ–ª–±—Ü–∞—Ö
                    for b in (*bars_rev, *bars_ebit, *bars_prof):
                        h = b.get_height()
                        if h and not np.isnan(h):
                            ax1.annotate(f"{h:.1f}", xy=(b.get_x() + b.get_width()/2, h),
                                         xytext=(0, 3), textcoords="offset points",
                                         ha="center", fontsize=8)
                    
                    # –ª–∏–Ω–∏—è EBIT-margin (%)
                    ax2 = ax1.twinx()
                    margins = [agg[y]["_ebit_margin"] if agg[y]["_ebit_margin"] else np.nan for y in YEARS]
                    ax2.plot(x, margins, linestyle="--", marker="o", label="EBIT margin, %")
                    
                    # –ø–æ–¥–ø–∏—Å–∏ ¬´—Ö %¬ª –Ω–∞–¥ —Ç–æ—á–∫–∞–º–∏ –ª–∏–Ω–∏–∏
                    for xx, yy in zip(x, margins):
                        if not np.isnan(yy):
                            ax2.annotate(f"{yy:.1f}%", xy=(xx, yy),
                                         xytext=(0, 5), textcoords="offset points",
                                         ha="center", fontsize=8)
                    
                    # –æ–±—ä–µ–¥–∏–Ω—è–µ–º –ª–µ–≥–µ–Ω–¥—É
                    h1, l1 = ax1.get_legend_handles_labels()
                    h2, l2 = ax2.get_legend_handles_labels()
                    ax1.legend(h1 + h2, l1 + l2, loc="upper left", fontsize=9)
                    # ‚üµ  –ø—Ä—è—á–µ–º —à–∫–∞–ª—ã Y
                    ax1.set_yticks([]); ax2.set_yticks([])
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                    
                    for ax in (ax1, ax2):
                        ax.spines[:].set_visible(False)
                    
                    # –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ
                    ax1.set_xticks(x); ax1.set_xticklabels(YEARS, fontsize=10)
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                    for ax in (ax1, ax2):
                        ax.spines[:].set_visible(False)
                    
                    fig.tight_layout(pad=1.0)
                    st.pyplot(fig)
            
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ü—Ä–æ—Ñ–∏–ª—å / —Ä—ã–Ω–æ–∫ / —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ (+ –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    first_name = names_full[0] or "–ö–æ–º–ø–∞–Ω–∏—è"
                    first_mkt  = mkts_full[0]
                    first_site = sites_full[0]
                    first_inn = inns[0] if inns else None
                    
                    if gen_doc:
                        st.subheader("üìù –û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏")
                        with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏‚Ä¶"):
                            doc = RAG(first_name, website=first_site, market=first_mkt).run()

                        html_main = _linkify(doc["summary"]).replace("\n", "<br>")
                        st.markdown(
                            f"<div style='background:#F7F9FA;border:1px solid #ccc;"
                            f"border-radius:8px;padding:18px;line-height:1.55'>{html_main}</div>",
                            unsafe_allow_html=True,
                        )

                        with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                            for i, q in enumerate(doc["queries"], 1):
                                st.markdown(f"**{i}.** {q}")

                        with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                            st.dataframe(
                                pd.DataFrame(doc["snippets"], columns=["URL", "Snippet"]).head(15),
                                use_container_width=True,
                            )

                        if doc.get("site_pass"):
                            with st.expander("üåê –ü–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞"):
                                st.markdown(
                                    f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                                    f"border-radius:8px;padding:18px;line-height:1.55'>"
                                    f"{_linkify(doc['site_pass']).replace(chr(10), '<br>')}</div>",
                                    unsafe_allow_html=True,
                                )
                        else:
                            st.info("–ü–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ –Ω–µ –ø–æ–ª—É—á–µ–Ω (–Ω–µ—Ç URL, –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ –∏—Å—Ç–µ–∫ —Ç–∞–π-–∞—É—Ç).")
                    
                    if gen_market and first_mkt:
                        st.subheader("üìà –†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç")
                        with st.spinner("–°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ä—ã–Ω–∫—É –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑‚Ä¶"):
                            mkt_res = get_market_rag(first_mkt)

                        mkt_html = _linkify(mkt_res["summary"]).replace("\n", "<br>")
                        st.markdown(
                            f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                            f"border-radius:8px;padding:18px;line-height:1.55'>{mkt_html}</div>",
                            unsafe_allow_html=True,
                        )

                        rows, chart_rows = [], []
                        for line in mkt_res["summary"].splitlines():
                            m = re.match(r"\s*(\d{4})\s*[|:-]\s*(.+)", line)
                            if m:
                                year, val = m.groups()
                                rows.append((year, val.strip()))
                                num = re.search(r"[\d,.]+", val.replace(" ", ""))
                                if num:
                                    chart_rows.append((year, float(num.group(0).replace(",", ".")), val.strip()))
                        if rows:
                            df_tbl = pd.DataFrame(rows, columns=["–ì–æ–¥", "–û–±—ä—ë–º —Ä—ã–Ω–∫–∞"])
                            st.dataframe(df_tbl, use_container_width=True)
                            if chart_rows:
                                years = [r[0] for r in chart_rows]
                                nums  = [r[1] for r in chart_rows]
                                labels = [r[2] for r in chart_rows]
                                fig, ax = plt.subplots(figsize=(6, 3))
                                bars = ax.bar(years, nums)
                                for b, lab in zip(bars, labels):
                                    ax.annotate(lab, xy=(b.get_x()+b.get_width()/2, b.get_height()),
                                                xytext=(0,3), textcoords="offset points",
                                                ha="center", fontsize=8)
                                    
                                ax.set_yticks([])
                                for spine in ax.spines.values():
                                    spine.set_visible(False)
                                st.pyplot(fig)

                        with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                            for i, q in enumerate(mkt_res["queries"], 1):
                                st.markdown(f"**{i}.** {q}")

                        with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                            st.dataframe(
                                pd.DataFrame(mkt_res["snippets"], columns=["URL", "Snippet"]).head(15),
                                use_container_width=True,
                            )
                    
                    if gen_leads:
                        st.subheader("üë• –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é")
                        with st.spinner("–°–æ–±–∏—Ä–∞–µ–º —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏ –∏–Ω—Ç–µ—Ä–≤—å—é‚Ä¶"):
                            company_info = df_companies.iloc[0].to_dict()
                            lead_res = get_leaders_rag(
                                first_name,
                                website=first_site,
                                market=first_mkt,
                                company_info=company_info,
                            )

                        st.markdown(
                            f"<div style='background:#F9FAFB;border:1px solid #ddd;"
                            f"border-radius:8px;padding:18px;line-height:1.55'>"
                            f"{lead_res['summary'].replace(chr(10), '<br>')}</div>",
                            unsafe_allow_html=True,
                        )

                        with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                            for i, q in enumerate(lead_res["queries"], 1):
                                st.markdown(f"**{i}.** {q}")

                        with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                            if lead_res["snippets"]:
                                df = (
                                    pd.DataFrame(lead_res["snippets"], columns=["URL", "Snippet"])
                                    .drop_duplicates(subset="URL")
                                    .head(15)
                                )
                                st.dataframe(df, use_container_width=True)
                            else:
                                st.info("–°–Ω–∏–ø–ø–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
                    
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –∫–æ–Ω–µ—Ü –±–ª–æ–∫–∞, –¥–∞–ª—å—à–µ –≤–∞—à –∫–æ–¥ (–µ—Å–ª–∏ –±—ã–ª) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            
            # ---------- ‚ë° –≤–∫–ª–∞–¥–∫–∏ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –∫–æ–º–ø–∞–Ω–∏—è–º ----------
            if aggregate_mode and len(inns) > 1:
                tabs = st.tabs(["Œ£ –°–≤–æ–¥–Ω–æ"] + [f"{n} ({inn})"
                                               for inn, n in zip(inns, names_full)])
            else:                                   # –æ–¥–∏–Ω–æ—á–Ω—ã–π —Ä–µ–∂–∏–º
                tabs = st.tabs([f"{n} ({inn})" for inn, n
                                in zip(inns, names_full)])
            
            start_idx = 1 if (aggregate_mode and len(inns) > 1) else 0
            
            for idx, (tab, inn, name, mkt, site) in enumerate(
                    zip(
                        tabs[start_idx:],   # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º Œ£-–≤–∫–ª–∞–¥–∫—É –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                        inns,
                        names_full,
                        mkts_full,
                        sites_full,
                    )
            ):
                with tab:
                    st.header(f"{name} ‚Äî {inn}")
                    st.caption(f"–†—ã–Ω–æ–∫: **{mkt or '‚Äî'}**")
            
                    # ---------- –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –ø—Ä–æ—Ñ–∏–ª—å ----------
                    fin = ck_fin(inn)
                    calc = {y: {} for y in YEARS}
            
                    for y in YEARS:
                        yr = fin.get(y, {})
                        # –ø—Ä—è–º—ã–µ —Å—Ç—Ä–æ–∫–∏ –æ—Ç—á—ë—Ç–∞
                        for _, code in PNL_CODES:
                            if code.isdigit():
                                v = yr.get(code)
                                calc[y][code] = (v / 1e6) if isinstance(v, (int, float)) else None
            
                        # —Ä–∞—Å—á—ë—Ç–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
                        cash  = yr.get("1250", 0) / 1e6
                        debt  = (yr.get("1400", 0) + yr.get("1500", 0)) / 1e6
                        pay   = yr.get("1520", 0) / 1e6
                        rev   = calc[y].get("2110")
                        ebit  = calc[y].get("2200")
                        net_debt = debt - cash - pay
                        ebit_m = _safe_div(ebit, rev)
                        ebit_m = ebit_m * 100 if ebit_m is not None else None
                        nd_eb     = _safe_div(net_debt, ebit)
            
                        calc[y].update({
                            "_total_debt": debt or None,
                            "_cash":       cash or None,
                            "_net_debt":   net_debt if rev is not None else None,
                            "_ebit_margin":ebit_m,
                            "_netdebt_ebit":nd_eb,
                        })
            
                    # --- –¥–ª–∏–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ ---
                    tbl = pd.DataFrame({"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å": [n for n, _ in PNL_CODES]})
                    for y in YEARS:
                        tbl[y] = [calc[y].get(code) for _, code in PNL_CODES]
            
                    def fmt(v, pct=False, d=1):
                        if v is None or (isinstance(v, float) and np.isnan(v)): return "‚Äî"
                        return f"{v:.{d}f}{'%' if pct else ''}".replace(".", ",")
            
                    for i, (nm, _) in enumerate(PNL_CODES):
                        pct  = nm.endswith("%")
                        digs = 2 if ("Net" in nm or pct) else 1
                        tbl.iloc[i, 1:] = [fmt(v, pct, digs) for v in tbl.iloc[i, 1:]]
            
                    st.dataframe(tbl.set_index("–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å"),
                                 use_container_width=True,
                                 height=min(880, 40 * len(tbl)))
            
                    # --- –≥—Ä–∞—Ñ–∏–∫: –≤—ã—Ä—É—á–∫–∞ / EBIT / —á–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å + EBIT-margin ---
                    fig, ax1 = plt.subplots(figsize=(7, 3.5))
                    x = np.arange(len(YEARS)); w = 0.25
                    bars_r  = ax1.bar(x - w, [calc[y]["2110"] or 0 for y in YEARS], w, label="–í—ã—Ä—É—á–∫–∞")
                    bars_e  = ax1.bar(x,     [calc[y]["2200"] or 0 for y in YEARS], w, label="EBIT")
                    bars_p  = ax1.bar(x + w, [calc[y]["2400"] or 0 for y in YEARS], w, label="–ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å")
            
                    for b in (*bars_r, *bars_e, *bars_p):
                        h = b.get_height()
                        if h and not np.isnan(h):
                            ax1.annotate(f"{h:.1f}", xy=(b.get_x()+b.get_width()/2, h),
                                         xytext=(0,3), textcoords="offset points",
                                         ha="center", fontsize=8)
            
                    ax2 = ax1.twinx()
                    m_vals = [calc[y]["_ebit_margin"] if calc[y]["_ebit_margin"] else np.nan for y in YEARS]
                    ax2.plot(x, m_vals, linestyle="--", marker="o", label="EBIT margin, %")
                    # ----- –µ–¥–∏–Ω–∞—è –ª–µ–≥–µ–Ω–¥–∞ -----
                    h1, l1 = ax1.get_legend_handles_labels()   # bars
                    h2, l2 = ax2.get_legend_handles_labels()   # –ª–∏–Ω–∏—è margin
                    ax1.legend(h1 + h2, l1 + l2, loc='upper left', fontsize=9)
            
            
                    
                    for xx, yy in zip(x, m_vals):
                        if yy and not np.isnan(yy):
                            ax2.annotate(f"{yy:.1f}%", xy=(xx, yy),
                                         xytext=(0,5), textcoords="offset points",
                                         ha="center", fontsize=8)
            
                    ax1.set_xticks(x); ax1.set_xticklabels(YEARS, fontsize=10)
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                     # ‚üµ  –ø—Ä—è—á–µ–º —à–∫–∞–ª—ã Y
                    ax1.set_yticks([]); ax2.set_yticks([])
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                    
                    for ax in (ax1, ax2):
                        ax.spines[:].set_visible(False)
                    for ax in (ax1, ax2): ax.spines[:].set_visible(False)
                    ax1.legend(loc="upper left", fontsize=9)
                    fig.tight_layout(pad=1.0)
                    st.pyplot(fig)
            
                    
                    
                    if gen_doc:
                        st.subheader("üìù –û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏")
                        with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏‚Ä¶"):
                            doc = RAG(name, website=site, market=mkt).run()

                        main_html = _linkify(doc["summary"]).replace("\n", "<br>")
                        st.markdown(
                            f"<div style='background:#F7F9FA;border:1px solid #ccc;"
                            f"border-radius:8px;padding:18px;line-height:1.55'>{main_html}</div>",
                            unsafe_allow_html=True
                        )

                        with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                            for i, q in enumerate(doc["queries"], 1):
                                st.markdown(f"**{i}.** {q}")

                        with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                            st.dataframe(
                                pd.DataFrame(doc["snippets"], columns=["URL", "Snippet"]).head(15),
                                use_container_width=True,
                            )

                        if doc.get("site_pass"):
                            with st.expander("üåê –ü–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞"):
                                st.markdown(
                                    f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                                    f"border-radius:8px;padding:18px;line-height:1.55'>"
                                    f"{_linkify(doc['site_pass']).replace(chr(10), '<br>')}</div>",
                                    unsafe_allow_html=True,
                                )
                        else:
                            st.info("–ü–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ –Ω–µ –ø–æ–ª—É—á–µ–Ω (–Ω–µ—Ç URL, –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ –∏—Å—Ç–µ–∫ —Ç–∞–π-–∞—É—Ç).")
                    
                    if gen_market and mkt:
                        st.subheader("üìà –†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç")
                        with st.spinner("–°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ä—ã–Ω–∫—É –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑‚Ä¶"):
                            mkt_res = get_market_rag(mkt)

                        mkt_html = _linkify(mkt_res["summary"]).replace("\n", "<br>")
                        st.markdown(
                            f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                            f"border-radius:8px;padding:18px;line-height:1.55'>{mkt_html}</div>",
                            unsafe_allow_html=True,
                        )

                        with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                            for i, q in enumerate(mkt_res["queries"], 1):
                                st.markdown(f"**{i}.** {q}")

                        with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                            st.dataframe(
                                pd.DataFrame(mkt_res["snippets"], columns=["URL", "Snippet"]).head(15),
                                use_container_width=True,
                            )
                    
                    if gen_leads:
                        st.subheader("üë• –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é")
                        with st.spinner("–°–æ–±–∏—Ä–∞–µ–º —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏ –∏–Ω—Ç–µ—Ä–≤—å—é‚Ä¶"):
                            company_info = {
                                "leaders_raw":  df_companies.loc[idx, "leaders_raw"]  or [],
                                "founders_raw": df_companies.loc[idx, "founders_raw"] or [],
                            }
                            lead_res = get_leaders_rag(
                                name,
                                website=site,
                                market=mkt,
                                company_info=company_info,
                            )

                        st.markdown(
                            f"<div style='background:#F9FAFB;border:1px solid #ddd;"
                            f"border-radius:8px;padding:18px;line-height:1.55'>"
                            f"{lead_res['summary'].replace(chr(10), '<br>')}</div>",
                            unsafe_allow_html=True,
                        )

                        with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                            for i, q in enumerate(lead_res["queries"], 1):
                                st.markdown(f"**{i}.** {q}")

                        with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                            if lead_res["snippets"]:
                                df = (
                                    pd.DataFrame(lead_res["snippets"], columns=["URL", "Snippet"])
                                    .drop_duplicates(subset="URL")
                                    .head(15)
                                )
                                st.dataframe(df, use_container_width=True)
                            else:
                                st.info("–°–Ω–∏–ø–ø–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")

        report = {"tbl": tbl, "graphics": fig}
        if doc is not None:
            report["doc"] = doc
        if mkt_res is not None:
            report["mkt_res"] = mkt_res
        if lead_res is not None:
            report["lead_res"] = lead_res
        st.session_state["ai_report"] = report
        st.session_state["ai_result_ready"] = True

def long_job(total_sec: int = 180, key_prog: str = "ai_prog"):
    """–§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞, –∫–∞–∂–¥—ã–µ 1 —Å –æ–±–Ω–æ–≤–ª—è–µ—Ç progress –≤ session_state."""
    for i in range(total_sec + 1):
        time.sleep(1)
        st.session_state[key_prog] = i / total_sec     # 0 ‚Ä¶ 1
    st.session_state["ai_done"] = True                 # –æ—Ç—á—ë—Ç –≥–æ—Ç–æ–≤

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 2. UI-—Ñ—É–Ω–∫—Ü–∏–∏ –¥–≤—É—Ö –≤–∫–ª–∞–¥–æ–∫
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def run_advance_eye_tab() -> None:
    st.header("üëÅÔ∏è Advance Eye")

    user_query = st.text_input("–í–≤–µ–¥–∏—Ç–µ –ò–ù–ù –∏–ª–∏ –§–ò–û")
    if st.button("üîç –ù–∞–π—Ç–∏ –∫–æ–Ω—Ç–∞–∫—Ç—ã") and user_query:
        with st.spinner("–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º Dyxless‚Ä¶"):
            res = dyxless_query(user_query, token=DYXLESS_TOKEN, max_rows=20_000)

        if res.get("status"):
            st.success(f"–ü–æ–∫–∞–∑–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: **{res['counts']}**")
            st.json(res["data"] or {"note": "–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"})
        else:
            st.warning(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {res.get('error', '–ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç')}")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ (–æ–¥–∏–Ω —Ä–∞–∑ –∑–∞ —Å–µ—Å—Å–∏—é)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.session_state.setdefault("ai_prog", None)   # float 0‚Ä¶1 –∏–ª–∏ None
st.session_state.setdefault("ai_done", False)  # –æ—Ç—á—ë—Ç –≥–æ—Ç–æ–≤?

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 4. –î–≤–µ –≤–∫–ª–∞–¥–∫–∏
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
tab_ai, tab_eye = st.tabs(["üìä AI-Insight", "üëÅÔ∏è Advance Eye"])

# === –≤–∫–ª–∞–¥–∫–∞ 1: AI-Insight =========================================

with tab_ai:
    run_ai_insight_tab()       # –≤—Å—è –ª–æ–≥–∏–∫–∞ –≤–Ω—É—Ç—Ä–∏

with tab_eye:
    run_advance_eye_tab()      # –ø–æ–∏—Å–∫ Dyxless
