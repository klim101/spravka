#!/usr/bin/env python
# coding: utf-8

# In[ ]:


#!/usr/bin/env python
# coding: utf-8

# In[6]:


# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º API-–∫–ª—é—á OpenAI
import os
import requests
import pandas as pd
import re
import numpy as np
import requests
from bs4 import BeautifulSoup
import openai
from typing import List, Dict, Any, Tuple
import json
import streamlit as st
from collections import defaultdict
import asyncio, aiohttp, re, textwrap, nest_asyncio, openai, tiktoken
from collections import defaultdict
from urllib.parse import urlparse
import tldextract, re, asyncio, aiohttp                      # –µ—Å–ª–∏ tldextract —É–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω ‚Äì —ç—Ç—É —Å—Ç—Ä–æ–∫—É –º–æ–∂–Ω–æ –∫–æ—Ä–æ—á–µ
from functools import partial  
import threading
import time
import functools
import ast
KEYS = {
    "OPENAI_API_KEY": st.secrets["OPENAI_API_KEY"],
    "GOOGLE_API_KEY": st.secrets["GOOGLE_API_KEY"],
    "GOOGLE_CX":      st.secrets["GOOGLE_CX"],
    "CHECKO_API_KEY": st.secrets["CHECKO_API_KEY"],
    "DYXLESS_TOKEN": st.secrets["DYXLESS_TOKEN"],
    "SONAR_API_KEY": st.secrets["SONAR_API_KEY"],
}

DYXLESS_TOKEN = KEYS["DYXLESS_TOKEN"]


# ‚îÄ‚îÄ –û–±—â–∏–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã (–µ–¥–∏–Ω—ã–µ –¥–ª—è –≤—Å–µ–≥–æ —Ñ–∞–π–ª–∞)
HEADERS = {"User-Agent": "Mozilla/5.0 (Win64) AppleWebKit/537.36 Chrome/125 Safari/537.36"}
_URL_PAT = re.compile(r"https?://[^\s)<>\"']+", flags=re.I)

# –î–≤–µ —á—ë—Ç–∫–∏–µ –≤–µ—Ä—Å–∏–∏ linkify, —á—Ç–æ–±—ã –Ω–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞—Ç—å:
def linkify_as_word(text: str, label: str = "—Å—Å—ã–ª–∫–∞") -> str:
    """–ó–∞–º–µ–Ω—è–µ—Ç URL –Ω–∞ <a>label</a> (–∫—Ä–∞—Ç–∫–∞—è –≤–µ—Ä—Å–∏—è)."""
    if not isinstance(text, str):
        text = "" if text is None else str(text)
    return _URL_PAT.sub(lambda m: f'<a href="{html.escape(m.group(0))}" target="_blank" rel="noopener">{label}</a>', text)

def linkify_keep_url(text: str) -> str:
    """–ó–∞–º–µ–Ω—è–µ—Ç URL –Ω–∞ <a>—Å–∞–º URL</a> (–ø–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è)."""
    if not isinstance(text, str):
        text = "" if text is None else str(text)
    return _URL_PAT.sub(lambda m: f'<a href="{html.escape(m.group(0))}" target="_blank" rel="noopener">{html.escape(m.group(0))}</a>', text)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ app.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import os, re, asyncio, aiohttp, requests, nest_asyncio, logging
import streamlit as st
import pandas as pd, numpy as np, matplotlib.pyplot as plt

nest_asyncio.apply()
logging.basicConfig(level=logging.WARNING)
import os, re, html, textwrap, asyncio, logging, nest_asyncio
import aiohttp, requests, streamlit as st
import pandas as pd, numpy as np, matplotlib.pyplot as plt
import tldextract, openai


import html, re

_URL_PAT = re.compile(r"https?://[^\s)]+", flags=re.I)

def _linkify(text) -> str:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç URL –≤ <a ‚Ä¶>—Å—Å—ã–ª–∫–∞</a>."""
    if not isinstance(text, str):                      # << –≥–ª–∞–≤–Ω–æ–µ
        text = "" if text is None else str(text)

    def repl(m):
        u = html.escape(m.group(0))
        return f'<a href="{u}" target="_blank">—Å—Å—ã–ª–∫–∞</a>'
    return _URL_PAT.sub(repl, text)





# ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
async def _site_snippet(domain: str) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—ã–π Google-—Å–Ω–∏–ø–ø–µ—Ç –¥–ª—è site:domain (–∏–ª–∏ '')."""
    if not domain:
        return ""
    async with aiohttp.ClientSession() as sess:
        q = f"site:{domain}"
        snips = await _google(sess, q, n=1)
    return snips[0][1] if snips else ""



@st.cache_data(ttl=3_600, show_spinner=False)
def dyxless_query(query: str,
                  token: str,
                  max_rows: int = 20_000) -> Dict[str, Any]:
    """
    –û–±—ë—Ä—Ç–∫–∞ Dyxless. –ï—Å–ª–∏ –∑–∞–ø–∏—Å–µ–π > max_rows ‚Äì –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ max_rows,
    –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–∏—à–µ–º truncated=True –∏ original_counts.
    """
    url = "https://api-dyxless.cfd/query"
    try:
        r = requests.post(url, json={"query": query, "token": token}, timeout=15)
        r.raise_for_status()
        res = r.json()

        if res.get("status") and isinstance(res.get("data"), list):
            full = len(res["data"])
            if full > max_rows:
                res["data"] = res["data"][:max_rows]
                res.update(truncated=True, original_counts=full, counts=max_rows)
        return res
    except requests.RequestException as e:
        return {"status": False, "error": str(e)}












# ‚ï≠‚îÄüîß  –≤—Å–ø–æ–º–æ–≥–∞–ª–∫–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
_BAD = ("vk.com", "facebook.", ".pdf", ".jpg", ".png")
HEADERS = {"User-Agent": "Mozilla/5.0 (Win64) AppleWebKit/537.36 Chrome/125 Safari/537.36"}
def _bad(u: str) -> bool: return any(b in u.lower() for b in _BAD)

async def _google(sess, q, n=3):
    q = re.sub(r'[\"\'‚Äú‚Äù]', " ", q)[:80]
    params = {"key": KEYS["GOOGLE_API_KEY"], "cx": KEYS["GOOGLE_CX"],
              "q": q, "num": n, "hl": "ru", "gl": "ru"}
    async with sess.get("https://www.googleapis.com/customsearch/v1",
                         params=params, headers=HEADERS, timeout=8) as r:
        if r.status != 200:
            logging.warning(f"Google error {r.status}")
            return []
        js = await r.json()
        return [(i["link"], i.get("snippet", "")) for i in js.get("items", [])
                if not _bad(i["link"])]






async def _gpt(messages, *, model="gpt-4o-mini", T=0.2):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ OpenAI ChatCompletion ‚Üí str."""
    chat = await openai.ChatCompletion.acreate(
        model=model, temperature=T, messages=messages)
    return chat.choices[0].message.content.strip()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –æ—Å–Ω–æ–≤–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞ (–≤ —Å—Ç–∏–ª–µ –≤–∞—à–µ–≥–æ RAG) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class SiteRAG:
    """
    url        ‚Äì –∞–¥—Ä–µ—Å —Å–∞–π—Ç–∞ (–º–æ–∂–Ω–æ –±–µ–∑ http/https)
    max_chunk  ‚Äì –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ–¥–Ω–æ–≥–æ –∫—É—Å–∫–∞ HTML, –∫–æ—Ç–æ—Ä—ã–π —É–π–¥—ë—Ç LLM
    summary    ‚Äì –∏—Ç–æ–≥–æ–≤—ã–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç –∫–æ–º–ø–∞–Ω–∏–∏
    chunks_out ‚Äì —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—é–º–µ c –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏ HTML
    html_size  ‚Äì —Ä–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ HTML-—Ñ–∞–π–ª–∞, bytes
    """
    def __init__(self, url: str, *, model="gpt-4o-mini",
                 max_chunk: int = 6_000, T: float = 0.18):
        if not url.startswith(("http://", "https://")):
            url = "http://" + url
        self.url       = url
        self.model     = model
        self.max_chunk = max_chunk
        self.T         = T

    # ---------- 1. —Å–∫–∞—á–∏–≤–∞–µ–º HTML ------------------------------------------------
    async def _fetch(self) -> str:
        h = {"User-Agent": "Mozilla/5.0"}
        async with aiohttp.ClientSession(headers=h) as sess:
            async with sess.get(self.url, timeout=20) as r:
                if r.status == 200 and "text/html" in r.headers.get("Content-Type", ""):
                    return await r.text("utf-8", errors="ignore")
                raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å {self.url} (status={r.status})")

    # ---------- 2. –¥–µ–ª–∏–º HTML –Ω–∞ ¬´–±–µ–∑–æ–ø–∞—Å–Ω—ã–µ¬ª –∫—É—Å–∫–∏ -----------------------------
    def _split(self, html_raw: str) -> list[str]:
        # –ø—Ä–æ–±—É–µ–º —Ä–µ–∑–∞—Ç—å –ø–æ –∫—Ä—É–ø–Ω—ã–º —Ç–µ–≥–∞–º, —á—Ç–æ–±—ã –∫—É—Å–∫–∏ –±—ã–ª–∏ —Å–≤—è–∑–Ω—ã
        body = re.split(r"</?(?:body|div|section|article)[^>]*>", html_raw, flags=re.I)
        chunks, buf = [], ""
        for part in body:
            if len(buf) + len(part) > self.max_chunk:
                chunks.append(buf); buf = part
            else:
                buf += part
        if buf:
            chunks.append(buf)
        return chunks

    # ---------- 3. map-—Ñ–∞–∑–∞: –∫–æ–Ω—Å–ø–µ–∫—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∫—É—Å–æ–∫ -------------------------
    async def _summarise_chunk(self, n: int, total: int, chunk: str) -> str:
        sys = (
            "–¢—ã ‚Äì –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –ü—Ä–æ—á–∏—Ç–∞–π –¥–∞–Ω–Ω—ã–π HTML-—Ñ—Ä–∞–≥–º–µ–Ω—Ç –∏ "
            "–≤—ã–ø–∏—à–∏ –í–°–ï –∑–Ω–∞—á–∏–º—ã–µ —Ñ–∞–∫—Ç—ã –æ –∫–æ–º–ø–∞–Ω–∏–∏ (–ø—Ä–æ–¥—É–∫—Ç—ã, —É—Å–ª—É–≥–∏, –∏—Å—Ç–æ—Ä–∏—è, "
            "–≥–µ–æ–≥—Ä–∞—Ñ–∏—è, –∫–ª–∏–µ–Ω—Ç—ã, —Ü–∏—Ñ—Ä—ã, –∫–æ–º–∞–Ω–¥–∞, –∫–æ–Ω—Ç–∞–∫—Ç—ã –∏ –ø—Ä.). "
            "–£–¥–∞–ª–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏—é/footer/—Å–∫—Ä–∏–ø—Ç—ã. –°–æ—Ö—Ä–∞–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞–±–∑–∞—Ü–∞–º–∏."
        )
        return await _gpt([
            {"role": "system", "content": sys},
            {"role": "user",
             "content": f"HTML_CHUNK_{n}/{total} (len={len(chunk):,}):\n{chunk}"}],
            model=self.model, T=self.T)

    # ---------- 4. reduce-—Ñ–∞–∑–∞: –¥–µ–ª–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç ------------------------
    async def _summarise_overall(self, parts: list[str]) -> str:
        sys = (
            "–ù–∏–∂–µ —É–∂–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–Ω—Å–ø–µ–∫—Ç—ã —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–µ–π —Å–∞–π—Ç–∞. "
            "–ù–∞ –∏—Ö –æ—Å–Ω–æ–≤–µ —Å–æ—Å—Ç–∞–≤—å –æ–¥–∏–Ω –ü–û–õ–ù–´–ô –∏ —Å–≤—è–∑–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç –∫–æ–º–ø–∞–Ω–∏–∏: "
            "‚Ä¢ –∫—Ç–æ –æ–Ω–∏ –∏ —á–µ–º –∑–∞–Ω–∏–º–∞—é—Ç—Å—è; ‚Ä¢ –ø—Ä–æ–¥—É–∫—Ç—ã / —É—Å–ª—É–≥–∏; ‚Ä¢ —Ä—ã–Ω–æ–∫ –∏ –∫–ª–∏–µ–Ω—Ç—ã; "
            "‚Ä¢ –∏—Å—Ç–æ—Ä–∏—è –∏ –∫–ª—é—á–µ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è; ‚Ä¢ –≥–µ–æ–≥—Ä–∞—Ñ–∏—è –∏ –º–∞—Å—à—Ç–∞–±—ã; "
            "‚Ä¢ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ / –∫–æ–º–∞–Ω–¥–∞; ‚Ä¢ –ª—é–±—ã–µ —Ü–∏—Ñ—Ä—ã –∏ —Ñ–∞–∫—Ç—ã; "
            "‚Ä¢ –≤—ã–≤–æ–¥ –æ –ø–æ–∑–∏—Ü–∏–∏ –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞—Ö. –ù–∏—á–µ–≥–æ –≤–∞–∂–Ω–æ–≥–æ –Ω–µ —É–ø—É—Å—Ç–∏."
        )
        merged = "\n\n".join(parts)
        return await _gpt([
            {"role": "system", "content": sys},
            {"role": "user",   "content": merged}],
            model=self.model, T=self.T)

    # ---------- orchestrator ----------------------------------------------------
    async def _run_async(self):
        html_raw = await self._fetch()
        chunks   = self._split(html_raw)

        # map
        part_summaries = []
        for idx, ch in enumerate(chunks, 1):
            print(f"‚Üí LLM chunk {idx}/{len(chunks)} ‚Ä¶")
            part_summaries.append(await self._summarise_chunk(idx, len(chunks), ch))

        # reduce
        summary_final = await self._summarise_overall(part_summaries)

        return {"summary":    summary_final,
                "chunks_out": part_summaries,
                "html_size":  f"{len(html_raw):,} bytes",
                "url":        self.url}

    # ---------- –ø—É–±–ª–∏—á–Ω—ã–π —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å ----------------------------------
    def run(self) -> dict:
        loop = asyncio.get_event_loop()
        if loop and loop.is_running():
            return loop.run_until_complete(self._run_async())
        return asyncio.run(self._run_async())




# ---------- helper: —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –¥–æ—Å—Ç–∞—ë–º –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ -----------------
def _site_passport_sync(url: str, *, max_chunk: int = 6_000) -> str:
    """–í—ã–∑—ã–≤–∞–µ—Ç SiteRAG(url).run() –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ summary."""
    try:
        return SiteRAG(url, max_chunk=max_chunk).run()["summary"]
    except Exception as exc:
        return f"[site passport error: {exc}]"

# ‚ï≠‚îÄüßæ  INVEST SNAPSHOT (cheap, 1 call) ‚Äî –∞–¥—Ä–µ—Å–∞ –º–æ—â–Ω–æ—Å—Ç–µ–π, —Å–æ—Ü—Å–µ—Ç–∏, –Ω–æ–≤–æ—Å—Ç–∏, –∏–Ω—Ç–µ—Ä–≤—å—é, –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã, headcount ‚îÄ‚ïÆ
import json, requests, re
from typing import Optional

API_URL_INVEST = "https://api.perplexity.ai/chat/completions"

class PPLXError(Exception):
    ...

def _pplx_call_invest(
    prompt: str,
    model: str = "sonar",            # –¥–µ—à—ë–≤–∞—è –º–æ–¥–µ–ª—å
    recency: Optional[str] = None,   # None = —à–∏—Ä–æ–∫–∏–π –æ—Ö–≤–∞—Ç (5 –ª–µ—Ç)
    temperature: float = 0.0,
    max_tokens: int = 1500,
    timeout: int = 60,
) -> str:
    key = (os.getenv("SONAR_API_KEY") or os.getenv("PPLX_API_KEY") or os.getenv("PERPLEXITY_API_KEY") or "").strip()
    assert key.startswith("pplx-"), "–£—Å—Ç–∞–Ω–æ–≤–∏ SONAR_API_KEY = pplx-..."
    headers = {"Authorization": f"Bearer {key}", "Content-Type": "application/json", "Accept": "application/json"}
    payload = {
        "model": model,
        "messages": [
            {"role":"system","content":(
                "–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ M&A. –û—Ö–≤–∞—Ç 5 –ª–µ—Ç. –°—Ç—Ä–æ–≥–æ —Ñ–∞–∫—Ç—ã –∏–∑ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. "
                "–ó–∞–ø—Ä–µ—â–µ–Ω–æ —É–ø–æ–º–∏–Ω–∞—Ç—å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ (–≤—ã—Ä—É—á–∫–∞, –ø—Ä–∏–±—ã–ª—å, EBITDA –∏ —Ç.–ø.), "
                "–ò–ù–ù/–û–ì–†–ù –∏ –ª—é–±—ã–µ –≤—ã–≤–æ–¥—ã –Ω–∞ –∏—Ö –æ—Å–Ω–æ–≤–µ. –ú–æ–∂–Ω–æ —É–ø–æ–º–∏–Ω–∞—Ç—å –∏–º–µ–Ω–∞ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤/—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π "
                "–¢–û–õ–¨–ö–û –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏—Ö –∏–Ω—Ç–µ—Ä–≤—å—é. "
                "–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ –ø—É–Ω–∫—Ç—É –Ω–µ—Ç ‚Äî –ø–∏—à–∏ '–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö'. –ù–µ –ø–æ–≤—Ç–æ—Ä—è–π –æ–¥–Ω—É –∏ —Ç—É –∂–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö. "
                "–í –∫–æ–Ω—Ü–µ —É–∫–∞–∂–∏ –ø—Ä—è–º—ã–µ URL –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."
            )},
            {"role":"user","content": prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    if recency in {"hour","day","week","month","year"}:
        payload["search_recency_filter"] = recency

    r = requests.post(API_URL_INVEST, headers=headers, json=payload, timeout=timeout)
    if r.status_code != 200:
        try: err = r.json()
        except Exception: err = {"error": r.text}
        raise PPLXError(f"HTTP {r.status_code}: {json.dumps(err, ensure_ascii=False)[:900]}")
    return r.json()["choices"][0]["message"]["content"]

# —Ñ–∏–ª—å—Ç—Ä –∑–∞–ø—Ä–µ—Ç–Ω—ã—Ö —Ç–µ–º: —Ñ–∏–Ω–ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –∏ —Ä–µ–≥.–Ω–æ–º–µ—Ä–∞ (—Ä–∞–∑—Ä–µ—à–∞–µ–º –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤ —Ç–æ–ª—å–∫–æ –≤ –∏–Ω—Ç–µ—Ä–≤—å—é)
_FORBIDDEN_INVEST = re.compile(
    r"(\b–≤—ã—Ä—É—á–∫|\b–ø—Ä–∏–±—ã–ª|\bebit(?:da)?\b|–º–∞—Ä–∂–∏–Ω–∞–ª|—Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω|—Ñ–∏–Ω–∞–Ω—Å|\b–∏–Ω–Ω\b|\b–æ–≥—Ä–Ω\b|—É—Å—Ç–∞–≤–Ω\w*\s+–∫–∞–ø–∏—Ç–∞–ª)",
    re.IGNORECASE
)

def _dedup_lines_invest(text: str) -> str:
    """–£–¥–∞–ª–∏—Ç—å —Ç–æ—á–Ω—ã–µ –ø–æ–≤—Ç–æ—Ä—ã —Å—Ç—Ä–æ–∫ –∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è URL –≤ —Å—Ç—Ä–æ–∫–∞—Ö, —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Ä—è–¥–æ–∫."""
    seen_lines, seen_urls, out = set(), set(), []
    url_re = re.compile(r'https?://\S+')
    for raw in text.splitlines():
        ln = raw.strip()
        if not ln:
            if out and out[-1] != "":
                out.append("")
            continue
        urls = url_re.findall(ln)
        for u in urls:
            if u in seen_urls:
                ln = ln.replace(u, "")
            else:
                seen_urls.add(u)
        ln = re.sub(r'\s{2,}', ' ', ln).strip()
        if ln and ln not in seen_lines:
            out.append(ln); seen_lines.add(ln)
    cleaned = []
    for i, ln in enumerate(out):
        if ln == "" and (i == 0 or (i+1 < len(out) and out[i+1] == "")):
            continue
        cleaned.append(ln)
    return "\n".join(cleaned).strip()

def sanitize_invest(text: str) -> str:
    """–£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å —Ñ–∏–Ω–ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º–∏/–ò–ù–ù/–û–ì–†–ù/—É—Å—Ç–∞–≤–Ω—ã–º –∫–∞–ø–∏—Ç–∞–ª–æ–º –∏ —á–∏—Å—Ç–∏–º –ø–æ–≤—Ç–æ—Ä—ã."""
    keep = []
    for ln in text.splitlines():
        if _FORBIDDEN_INVEST.search(ln):
            continue
        keep.append(ln)
    return _dedup_lines_invest("\n".join(keep))

def build_invest_prompt(company: str, site_hint: Optional[str] = None) -> str:
    site = f"\n–í–æ–∑–º–æ–∂–Ω—ã–π –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç: {site_hint}." if site_hint else ""
    return f"""
–°–¥–µ–ª–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á—ë—Ç –ø–æ –∫–æ–º–ø–∞–Ω–∏–∏ ¬´{company}¬ª –Ω–∞ —Ä—É—Å—Å–∫–æ–º (–æ—Ö–≤–∞—Ç: 5 –ª–µ—Ç).{site}
–§–æ—Ä–º–∞—Ç —Å—Ç—Ä–æ–≥–æ Markdown —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ —É—Ä–æ–≤–Ω—è ### –∏ –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –∞–±–∑–∞—Ü–∞–º–∏ (–±–µ–∑ —Å–ø–∏—Å–∫–æ–≤). –ù–µ –¥—É–±–ª–∏—Ä—É–π —Ñ–∞–∫—Ç—ã –º–µ–∂–¥—É —Ä–∞–∑–¥–µ–ª–∞–º–∏.

### –ü—Ä–æ—Ñ–∏–ª—å
1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: —á—Ç–æ –∑–∞ –∫–æ–º–ø–∞–Ω–∏—è, —á–µ–º –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è (–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–æ–≤–∞—Ä–æ–≤/—É—Å–ª—É–≥), –≥–µ–æ–≥—Ä–∞—Ñ–∏—è. –ï—Å–ª–∏ –µ—Å—Ç—å –æ–¥–Ω–æ–∏–º—ë–Ω–Ω—ã–µ —Ñ–∏—Ä–º—ã ‚Äî –∫—Ä–∞—Ç–∫–æ —É–∫–∞–∂–∏ –¥–∏–∑–∞–º–±–∏–≥—É–∞—Ü–∏—é –ø–æ –ø—Ä–æ—Ñ–∏–ª—é/—Å–∞–π—Ç—É.

### –ë–∏–∑–Ω–µ—Å-–º–æ–¥–µ–ª—å
–ö–∞–∫ –∫–æ–º–ø–∞–Ω–∏—è –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç: –∫–∞–Ω–∞–ª—ã (—Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–¥–∞–∂–∏/–¥–∏—Å—Ç—Ä–∏–±—É—Ü–∏—è/–æ–ø—Ç/—Ä–æ–∑–Ω–∏—Ü–∞/–º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å—ã/–¥–∏–ª–µ—Ä—ã), —Å–µ—Ä–≤–∏—Å–Ω—ã–µ –º–æ–¥–µ–ª–∏, –ø–æ–¥–ø–∏—Å–∫–∏/—Å–µ—Ä–≤–∏—Å–Ω—ã–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏. –ë–µ–∑ —Ü–∏—Ñ—Ä –∏ –æ—Ü–µ–Ω–æ–∫ ‚Äî —Ç–æ–ª—å–∫–æ —è–≤–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å).

### –ê–∫—Ç–∏–≤—ã –∏ –ø–ª–æ—â–∞–¥–∫–∏
–ê–¥—Ä–µ—Å–∞ –≤—Å–µ—Ö –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ (–æ—Ñ–∏—Å—ã, —Å–∫–ª–∞–¥—ã, –†–¶, –º–∞–≥–∞–∑–∏–Ω—ã/–ü–í–ó). –ü–ª–æ—â–∞–¥–∏ (–º¬≤) –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (–∫–æ–ª-–≤–æ –†–¶/—Å–∫–ª–∞–¥–æ–≤, —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ/–∞—Ä–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ) ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä—è–º–æ —É–∫–∞–∑–∞–Ω–æ –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö.

### –ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–æ—â–Ω–æ—Å—Ç–∏ –∏ –∞–¥—Ä–µ—Å–∞
–£–∫–∞–∂–∏ –Ω–∞–ª–∏—á–∏–µ/–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞. –ü—Ä–∏–≤–µ–¥–∏ –ê–î–†–ï–°–ê –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–ª–æ—â–∞–¥–æ–∫/—Ü–µ—Ö–æ–≤/–∫–æ–º–±–∏–Ω–∞—Ç–æ–≤ –∏ –ø–æ –∫–∞–∂–¥–æ–π ‚Äî —á—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è. –ú–æ—â–Ω–æ—Å—Ç–∏ (–µ–¥./–º¬≤/—Ç–æ–Ω–Ω/–º–µ—Å—è—Ü) –∏ —Å—Ç–µ–ø–µ–Ω—å –∑–∞–≥—Ä—É–∑–∫–∏ ‚Äî –µ—Å–ª–∏ —Ä–∞—Å–∫—Ä—ã—Ç—ã. –õ–æ–∫–∞—Ü–∏–∏ –ª–æ–≥–∏—Å—Ç–∏–∫–∏ –∏ —Å–∫–ª–∞–¥—Å–∫–æ–π —Å–µ—Ç–∏ ‚Äî –∫—Ä–∞—Ç–∫–æ.

### –ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–ª–∞–Ω—ã –∏ –ø—Ä–æ–µ–∫—Ç—ã
–ó–∞—è–≤–ª–µ–Ω–Ω—ã–µ/–æ–∂–∏–¥–∞–µ–º—ã–µ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –≤ –Ω–æ–≤—ã–µ –ª–∏–Ω–∏–∏/—Å–∫–ª–∞–¥—ã/–†–¶/–ø–ª–æ—â–∞–¥–∫–∏; —Å—Ä–æ–∫–∏ –∏ —Å—Ç–∞—Ç—É—Å. –°—Å—ã–ª–∫–∏ –Ω–∞ –ø–µ—Ä–≤–æ–∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ —Å–∫–æ–±–∫–∞—Ö.

### –ö–ª–∏–µ–Ω—Ç—ã –∏ –∫–∞–Ω–∞–ª—ã —Å–±—ã—Ç–∞
B2B/B2C; —Å–µ–≥–º–µ–Ω—Ç—ã/–≤–µ—Ä—Ç–∏–∫–∞–ª–∏; –ø—Ä–æ–¥–∞–∂–∏ —á–µ—Ä–µ–∑ —Å–∞–π—Ç/–º–∞–≥–∞–∑–∏–Ω—ã/–º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å—ã/–¥–∏–ª–µ—Ä–æ–≤; –æ—Ä–∏–µ–Ω—Ç–∏—Ä—ã –ø–æ —á–∏—Å–ª—É –∫–ª–∏–µ–Ω—Ç–æ–≤ ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—É–±–ª–∏—á–Ω–æ –∏ —Å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º.

### –ß–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å –ø–µ—Ä—Å–æ–Ω–∞–ª–∞
–ß–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤/—à—Ç–∞—Ç–∞ (–µ—Å–ª–∏ –ø—É–±–ª–∏—á–Ω–æ), –¥–∞—Ç–∞/–ø–µ—Ä–∏–æ–¥ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫ –≤ —Å–∫–æ–±–∫–∞—Ö. –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ—Ü–µ–Ω–∫–∏ ‚Äî –ø–µ—Ä–µ–¥–∞–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞.

### –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã (–†–æ—Å—Å–∏—è)
–ü–µ—Ä–µ—á–∏—Å–ª–∏ 5‚Äì12 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –ø–æ –ø—Ä–æ—Ñ–∏–ª—é –±–∏–∑–Ω–µ—Å–∞; —Ñ–æ—Ä–º–∞—Ç: –ù–∞–∑–≤–∞–Ω–∏–µ (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç) –≤ –æ–¥–Ω–æ–º –∞–±–∑–∞—Ü–µ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é. –¢–æ–ª—å–∫–æ –∫–æ–º–ø–∞–Ω–∏–∏, —Ä–µ–∞–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞—é—â–∏–µ –≤ –†–§ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –ª–µ—Ç.

### –ù–æ–≤–æ—Å—Ç–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –ª–µ—Ç)
–î–∞–π 5‚Äì12 –∑–Ω–∞—á–∏–º—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –æ –∫–æ–º–ø–∞–Ω–∏–∏: ¬´–ó–∞–≥–æ–ª–æ–≤–æ–∫¬ª ‚Äî URL (–¥–∞—Ç–∞). –í –æ–¥–Ω–æ–º –∞–±–∑–∞—Ü–µ; –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π —Å—Å—ã–ª–∫–∏.

### –ò–Ω—Ç–µ—Ä–≤—å—é (–≤–ª–∞–¥–µ–ª—å—Ü—ã/—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ)
–î–∞–π 3‚Äì8 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤—å—é/–ø—É–±–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤: ¬´–°–ø–∏–∫–µ—Ä ‚Äî –ó–∞–≥–æ–ª–æ–≤–æ–∫/—Ç–µ–º–∞¬ª ‚Äî URL (–¥–∞—Ç–∞). –í –æ–¥–Ω–æ–º –∞–±–∑–∞—Ü–µ. –ù–µ —Ä–∞—Å–∫—Ä—ã–≤–∞–π –¥–æ–ª–∏/—Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤–ª–∞–¥–µ–Ω–∏—è, —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç –∏–Ω—Ç–µ—Ä–≤—å—é.

### –¶–∏—Ñ—Ä–æ–≤—ã–µ –∫–∞–Ω–∞–ª—ã –∏ –∫–æ–Ω—Ç–∞–∫—Ç—ã
–°–∞–π—Ç (–µ—Å–ª–∏ –µ—Å—Ç—å), e-mail/—Ç–µ–ª–µ—Ñ–æ–Ω—ã (–µ—Å–ª–∏ –ø—É–±–ª–∏—á–Ω–æ), –í–°–ï –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–æ—Ü—Å–µ—Ç–∏ (VK, Telegram, YouTube, RuTube, OK, Instagram*, Facebook*, LinkedIn), –∫–∞—Ç–∞–ª–æ–≥–∏/–∫–∞—Ä—Ç—ã (2–ì–ò–°, –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç—ã, Google Maps) ‚Äî –ø—Ä–∏–≤–æ–¥–∏ –ü–†–Ø–ú–´–ï URL.

### –ò—Å—Ç–æ—á–Ω–∏–∫–∏
–ü–µ—Ä–µ—á–∏—Å–ª–∏ –≤—Å–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä—è–º—ã–µ URL —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é, –±–µ–∑ –Ω—É–º–µ—Ä–∞—Ü–∏–∏.

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
‚Äî –ù–µ —É–∫–∞–∑—ã–≤–∞–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–≤—ã—Ä—É—á–∫–∞/–ø—Ä–∏–±—ã–ª—å/EBITDA –∏ —Ç.–ø.), –ò–ù–ù/–û–ì–†–ù/—É—Å—Ç–∞–≤–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª –∏ –≤—ã–≤–æ–¥—ã –Ω–∞ –∏—Ö –æ—Å–Ω–æ–≤–µ.
‚Äî –°—Ç—Ä–æ–≥–æ –∏–∑–±–µ–≥–∞–π –ø–æ–≤—Ç–æ—Ä–æ–≤ –º–µ–∂–¥—É —Ä–∞–∑–¥–µ–ª–∞–º–∏ –∏ –ø–æ–≤—Ç–æ—Ä–æ–≤ —Å—Å—ã–ª–æ–∫.
‚Äî –¢–æ–ª—å–∫–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ–º—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤; –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî '–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö'.
""".strip()

def invest_snapshot(company: str, site_hint: Optional[str] = None,
                    model: str = "sonar", recency: Optional[str] = None,
                    max_tokens: int = 1500) -> str:
    prompt = build_invest_prompt(company, site_hint=site_hint)
    raw = _pplx_call_invest(prompt, model=model, recency=recency, max_tokens=max_tokens)
    return sanitize_invest(raw)

@st.cache_data(ttl=86_400, show_spinner="üìù –°–æ–±–∏—Ä–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏‚Ä¶")
def get_invest_snapshot(company: str,
                        site_hint: Optional[str] = None,
                        model: str = "sonar",
                        recency: Optional[str] = None,
                        max_tokens: int = 1500) -> dict:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict: {'md': markdown_text, 'raw': raw_text_for_debug}
    """
    try:
        md = invest_snapshot(company, site_hint=site_hint, model=model, recency=recency, max_tokens=max_tokens)
        return {"md": md, "raw": md}
    except PPLXError as e:
        return {"md": f"_–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å INVEST SNAPSHOT: {e}_", "raw": ""}
# ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ





# ‚ï≠‚îÄüë• –ò–Ω—Ç–µ—Ä–≤—å—é: –æ–±–æ–≥–∞—â–∞–µ–º INVEST SNAPSHOT –ª–∏—Ü–∞–º–∏ –∏–∑ Checko (Sonar-only) ‚îÄ‚ïÆ
#   ‚Ä¢ –≠—Ç–∞–ø 1 ‚Äî –∏–Ω—Ç–µ—Ä–≤—å—é –ø–æ –§–ò–û –∏–∑ Checko
#   ‚Ä¢ –≠—Ç–∞–ø 2 ‚Äî –µ—Å–ª–∏ –Ω–µ—Ç/–º–∞–ª–æ –§–ò–û ‚Üí discovery –§–ò–û —á–µ—Ä–µ–∑ Sonar
#   ‚Ä¢ –ü–æ–¥–º–µ–Ω–∞ —Å–µ–∫—Ü–∏–∏ "### –ò–Ω—Ç–µ—Ä–≤—å—é (–≤–ª–∞–¥–µ–ª—å—Ü—ã/—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ)" –≤ –≥–æ—Ç–æ–≤–æ–º Markdown
# –¢—Ä–µ–±—É–µ—Ç: _pplx_call_invest, sanitize_invest –∏–∑ –±–ª–æ–∫–∞ INVEST SNAPSHOT
# ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ
import re, html
from typing import Optional

def _clean_person(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s*\(.*?\)\s*$", "", s)            # —É–±–∏—Ä–∞–µ–º —Ö–≤–æ—Å—Ç –≤ —Å–∫–æ–±–∫–∞—Ö: (–ò–ù–ù‚Ä¶, –¥–æ–ª—è‚Ä¶)
    s = re.sub(r"\s{2,}", " ", s)
    return s

def _names_from_checko(company_info: dict | None) -> list[str]:
    if not isinstance(company_info, dict):
        return []
    raw = []
    for key in ("leaders_raw", "founders_raw"):
        v = company_info.get(key) or []
        if isinstance(v, list):
            raw.extend([str(x) for x in v if x])
        elif isinstance(v, str):
            raw.append(v)
    out, seen = [], set()
    for p in raw:
        fio = _clean_person(p)
        k = fio.lower()
        if fio and k not in seen:
            seen.add(k); out.append(fio)
    return out

def _domain_from_site(site_hint: str | None) -> str:
    if not site_hint:
        return ""
    m = re.search(r"^(?:https?://)?([^/]+)", site_hint.strip(), re.I)
    return (m.group(1) if m else "").lower()

def _extract_urls(text: str) -> list[str]:
    return list(dict.fromkeys(re.findall(r'https?://[^\s<>)"\'\]]+', text or "")))

def _build_people_discovery_prompt(company: str,
                                   site_hint: str | None,
                                   market: str | None) -> str:
    dom = _domain_from_site(site_hint)
    mkt = f"(—Ä—ã–Ω–æ–∫: {market}). " if market else ""
    site_line = f"–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç (–µ—Å–ª–∏ –≤–µ—Ä–Ω–æ): {site_hint}. " if site_hint else ""
    return f"""
–ù–∞–π–¥–∏ –¥–µ–π—Å—Ç–≤—É—é—â–∏—Ö —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏/–∏–ª–∏ –æ—Å–Ω–æ–≤–∞—Ç–µ–ª–µ–π –∫–æ–º–ø–∞–Ω–∏–∏ ¬´{company}¬ª. {mkt}{site_line}
–û—Ö–≤–∞—Ç 5 –ª–µ—Ç. –¢–æ–ª—å–∫–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã —Å –ü–†–Ø–ú–´–ú–ò URL.

–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ ‚Äî —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏:
PERSON: <–§–ò–û> ‚Äî <–¥–æ–ª–∂–Ω–æ—Å—Ç—å/—Ä–æ–ª—å> ‚Äî <–ø—Ä—è–º–æ–π URL –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫>

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
‚Äî –ù–µ —É–∫–∞–∑—ã–≤–∞—Ç—å –ò–ù–ù/–û–ì–†–ù, –¥–æ–ª–∏, —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤–ª–∞–¥–µ–Ω–∏—è –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏.
‚Äî –ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç{(' ('+dom+')' if dom else '')}, –°–ú–ò, –ø—Ä–æ—Ñ–∏–ª—å–Ω—ã–µ –º–µ–¥–∏–∞, –≤–∏–¥–µ–æ/–ø–æ–¥–∫–∞—Å—Ç—ã, —Å–æ—Ü—Å–µ—Ç–∏ –∫–æ–º–ø–∞–Ω–∏–∏.
‚Äî –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî –≤—ã–≤–µ–¥–∏ ¬´PERSON: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö¬ª.
""".strip()

def _parse_people_lines(text: str) -> list[str]:
    if not text:
        return []
    ppl = []
    for ln in text.splitlines():
        m = re.match(r"\s*PERSON:\s*(.+?)\s+‚Äî\s+.+?\s+‚Äî\s+https?://", ln.strip(), re.I)
        if m:
            fio = _clean_person(m.group(1))
            if fio:
                ppl.append(fio)
    return list(dict.fromkeys(ppl))

def _build_interviews_prompt(company: str,
                             names: list[str],
                             site_hint: str | None,
                             market: str | None) -> str:
    names_block = "; ".join(names[:10]) or "‚Äî"
    site_line = f"–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç: {site_hint}. " if site_hint else ""
    mkt = f"(—Ä—ã–Ω–æ–∫: {market})" if market else ""
    return f"""
–¢—ã ‚Äî –º–µ–¥–∏–∞-–∞–Ω–∞–ª–∏—Ç–∏–∫. –ù–∞–π–¥–∏ –∏–Ω—Ç–µ—Ä–≤—å—é/–ø—É–±–ª–∏—á–Ω—ã–µ —Ä–∞–∑–≥–æ–≤–æ—Ä—ã –ø–æ –ª—é–¥—è–º [{names_block}] –∏–∑ –∫–æ–º–ø–∞–Ω–∏–∏ ¬´{company}¬ª {mkt}.
{site_line}–û—Ö–≤–∞—Ç 5 –ª–µ—Ç. –¢–æ–ª—å–∫–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ–º—ã–µ —Ñ–∞–∫—Ç—ã –∏ –ü–†–Ø–ú–´–ï URL. –ù–∏–∫–∞–∫–∏—Ö –ò–ù–ù/–û–ì–†–ù/—Ñ–∏–Ω–∞–Ω—Å–æ–≤.

–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ ‚Äî –û–î–ò–ù –∞–±–∑–∞—Ü (–±–µ–∑ —Å–ø–∏—Å–∫–æ–≤):
¬´–§–ò–û ‚Äî –ø–ª–æ—â–∞–¥–∫–∞/–∏–∑–¥–∞–Ω–∏–µ ‚Äî –∫—Ä–∞—Ç–∫–∞—è —Å—É—Ç—å (1 —Ñ—Ä–∞–∑–∞) ‚Äî URL (YYYY-MM-DD)¬ª;
–∑–∞–ø–∏—Å–∏ —Ä–∞–∑–¥–µ–ª—è–π —Ç–æ—á–∫–æ–π —Å –∑–∞–ø—è—Ç–æ–π ¬´;¬ª, –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π —Å—Å—ã–ª–∫–∏. –í—Å–µ–≥–æ 3‚Äì8 –∑–∞–ø–∏—Å–µ–π.
–í –∫–æ–Ω—Ü–µ –∞–±–∑–∞—Ü–∞ —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª –¥–æ–±–∞–≤—å: ¬´–ò—Å—Ç–æ—á–Ω–∏–∫–∏: <URL1>, <URL2>, ...¬ª (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ).
""".strip()

_SEC_INTERV_RE = re.compile(r"(^|\n)###\s*–ò–Ω—Ç–µ—Ä–≤—å—é[^\n]*\n.*?(?=\n###\s|\Z)",
                            flags=re.S | re.I)

def _replace_interviews_section(md: str, new_paragraph: str) -> str:
    block = f"\n### –ò–Ω—Ç–µ—Ä–≤—å—é (–≤–ª–∞–¥–µ–ª—å—Ü—ã/—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ)\n{new_paragraph.strip()}\n"
    if _SEC_INTERV_RE.search(md or ""):
        return _SEC_INTERV_RE.sub(block, md, count=1)
    # –µ—Å–ª–∏ —Ä–∞–∑–¥–µ–ª–∞ –Ω–µ—Ç ‚Äî –≤—Å—Ç–∞–≤–∏–º –ø–µ—Ä–µ–¥ ¬´–¶–∏—Ñ—Ä–æ–≤—ã–µ –∫–∞–Ω–∞–ª—ã¬ª –∏–ª–∏ –≤ –∫–æ–Ω–µ—Ü
    m = re.search(r"(^|\n)###\s*–¶–∏—Ñ—Ä–æ–≤—ã–µ\s+–∫–∞–Ω–∞–ª—ã[^\n]*", md or "", flags=re.I)
    if m:
        return md[:m.start()] + block + md[m.start():]
    return (md or "").rstrip() + block

def interviews_from_checko_sonar(company: str,
                                 company_info: dict | None = None,
                                 site_hint: str | None = None,
                                 market: str | None = None,
                                 max_people_discovery: int = 6) -> tuple[list[str], str]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (names, paragraph_markdown).
    names ‚Äî —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –§–ò–û, paragraph_markdown ‚Äî –æ–¥–∏–Ω –∞–±–∑–∞—Ü —Å –∏–Ω—Ç–µ—Ä–≤—å—é.
    """
    # 1) –∏–º–µ–Ω–∞ –∏–∑ Checko
    names = _names_from_checko(company_info)

    # 2) –µ—Å–ª–∏ –∏–º—ë–Ω –Ω–µ—Ç/–º–∞–ª–æ ‚Äî discovery —á–µ—Ä–µ–∑ Sonar
    if len(names) < 2:
        try:
            p_disc = _build_people_discovery_prompt(company, site_hint, market)
            raw = _pplx_call_invest(p_disc, model="sonar", recency=None, max_tokens=900)
            discovered = _parse_people_lines(raw)
        except Exception:
            discovered = []
        for fio in discovered:
            if fio.lower() not in {n.lower() for n in names}:
                names.append(fio)
        names = names[:max_people_discovery] or ["–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"]

    # 3) –∏–Ω—Ç–µ—Ä–≤—å—é –ø–æ –∏—Ç–æ–≥–æ–≤–æ–º—É —Å–ø–∏—Å–∫—É
    try:
        p_int = _build_interviews_prompt(company, names, site_hint, market)
        digest = _pplx_call_invest(p_int, model="sonar", recency=None, max_tokens=1400)
    except Exception as e:
        digest = f"–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö (–æ—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ç–µ—Ä–≤—å—é: {e})"

    # –¥–µ–¥—É–ø URL –≤–Ω—É—Ç—Ä–∏ –∞–±–∑–∞—Ü–∞
    def _urls_in(t: str) -> list[str]: return _extract_urls(t)
    seen = set(); parts = []
    for part in re.split(r"\s*;\s*", (digest or "").strip()):
        u = next(iter(_urls_in(part)), None)
        if not u or u not in seen:
            parts.append(part.strip())
            if u: seen.add(u)
    paragraph = "; ".join(parts)
    paragraph = sanitize_invest(paragraph)   # —Ñ–∏–ª—å—Ç—Ä —Ñ–∏–Ω–∞–Ω—Å–æ–≤/–ò–ù–ù/–û–ì–†–ù

    return names, paragraph

def invest_snapshot_enriched(company: str,
                             site_hint: Optional[str] = None,
                             company_info: dict | None = None,
                             market: str | None = None,
                             model: str = "sonar",
                             recency: Optional[str] = None,
                             max_tokens: int = 1500) -> str:
    """
    –°—Ç—Ä–æ–∏—Ç –æ–±—ã—á–Ω—ã–π INVEST SNAPSHOT, –ø–æ—Ç–æ–º –∑–∞–º–µ–Ω—è–µ—Ç/–≤—Å—Ç–∞–≤–ª—è–µ—Ç —Å–µ–∫—Ü–∏—é ¬´–ò–Ω—Ç–µ—Ä–≤—å—é¬ª
    –º–∞—Ç–µ—Ä–∏–∞–ª–æ–º, —Å–æ–±—Ä–∞–Ω–Ω—ã–º –Ω–∞ –±–∞–∑–µ Checko (+ Sonar discovery –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏).
    """
    base_md = invest_snapshot(company, site_hint=site_hint, model=model,
                              recency=recency, max_tokens=max_tokens)
    _, paragraph = interviews_from_checko_sonar(company,
                                                company_info=company_info,
                                                site_hint=site_hint,
                                                market=market)
    return _replace_interviews_section(base_md, paragraph)

@st.cache_data(ttl=86_400, show_spinner="üìù –°–æ–±–∏—Ä–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ (enriched)‚Ä¶")
def get_invest_snapshot_enriched(company: str,
                                 site_hint: Optional[str] = None,
                                 company_info: dict | None = None,
                                 market: str | None = None,
                                 model: str = "sonar",
                                 recency: Optional[str] = None,
                                 max_tokens: int = 1500) -> dict:
    try:
        md = invest_snapshot_enriched(company, site_hint=site_hint,
                                      company_info=company_info, market=market,
                                      model=model, recency=recency, max_tokens=max_tokens)
        return {"md": md, "raw": md}
    except PPLXError as e:
        return {"md": f"_–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å INVEST SNAPSHOT (enriched): {e}_", "raw": ""}







class RAG:
    """
    summary    ‚Äì —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç (Google-—Å–Ω–∏–ø–ø–µ—Ç—ã + –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞)
    queries    ‚Äì –∑–∞–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ LLM —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞ –¥–ª—è Google
    snippets   ‚Äì —Å–ø–∏—Å–æ–∫ (url, text) –∏–∑ Google
    news_snippets ‚Äì —Å–Ω–∏–ø–ø–µ—Ç—ã —Å –∫—Ä—É–ø–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
    site_ctx   ‚Äì –∫–æ—Ä–æ—Ç–∫–∏–π —Å–Ω–∏–ø–ø–µ—Ç ¬´site:<–¥–æ–º–µ–Ω> ‚Ä¶¬ª
    site_pass  ‚Äì –ø–æ–¥—Ä–æ–±–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ (–≥–æ—Ç–æ–≤—ã–π summary –æ—Ç SiteRAG)
    """
    def __init__(self, company: str, *, website: str = "", market: str = "",
                 years=(2022, 2023, 2024), country: str = "–†–æ—Å—Å–∏—è",
                 steps: int = 3, snips: int = 4,
                 llm_model: str = "gpt-4o-mini",company_info: dict | None = None,):
        self.company   = company.strip()
        self.website   = website.strip()
        self.market    = market.strip()
        self.country   = country
        self.years     = years
        self.steps     = steps
        self.snips     = snips
        self.llm_model = llm_model
        self.company_info = company_info or {}

    # ---------- site-snippet –∏–∑ Google ---------------------------------
    async def _site_ctx(self) -> str:
        dom = tldextract.extract(self.website).registered_domain if self.website else ""
        snip = await _site_snippet(dom)
        if snip:
            return f"{snip}\n—Ä—ã–Ω–æ–∫ –∫–æ–º–ø–∞–Ω–∏–∏ ‚Äì {self.market}" if self.market else snip
        return f"—Ä—ã–Ω–æ–∫ –∫–æ–º–ø–∞–Ω–∏–∏ ‚Äì {self.market}" if self.market else ""

    # ---------- GPT ‚Üí –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã --------------------------------
    async def _queries(self, hist="") -> list[str]:
        dom  = tldextract.extract(self.website).registered_domain if self.website else ""
        base = f'"{self.company}"' + (f' OR site:{dom}' if dom else "")
        sys  = (
            "–¢–´ ‚Äî –û–ü–´–¢–ù–´–ô –ò–°–°–õ–ï–î–û–í–ê–¢–ï–õ–¨ –†–´–ù–ö–û–í –ò –î–ê–ù–ù–´–•. –°–§–û–†–ú–£–õ–ò–†–£–ô –ù–ï –ú–ï–ù–ï–ï 30 –ü–†–û–°–¢–´–• –†–ê–ó–ù–û–û–ë–†–ê–ó–ù–´–• GOOGLE-–ó–ê–ü–†–û–°–û–í –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï, "
            f"–ü–û–ó–í–û–õ–Ø–Æ–©–ò–• –°–û–ë–†–ê–¢–¨ –ò–ù–§–û–†–ú–ê–¶–ò–Æ –û –ö–û–ú–ü–ê–ù–ò–ò ¬´{self.company}¬ª –ù–ê –†–´–ù–ö–ï ¬´{self.market}¬ª "
            f"({self.country}, {', '.join(map(str, self.years))}).\n"
            "–ö–ê–ñ–î–´–ô –ó–ê–ü–†–û–° –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –î–û–õ–ñ–ï–ù –°–û–î–ï–†–ñ–ê–¢–¨ –ù–ê–ó–í–ê–ù–ò–ï –ö–û–ú–ü–ê–ù–ò–ò.\n"
            "### –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –ë–õ–û–ö–ò\n"
            "1. –û–ü–ò–°–ê–ù–ò–ï –ö–û–ú–ü–ê–ù–ò–ò –ò –ë–†–ï–ù–î–´.\n"
            "2. –ß–ò–°–õ–ï–ù–ù–û–°–¢–¨ –°–û–¢–†–£–î–ù–ò–ö–û–í.\n"
            "3. –ü–†–û–ò–ó–í–û–î–°–¢–í–ï–ù–ù–´–ï –ú–û–©–ù–û–°–¢–ò.\n"
            "4. –ò–ù–í–ï–°–¢–ò–¶–ò–ò –ò –†–ê–°–®–ò–†–ï–ù–ò–Ø.\n"
            "5. –ê–î–†–ï–°–ê –®–¢–ê–ë-–ö–í–ê–†–¢–ò–†–´ –ò –ü–†–û–ò–ó–í–û–î–°–¢–í.\n"
            "6. –°–û–¶–ò–ê–õ–¨–ù–´–ï –°–ï–¢–ò.\n"
            "7. –ò–°–¢–û–†–ò–Ø.\n"
            "8. –ü–†–ò–ë–´–õ–¨ –ò –û–ë–™–Å–ú–´ –ü–†–û–î–£–ö–¶–ò–ò.\n"
            "9. –ö–û–ù–ö–£–†–ï–ù–¢–´ (–ù–ê–ó–í–ê–ù–ò–ï –ò –°–ê–ô–¢).\n"
            "10. –£–ü–û–ú–ò–ù–ê–ù–ò–Ø –ù–ê –§–û–†–£–ú–ê–• –ò –í –†–ï–ô–¢–ò–ù–ì–ê–•.\n"
            "–ü–û –ö–ê–ñ–î–û–ú–£ –ë–õ–û–ö–£ –°–î–ï–õ–ê–ô –ù–ï–°–ö–û–õ–¨–ö–û –†–ê–ó–ù–´–• –ó–ê–ü–†–û–°–û–í.\n"
            "### –°–û–í–ï–¢–´ –ü–û –ö–û–ù–°–¢–†–£–ö–¶–ò–ò –ó–ê–ü–†–û–°–û–í\n"
            "- –ò–°–ü–û–õ–¨–ó–£–ô –û–ü–ï–†–ê–¢–û–†–´: `site:`, `intitle:`, `inurl:`, `filetype:pdf`, `OR`.\n"
            "- –î–û–ë–ê–í–õ–Ø–ô –ì–û–î–´ –ò –ù–ê–ó–í–ê–ù–ò–Ø –ü–†–û–î–£–ö–¢–û–í –ò –ë–†–ï–ù–î–û–í, –ï–°–õ–ò –ù–£–ñ–ù–û.\n"
            f"- –î–õ–Ø –û–§–ò–¶–ò–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–• –ü–†–ò–ú–ï–ù–Ø–ô `site:{dom}` –ò–õ–ò –°–ê–ô–¢–´ –†–ï–ì–£–õ–Ø–¢–û–†–û–í.\n"
            "### –ü–†–ê–í–ò–õ–ê\n"
            "- –ù–ï –î–£–ë–õ–ò–†–£–ô –ó–ê–ü–†–û–°–´.\n"
            "- –ù–ï –î–û–ë–ê–í–õ–Ø–ô –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ò, –ù–£–ú–ï–†–ê–¶–ò–Æ –ò –≠–ú–û–î–ó–ò.\n"
            "- –í–´–í–û–î–ò –¢–û–õ–¨–ö–û –°–¢–†–û–ö–ò –í –í–ò–î–ï `QUERY: ...`.\n"
            "### CHAIN OF THOUGHTS (–í–ù–£–¢–†–ï–ù–ù–ï, –ù–ï –í–´–í–û–î–ò–¢–¨)\n"
            "1. –ü–û–ù–Ø–¢–¨ –∑–∞–¥–∞—á—É.\n"
            "2. –°–§–û–†–ú–ò–†–û–í–ê–¢–¨ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã.\n"
            "3. –°–ì–ï–ù–ï–†–ò–†–û–í–ê–¢–¨ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã.\n"
            "4. –°–ö–û–ú–ë–ò–ù–ò–†–û–í–ê–¢–¨ –∏—Ö —Å –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞–º–∏.\n"
            "5. –í–´–í–ï–°–¢–ò —Å—Ç—Ä–æ–∫–∏ `QUERY:`.\n"
        )
        raw = await _gpt(
            [{"role": "system", "content": sys},
             {"role": "user",   "content": f'base={base}{hist}'}],
            model=self.llm_model, T=0.1)
        ql = re.findall(r"QUERY:\s*(.+)", raw, flags=re.I)

        if not hist:
            templates = [
                f'"{self.company}" –æ–ø–∏—Å–∞–Ω–∏–µ',
                f'"{self.company}" –±—Ä–µ–Ω–¥—ã',
                f'"{self.company}" —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∏',
                f'"{self.company}" —á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å',
                f'"{self.company}" –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–æ—â–Ω–æ—Å—Ç–∏',
                f'"{self.company}" –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏',
                f'"{self.company}" —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ',
                f'"{self.company}" –∞–¥—Ä–µ—Å',
                f'"{self.company}" –∏—Å—Ç–æ—Ä–∏—è',
                f'"{self.company}" –ø—Ä–∏–±—ã–ª—å',
                f'"{self.company}" –æ–±—ä—ë–º –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞',
                f'"{self.company}" –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã',
                f'"{self.company}" —Ä–µ–π—Ç–∏–Ω–≥',
                f'—Ñ–æ—Ä—É–º "{self.company}"',
                f'site:news.* "{self.company}"',
            ]
            ql = templates + [q for q in ql if q not in templates]

        # ‚îÄ‚îÄ‚îÄ —Ü–µ–ª–µ–≤—ã–µ —Å–æ—Ü—Å–µ—Ç–∏ –∏ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        social_sites = ["vk.com", "facebook.com", "linkedin.com",
                        "youtube.com", "ok.ru"]
        extras = [f'"{self.company}" site:{s}' for s in social_sites]
        if dom:
            extras.append(f'"{self.company}" site:{dom}')

        # dedup —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        ql.extend(extras)
        ql = list(dict.fromkeys(ql))
        return ql

    # ---------- —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç ----------------------------------------
    async def _summary(self, ctx: str) -> str:
        sys = (
            "–¢–´ ‚Äî –í–´–°–û–ö–û–ö–í–ê–õ–ò–§–ò–¶–ò–†–û–í–ê–ù–ù–´–ô –ê–ù–ê–õ–ò–¢–ò–ö –†–´–ù–ö–û–í. –°–û–°–¢–ê–í–¨ –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–´–ô "
            "–ê–ù–ê–õ–ò–¢–ò–ß–ï–°–ö–ò–ô –û–¢–ß–Å–¢ –û –ö–û–ú–ü–ê–ù–ò–ò –ò–ó –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–´–• –ê–ë–ó–ê–¶–ï–í –í –°–õ–ï–î–£–Æ–©–ï–ú "
            "–§–ò–ö–°–ò–†–û–í–ê–ù–ù–û–ú –ü–û–†–Ø–î–ö–ï: "
            "1) –û–ü–ò–°–ê–ù–ò–ï; "
            "2) –ë–†–ï–ù–î–´ (–ø–µ—Ä–µ—á–µ–Ω—å –∏ –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ); "
            "3) –ß–ò–°–õ–ï–ù–ù–û–°–¢–¨ –°–û–¢–†–£–î–ù–ò–ö–û–í; "
            "4) –ü–†–û–ò–ó–í–û–î–°–¢–í–ï–ù–ù–´–ï –ú–û–©–ù–û–°–¢–ò (–ø–ª–æ—â–∞–¥—å, –æ–±—ä—ë–º—ã –ø–æ –≥–æ–¥–∞–º/–¥–Ω—è–º); "
            "5) –ò–ù–í–ï–°–¢–ò–¶–ò–ò –ò –ü–†–û–ï–ö–¢–´ –†–ê–°–®–ò–†–ï–ù–ò–Ø (—Å—É–º–º–∞, –ø–ª–∞–Ω—ã, —Ä—ã–Ω–∫–∏); "
            "6) –ê–î–†–ï–° HQ –ò –ü–†–û–ò–ó–í–û–î–°–¢–í–ï–ù–ù–´–• –ü–õ–û–©–ê–î–û–ö; "
            "7) –°–û–¶–ò–ê–õ–¨–ù–´–ï –°–ï–¢–ò (–í–ö, Facebook, LinkedIn, YouTube, –û–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–∏, —Å–∞–π—Ç); "
            "8) –ò–°–¢–û–†–ò–Ø –ò –ö–õ–Æ–ß–ï–í–´–ï –°–û–ë–´–¢–ò–Ø; "
            "9) –ü–†–ò–ë–´–õ–¨/–û–ë–™–Å–ú–´ –ü–†–û–î–£–ö–¶–ò–ò; "
            "10) –ö–û–ù–ö–£–†–ï–ù–¢–´ (–Ω–∞–∑–≤–∞–Ω–∏—è, —Å–∞–π—Ç—ã, –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ); "
            "11) –£–ß–ê–°–¢–ò–ï –í –§–û–†–£–ú–ê–•/–ù–û–í–û–°–¢–Ø–•/–†–ï–ô–¢–ò–ù–ì–ê–•. "
            "–ü–û–°–õ–ï –ö–ê–ñ–î–û–ì–û –§–ê–ö–¢–ê –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –£–ö–ê–ó–´–í–ê–ô –°–°–´–õ–ö–£-–ò–°–¢–û–ß–ù–ò–ö –í –ö–†–£–ì–õ–´–• –°–ö–û–ë–ö–ê–• (–ü–û–õ–ù–´–ô URL). "
            "–ï–°–õ–ò –î–ê–ù–ù–´–• –ù–ï–¢ ‚Äî –ü–ò–®–ò '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'. "
            "–ù–ï –î–£–ë–õ–ò–†–£–ô –ò–ù–§–û–†–ú–ê–¶–ò–Æ –ò –ù–ï –í–´–î–£–ú–´–í–ê–ô –§–ê–ö–¢–û–í. "
            "–ù–ï –ò–°–ü–û–õ–¨–ó–£–ô MARKDOWN, –ù–ï –£–ö–ê–ó–´–í–ê–ô –í–´–†–£–ß–ö–£ (REVENUE) –ù–ò –í –ö–ê–ö–û–ú –í–ò–î–ï, –ù–û –ú–û–ñ–ù–û –£–ö–ê–ó–´–í–ê–¢–¨ –ü–†–ò–ë–´–õ–¨ –ü–û –ü–†–û–î–£–ö–¢–ê–ú.\n"
        )
        
        return await _gpt(
            [{"role": "system", "content": sys},
             {"role": "user",   "content": ctx[:20_000]}],
            model=self.llm_model, T=0.25)

    def _normalize_sections(self, summary: str) -> str:
        sections = summary.split("\n\n")
        norm = []
        for sec in sections:
            lines = [l.strip() for l in sec.splitlines() if l.strip()]
            seen, uniq = set(), []
            for line in lines:
                if line not in seen:
                    seen.add(line)
                    uniq.append(line)
            norm.append("\n".join(uniq) if uniq else "–Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return "\n\n".join(norm)

    # ---------- orchestrator -------------------------------------------
    async def _run_async(self):
        # paralell: —Å–Ω–∏–ø–ø–µ—Ç + –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞
        site_ctx_task = asyncio.create_task(self._site_ctx())
        site_pass_task = (
            asyncio.create_task(asyncio.to_thread(_site_passport_sync, self.website))
            if self.website else None
        )
        

        queries, snippets, hist = [], [], ""
        news_snippets: list[tuple[str, str]] = []
        async with aiohttp.ClientSession() as s:
            for _ in range(self.steps):
                ql = await self._queries(hist)
                ql = [f"{q} {self.market}" if self.market and
                      self.market.lower() not in q.lower() else q for q in ql]
                queries += ql
                res = await asyncio.gather(*[_google(s, q, self.snips) for q in ql])
                snippets += sum(res, [])
                hist = f"\n–°–Ω–∏–ø–ø–µ—Ç–æ–≤: {len(snippets)}"

            news_domains = [
                "rbc.ru",
                "kommersant.ru",
                "vedomosti.ru",
                "tass.ru",
                "forbes.ru",
            ]
            news_queries = [f'site:{d} "{self.company}"' for d in news_domains]
            queries += news_queries
            res = await asyncio.gather(*[_google(s, q, self.snips) for q in news_queries])
            news_snippets = sum(res, [])
            snippets += news_snippets

        site_ctx  = await site_ctx_task
        site_pass = await site_pass_task if site_pass_task else ""

        # –≤—ã–¥–µ–ª—è–µ–º —Å–Ω–∏–ø–ø–µ—Ç—ã —Å —Å–æ—Ü—Å–µ—Ç—è–º–∏
        dom = tldextract.extract(self.website).registered_domain if self.website else ""
        social_domains = ["vk.com", "facebook.com", "linkedin.com",
                          "youtube.com", "ok.ru"]
        if dom:
            social_domains.append(dom)
        social_snips = [
            (u, t) for u, t in snippets
            if any(sd in u.lower() or sd in t.lower() for sd in social_domains)
        ]

        # ---------- —Å–æ–±–∏—Ä–∞–µ–º –µ–¥–∏–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è GPT -----------------
        ctx_parts: list[str] = []

        # 1) –∫–æ—Ä–æ—Ç–∫–∏–π —Å–Ω–∏–ø–ø–µ—Ç –∏–∑ Google (¬´site:‚Ä¶¬ª)
        if site_ctx:
            ctx_parts.append(f"SITE_SNIPPET:\n{site_ctx}")

        # 2) –ø–æ–ª–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π SiteRAG
        if site_pass:
            ctx_parts.append(f"SITE_PASSPORT:\n{site_pass}")

        # 3) —Ñ–∞–∫—Ç—ã –∏–∑ checko / fin-API
        company_doc_txt = ""
        if self.company_info:                       # ‚Üê –±—ã–ª –ø–µ—Ä–µ–¥–∞–Ω –≤ __init__
            def _pair(k, v):
                if v in (None, "", []): return ""
                if isinstance(v, list):
                    v = "; ".join(map(str, v[:10]))
                return f"* **{k}:** {v}"
            company_doc_txt = "\n".join(
                p for p in (_pair(k, v) for k, v in self.company_info.items()) if p
            )
            if company_doc_txt:
                ctx_parts.append(f"COMPANY_DOC:\n{company_doc_txt}")

        # 4) —Å–æ—Ü—Å–µ—Ç–∏
        if social_snips:
            ctx_parts.append(
                "SOCIAL_SNIPPETS:\n" +
                "\n".join(f"URL:{u}\nTXT:{t}" for u, t in social_snips)
            )

        # 5) Google-—Å–Ω–∏–ø–ø–µ—Ç—ã
        ctx_parts.append(
            "\n".join(f"URL:{u}\nTXT:{t}" for u, t in snippets)
        )

        # ---------- —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç ----------------------------------
        summary = await self._summary("\n\n".join(ctx_parts))
        summary = self._normalize_sections(summary)

        return {
            "summary":     summary,
            "queries":     queries,
            "snippets":    snippets,
            "news_snippets": news_snippets,
            "site_ctx":    site_ctx,
            "site_pass":   site_pass,
            "company_doc": company_doc_txt   # ‚Üê –Ω–æ–≤—ã–π –∫–ª—é—á (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω –≤–æ —Ñ—Ä–æ–Ω—Ç–µ)
        }


    # ---------- –ø—É–±–ª–∏—á–Ω—ã–π —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å -----------------
    def run(self) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å–æ –≤—Å–µ–º–∏ –ø–æ–ª—è–º–∏.
        –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –∫–æ–≥–¥–∞ event-loop —É–∂–µ –∑–∞–ø—É—â–µ–Ω
        (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ Jupyter, –≤–Ω—É—Ç—Ä–∏ Streamlit-callback –∏ —Ç.–ø.).
        """
        try:
            loop = asyncio.get_event_loop()
            if loop and loop.is_running():
                # nest_asyncio.patch() —É–∂–µ –≤—ã–∑–≤–∞–Ω –≤—ã—à–µ, –ø–æ—ç—Ç–æ–º—É –º–æ–∂–Ω–æ:
                return loop.run_until_complete(self._run_async())
        except RuntimeError:
            # get_event_loop() –º–æ–∂–µ—Ç –±—Ä–æ—Å–∏—Ç—å, –µ—Å–ª–∏ —Ü–∏–∫–ª–∞ –Ω–µ—Ç ‚Äî —Ç–æ–≥–¥–∞ –ø—Ä–æ—Å—Ç–æ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π
            pass

        return asyncio.run(self._run_async())












# ‚ï≠‚îÄüåê  Market RAG helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
async def google_snippets(query: str, num: int = 4):
    q = re.sub(r'[\"\'‚Äú‚Äù]', '', query)[:80]
    params = {
        "key": KEYS["GOOGLE_API_KEY"],
        "cx":  KEYS["GOOGLE_CX"],
        "q":   q, "num": num, "hl": "ru", "gl": "ru"
    }
    async with aiohttp.ClientSession() as sess:
        async with sess.get("https://www.googleapis.com/customsearch/v1",
                            params=params, headers=HEADERS, timeout=8) as r:
            if r.status != 200:
                logging.warning(f"[Google] {r.status}")
                return []
            js = await r.json()
            return [(it["link"], it.get("snippet", ""))
                    for it in js.get("items", []) if not _bad(it["link"])]

async def gpt_async(messages, T=0.2, model="gpt-4o-mini"):
    chat = await openai.ChatCompletion.acreate(
        model=model, temperature=T, messages=messages)
    return chat.choices[0].message.content.strip()

class FastMarketRAG:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict(summary, queries, snippets) –∑–∞ ~10 —Å."""
    def __init__(self, market, country="–†–æ—Å—Å–∏—è",
                 years=(2021, 2022, 2023, 2024),
                 steps=1, snips=6):
        self.market, self.country = market, country
        self.years, self.steps, self.snips = years, steps, snips



    
    async def _queries(self, hist=""):
        sys = (
            "–¢–´ ‚Äî –û–ü–´–¢–ù–´–ô –ò–°–°–õ–ï–î–û–í–ê–¢–ï–õ–¨ –†–´–ù–ö–û–í –ò –î–ê–ù–ù–´–•. –°–§–û–†–ú–£–õ–ò–†–£–ô 10‚Äì12 –¢–û–ß–ù–´–• –ò –≠–§–§–ï–ö–¢–ò–í–ù–´–• GOOGLE-–ó–ê–ü–†–û–°–û–í, "
            f"–ù–ê–ü–†–ê–í–õ–ï–ù–ù–´–• –ù–ê –°–ë–û–† –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–û–ô –ò–ù–§–û–†–ú–ê–¶–ò–ò –û –†–´–ù–ö–ï ¬´{self.market}¬ª –í –°–¢–†–ê–ù–ï {self.country.upper()} –ó–ê –ü–ï–†–ò–û–î {', '.join(map(str, self.years))}. "
            "–ü–û–ò–°–ö–û–í–´–ï –ó–ê–ü–†–û–°–´ –î–û–õ–ñ–ù–´ –û–•–í–ê–¢–´–í–ê–¢–¨ –°–õ–ï–î–£–Æ–©–ò–ï –ê–°–ü–ï–ö–¢–´ –†–´–ù–ö–ê: "
            "1) –û–ë–™–Å–ú –ò –î–ò–ù–ê–ú–ò–ö–ê –†–´–ù–ö–ê, "
            "2) –°–¢–†–£–ö–¢–£–†–ê –ò –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø, "
            "3) –û–°–ù–û–í–ù–´–ï –ò–ì–†–û–ö–ò –ò –ò–• –î–û–õ–ò, "
            "4) –¶–ï–ù–´ –ò –¶–ï–ù–û–í–´–ï –¢–ï–ù–î–ï–ù–¶–ò–ò, "
            "5) –ö–õ–Æ–ß–ï–í–´–ï –¢–†–ï–ù–î–´ –ò –ò–ù–û–í–ê–¶–ò–ò, "
            "6) –†–ï–ì–ò–û–ù–ê–õ–¨–ù–´–ô –†–ê–ó–†–ï–ó, "
            "7) –§–ê–ö–¢–û–†–´ –†–û–°–¢–ê –ò –ë–ê–†–¨–ï–†–´ –í–•–û–î–ê, "
            "8) –°–î–ï–õ–ö–ò, IPO, –°–õ–ò–Ø–ù–ò–Ø, "
            "9) –ê–ù–ê–õ–ò–¢–ò–ß–ï–°–ö–ò–ï –û–¢–ß–Å–¢–´ –ò –î–û–ö–õ–ê–î–´ "
            "–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê: QUERY: <–°–¢–†–û–ö–ê –î–õ–Ø –ü–û–ò–°–ö–ê –í GOOGLE>. "
            "–ù–ï –ü–û–í–¢–û–†–Ø–ô –ó–ê–ü–†–û–°–´. –ù–ï –î–û–ë–ê–í–õ–Ø–ô –õ–ò–®–ù–ò–• –ü–†–ï–î–ò–°–õ–û–í–ò–ô ‚Äî –¢–û–õ–¨–ö–û –°–ü–ò–°–ö–û–ú."
)
        raw = await gpt_async([
            {"role": "system", "content": sys},
            {"role": "user",   "content": hist}
        ], T=0.12)
        return re.findall(r"QUERY:\s*(.+)", raw, flags=re.I)

    async def _run_async(self):
        snippets, hist = [], ""
        for _ in range(self.steps):
            ql = await self._queries(hist)
            tasks = [google_snippets(q, self.snips) for q in ql]
            for res in await asyncio.gather(*tasks):
                snippets.extend(res)
            hist = f"—Å–Ω–∏–ø–ø–µ—Ç–æ–≤={len(snippets)}"

        context = "\n".join(f"URL:{u}\nTXT:{t}" for u, t in snippets)[:18000]
        sys = (
            f"–¢–´ ‚Äî –í–´–°–û–ö–û–ö–õ–ê–°–°–ù–´–ô –ê–ù–ê–õ–ò–¢–ò–ö –†–´–ù–ö–ê ¬´{self.market}¬ª –í –°–¢–†–ê–ù–ï {self.country.upper()}. "
            "–°–§–û–†–ú–ò–†–£–ô –ü–û–ì–û–î–û–í–û–ô –û–ë–ó–û–† –†–´–ù–ö–ê, –ì–î–ï –ö–ê–ñ–î–´–ô –ì–û–î –ü–†–ï–î–°–¢–ê–í–õ–ï–ù –û–¢–î–ï–õ–¨–ù–´–ú –ù–ê–ü–û–õ–ù–ï–ù–ù–´–ú –ê–ë–ó–ê–¶–ï–ú, –í–ö–õ–Æ–ß–ê–Æ–©–ò–ú –°–õ–ï–î–£–Æ–©–ò–ï –≠–õ–ï–ú–ï–ù–¢–´: "
            "1) –û–ë–™–Å–ú –†–´–ù–ö–ê (–µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∞), "
            "2) –¢–ï–ú–ü –†–û–°–¢–ê (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö –∏–ª–∏ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö), "
            "3) –°–¢–†–£–ö–¢–£–†–ê –ò –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø (–ø–æ —Ç–∏–ø—É –ø—Ä–æ–¥—É–∫—Ç–∞, –∫–ª–∏–µ–Ω—Ç—É, –∫–∞–Ω–∞–ª—É –∏ –¥—Ä.), "
            "4) –†–ï–ì–ò–û–ù–ê–õ–¨–ù–´–ï –†–ê–ó–†–ï–ó–´ (–∫–ª—é—á–µ–≤—ã–µ —Ä–µ–≥–∏–æ–Ω—ã –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–∞–Ω—ã), "
            "5) –û–°–ù–û–í–ù–´–ï –ò–ì–†–û–ö–ò –ò –ò–• –î–û–õ–ò (—Å –¥–æ–ª—è–º–∏ –≤ %, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã), "
            "6) –ö–†–£–ü–ù–´–ï –°–î–ï–õ–ö–ò –ò –°–û–ë–´–¢–ò–Ø (M&A, IPO, –ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–∞), "
            "7) –¶–ï–ù–û–í–û–ô –ê–ù–ê–õ–ò–ó (—É—Ä–æ–≤–Ω–∏ —Ü–µ–Ω, –¥–∏–Ω–∞–º–∏–∫–∞, —Ñ–∞–∫—Ç–æ—Ä—ã –≤–ª–∏—è–Ω–∏—è), "
            "8) –ö–õ–Æ–ß–ï–í–´–ï –¢–†–ï–ù–î–´ (—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, —Å–ø—Ä–æ—Å, —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –¥—Ä.), "
            "9) –ë–ê–†–¨–ï–†–´ –ò –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø (–≤—Ö–æ–¥, –ª–æ–≥–∏—Å—Ç–∏–∫–∞, –Ω–æ—Ä–º–∞—Ç–∏–≤–∫–∞), "
            "10) –í–´–í–û–î–´ –ü–û –ì–û–î–£ (–∫–ª—é—á–µ–≤—ã–µ –∏—Ç–æ–≥–∏ –∏ —Å–¥–≤–∏–≥–∏). "
            "11) –∏—Ç–æ–≥–æ–≤—ã–º –∞–±–∑–∞—Ü–µ–º –≤—ã–≤–µ–¥–∏ –æ–±—ä–µ–º—ã —Ä—ã–Ω–∫–∞ –ø–æ –≥–æ–¥–∞–º –∫–æ—Ç–æ—Ä—ã–µ —Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–ª–∏ –≤ –ø—Ä–æ—à–ª—ã—Ö –∞–±–∑–∞—Ü–∞—Ö"
            "–í–°–ï –§–ê–ö–¢–´ –î–û–õ–ñ–ù–´ –ë–´–¢–¨ –£–ù–ò–ö–ê–õ–¨–ù–´–ú–ò, –ù–ï –ü–û–í–¢–û–†–Ø–¢–¨–°–Ø –ò –ü–û–î–¢–í–ï–†–ñ–î–Å–ù–ù–´ –†–ï–ê–õ–¨–ù–´–ú–ò –°–°–´–õ–ö–ê–ú–ò –ù–ê –ò–°–¢–û–ß–ù–ò–ö–ò –í –ö–†–£–ì–õ–´–• –°–ö–û–ë–ö–ê–• (–§–û–†–ú–ê–¢: –ü–û–õ–ù–´–ô URL). "
            "–ù–ï –ò–°–ü–û–õ–¨–ó–£–ô MARKDOWN, –ù–ï –ü–†–ò–î–£–ú–´–í–ê–ô –§–ê–ö–¢–´ ‚Äî –¢–û–õ–¨–ö–û –î–û–ö–£–ú–ï–ù–¢–ò–†–û–í–ê–ù–ù–´–ï –î–ê–ù–ù–´–ï. "
            "–≤—Å–µ–≥–¥–∞ –æ—Å—Ç–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏"

)
        summary = await gpt_async([
            {"role": "system", "content": sys},
            {"role": "user",   "content": context}
        ], T=0.19)
        return {"summary": summary, "queries": ql, "snippets": snippets}

    def run(self):
        return asyncio.run(self._run_async())

# –∫–µ—à–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö –±—ã–ª–æ –º–≥–Ω–æ–≤–µ–Ω–Ω–æ
@st.cache_data(ttl=86_400, show_spinner="üîé –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç‚Ä¶")
def get_market_rag(market):
    return FastMarketRAG(market).run()


# ‚ï≠‚îÄüßæ  MARKET EVIDENCE (Perplexity) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
#   ‚Ä¢ –ê–±–∑–∞—Ü—ã-–∏—Å—Ç–æ—á–Ω–∏–∫–∏ + —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞ + 2 –±–ª–æ–∫–∞ "–°–¢–†–£–ö–¢–£–†–ê"
#   ‚Ä¢ –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π requests-–≤—ã–∑–æ–≤ Perplexity, –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–π –≤—ã–≤–æ–¥ –≤ Streamlit
# ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ
import os
import re
import json
import requests
import streamlit as st
from typing import Optional, Tuple, Dict

API_URL_PPLX = "https://api.perplexity.ai/chat/completions"


class PPLXError(Exception):
    pass


# –°–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –≤—ã–≤–æ–¥–∏–º (—é—Ä-—Å—É—â–Ω–æ—Å—Ç–∏)
_FORBIDDEN = re.compile(r"(–∞–∫—Ü–∏–æ–Ω–µ—Ä|–≤–ª–∞–¥–µ–ª—å—Ü|–±–µ–Ω–µ—Ñ–∏—Ü–∏–∞—Ä|–∏–Ω–Ω|–æ–≥—Ä–Ω)", re.IGNORECASE)


def _sanitize_evidence(text: str) -> str:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ —á–∏—Å—Ç–∏—Ç –ª–∏—à–Ω–∏–µ –ø—É—Å—Ç—ã–µ."""
    lines, out, blank = [], [], False
    for ln in (text or "").splitlines():
        if _FORBIDDEN.search(ln):
            continue
        lines.append(ln)
    for ln in lines:
        if ln.strip() == "":
            if not blank:
                out.append("")
            blank = True
        else:
            out.append(ln.rstrip())
            blank = False
    return "\n".join(out).strip()


def _get_pplx_key() -> str:
    key = (os.getenv("SONAR_API_KEY") or os.getenv("PPLX_API_KEY") or os.getenv("PERPLEXITY_API_KEY") or "").strip()
    if (not key.startswith("pplx-")) or (len(key) < 40) or key.endswith("..."):
        raise PPLXError("–ü–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä—å Perplexity API key: –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å 'pplx-' –∏ –±—ã—Ç—å –¥–ª–∏–Ω–Ω—ã–º (–æ–±—ã—á–Ω–æ >40 —Å–∏–º–≤–æ–ª–æ–≤).")
    return key


def _call_pplx(
    prompt: str,
    *,
    model: str = "sonar",
    recency: Optional[str] = None,
    temperature: float = 0.0,
    max_tokens: int = 1800,
    timeout: int = 60,
) -> str:
    headers = {
        "Authorization": f"Bearer {_get_pplx_key()}",
        "Content-Type": "application/json",
        "Accept": "application/json",
        "User-Agent": "adv-market-evidence/1.1",
    }
    payload = {
        "model": model,
        "messages": [
            {
                "role": "system",
                "content": (
                    "–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ —Ä—ã–Ω–∫–æ–≤. –û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–Ω–æ, —Å –ü–†–Ø–ú–´–ú–ò URL. "
                    "–ù–ï —É–ø–æ–º–∏–Ω–∞–π –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤/–∞–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤/–±–µ–Ω–µ—Ñ–∏—Ü–∏–∞—Ä–æ–≤, –ò–ù–ù/–û–ì–†–ù."
                ),
            },
            {"role": "user", "content": prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    if recency in {"hour", "day", "week", "month", "year"}:
        payload["search_recency_filter"] = recency

    r = requests.post(API_URL_PPLX, headers=headers, json=payload, timeout=timeout)
    if r.status_code != 200:
        try:
            err = r.json()
        except Exception:
            err = {"error": r.text[:800]}
        raise PPLXError(f"HTTP {r.status_code}: {json.dumps(err, ensure_ascii=False)[:800]}")
    data = r.json()
    return data["choices"][0]["message"]["content"]


def build_market_evidence_prompt(
    market: str,
    country: str = "–†–æ—Å—Å–∏—è",
    years_force: tuple = (2021, 2022, 2023, 2024),
    min_sources: int = 6,
) -> str:
    years_txt = ", ".join(str(y) for y in years_force)
    # –î–í–ê –Ø–í–ù–´–• –ó–ê–í–ï–†–®–ê–Æ–©–ò–• FENCED-–ë–õ–û–ö–ê: ```text ... ```
    return f"""
–°–æ–±–µ—Ä–∏ "evidence" –ø–æ —Ä—ã–Ω–∫—É ¬´{market}¬ª (—Å—Ç—Ä–∞–Ω–∞: {country}) –∏–∑ —Ä–∞–∑–Ω—ã—Ö –û–¢–ö–†–´–¢–´–• –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê ‚Äî –°–¢–†–û–ì–û –¢–ï–ö–°–¢ –° –ê–ë–ó–ê–¶–ê–ú–ò (–ë–ï–ó –°–ü–ò–°–ö–û–í/–¢–ê–ë–õ–ò–¶/CSV):
‚Äî –ö–∞–∂–¥—ã–π –ù–û–í–´–ô –ê–ë–ó–ê–¶ –ø–æ—Å–≤—è—â—ë–Ω –û–î–ù–û–ú–£ —Ä–µ—Å—É—Ä—Å—É (–∏—Å—Ç–æ—á–Ω–∏–∫—É): –Ω–∞—á–∏–Ω–∞–π —Ç–∞–∫ ‚Äî ¬´–ò—Å—Ç–æ—á–Ω–∏–∫: <–∏–∑–¥–∞—Ç–µ–ª—å/–Ω–∞–∑–≤–∞–Ω–∏–µ>, <–≥–æ–¥/–¥–∞—Ç–∞>, URL: <–ø—Ä—è–º–æ–π_–ª–∏–Ω–∫>.¬ª
‚Äî –í–Ω—É—Ç—Ä–∏ –∞–±–∑–∞—Ü–∞ –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–µ—Ä—ã. –ü–∏—à–∏ –∫–æ–º–ø–∞–∫—Ç–Ω–æ, –Ω–æ –≤–∫–ª—é—á–∞–π –í–°–ï –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ß–ò–°–õ–ê –ø–æ —Ä—ã–Ω–∫—É —Å –µ–¥–∏–Ω–∏—Ü–∞–º–∏:
   ‚Ä¢ –≥–æ–¥–æ–≤—ã–µ –æ–±—ä—ë–º—ã —Ä—ã–Ω–∫–∞ –≤ –¥–µ–Ω—å–≥–∞—Ö (–ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ ‚ÇΩ; –µ—Å–ª–∏ —Ç–æ–ª—å–∫–æ $, –≤—Å—ë —Ä–∞–≤–Ω–æ –≤–∫–ª—é—á–∞–π –∏ –ø–æ–º–µ—á–∞–π –∫–∞–∫ $);
   ‚Ä¢ –≥–æ–¥–æ–≤—ã–µ NATURAL-–æ–±—ä—ë–º—ã (—à—Ç., –º¬≤, —Ç, –ø–æ—Å–µ—â–µ–Ω–∏—è –∏ —Ç.–ø.), –µ—Å–ª–∏ –µ—Å—Ç—å;
   ‚Ä¢ –µ—Å–ª–∏ –µ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç—ã/—Ä–µ–≥–∏–æ–Ω—ã ‚Äî –∫—Ä–∞—Ç–∫–æ –¥–æ–±–∞–≤—å –∫–ª—é—á–µ–≤—ã–µ —Ü–∏—Ñ—Ä—ã.
‚Äî –ü–æ –ì–û–î–ê–ú {years_txt} —Å—Ç–∞—Ä–∞–π—Å—è –¥–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è, –µ—Å–ª–∏ —É –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –æ–Ω–∏ –µ—Å—Ç—å; –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî —è–≤–Ω–æ –Ω–∞–ø–∏—à–∏ ¬´–∑–∞ {years_txt} —É –∏—Å—Ç–æ—á–Ω–∏–∫–∞: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö¬ª.
‚Äî –£ –ö–ê–ñ–î–û–ì–û —Ñ–∞–∫—Ç–∞ ‚Äî –ü–†–Ø–ú–û–ô URL –≤ —ç—Ç–æ–º –∂–µ –∞–±–∑–∞—Ü–µ.
‚Äî –ú–∏–Ω–∏–º—É–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {min_sources}. –†–∞–∑–Ω—ã–µ –¥–æ–º–µ–Ω—ã/–∏–∑–¥–∞—Ç–µ–ª–∏ (–Ω–æ–≤–æ—Å—Ç–∏/–∞–Ω–∞–ª–∏—Ç–∏–∫–∞/–æ—Ç—á—ë—Ç—ã/–≥–æ—Å—Å—Ç–∞—Ç/–ø—Ä–æ—Ñ–∏–ª—å–Ω—ã–µ –º–µ–¥–∏–∞).

–ó–∞–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—ã–π –∞–±–∑–∞—Ü (–ø–æ—Å–ª–µ–¥–Ω–∏–π):
‚Äî –°—Ñ–æ—Ä–º–∏—Ä—É–π –æ–±—â—É—é –∫–∞—Ä—Ç–∏–Ω—É –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –†–Ø–î–ê–ú –ò –ü–†–û–ì–ù–û–ó–ê–ú: –ø–µ—Ä–µ—á–∏—Å–ª–∏, –∫–∞–∫–∏–µ —Å–µ—Ä–∏–∏ –≥–æ–¥–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –º—ã –ø–æ–ª—É—á–∏–ª–∏ (–∫—Ç–æ –∏–∑–¥–∞—Ç–µ–ª—å/–≤–∞–ª—é—Ç–∞/–ø–µ—Ä–∏–º–µ—Ç—Ä), –≤—ã—Å—Ç—Ä–æ–π –•–†–û–ù–û–õ–û–ì–ò–Æ 2021‚Üí2024 –∏ –ø—Ä–æ–≥–Ω–æ–∑—ã (—Å –µ–¥–∏–Ω–∏—Ü–∞–º–∏ –∏ —Å—Å—ã–ª–∫–∞–º–∏), –æ—Ç–º–µ—Ç—å —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è (baseline vs –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã) –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è.

–°–¢–†–£–ö–¢–£–†–ê (—Å–≤–æ–¥–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞; –î–û–ë–ê–í–¨ –ü–û–°–õ–ï –∑–∞–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–±–∑–∞—Ü–∞) ‚Äî –î–í–ê fenced-–±–ª–æ–∫–∞:
1) –î–µ–Ω—å–≥–∏:
```text
–ò—Å—Ç–æ—á–Ω–∏–∫ | –ü–µ—Ä–∏–æ–¥ 1 | –ü–µ—Ä–∏–æ–¥ 2 | –ü–µ—Ä–∏–æ–¥ 3 | ...
<–∫—Ä–∞—Ç–∫–æ–µ_–Ω–∞–∑–≤–∞–Ω–∏–µ_–∏—Å—Ç–æ—á–Ω–∏–∫–∞> | <YYYY>: <—á–∏—Å–ª–æ> <–≤–∞–ª—é—Ç–∞/–º–∞—Å—à—Ç–∞–±> | <YYYY>: <—á–∏—Å–ª–æ> <–≤–∞–ª—é—Ç–∞/–º–∞—Å—à—Ç–∞–±> | ...
2) –ù–∞—Ç—É—Ä–∞–ª—å–Ω—ã–µ:
–ò—Å—Ç–æ—á–Ω–∏–∫ | –ü–µ—Ä–∏–æ–¥ 1 | –ü–µ—Ä–∏–æ–¥ 2 | –ü–µ—Ä–∏–æ–¥ 3 | ...
<–∫—Ä–∞—Ç–∫–æ–µ_–Ω–∞–∑–≤–∞–Ω–∏–µ_–∏—Å—Ç–æ—á–Ω–∏–∫–∞> | <YYYY>: <—á–∏—Å–ª–æ> <–µ–¥.–∏–∑–º.> | <YYYY>: <—á–∏—Å–ª–æ> <–µ–¥.–∏–∑–º.> | ...
[–µ—Å–ª–∏ —É –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –Ω–µ—Ç –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ ‚Äî —É–∫–∞–∂–∏ ¬´–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö¬ª –æ–¥–Ω–æ–π —è—á–µ–π–∫–æ–π]
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Å—Ç–∏–ª—é:
‚Äî –¢–æ–ª—å–∫–æ –∞–±–∑–∞—Ü—ã –∏ –¥–≤–∞ –∑–∞–≤–µ—Ä—à–∞—é—â–∏—Ö fenced-–±–ª–æ–∫–∞ ```text, –±–µ–∑ —Å–ø–∏—Å–∫–æ–≤/–Ω—É–º–µ—Ä–∞—Ü–∏–∏/—Ç–∞–±–ª–∏—Ü/CSV.
‚Äî –ï–¥–∏–Ω–∏—Ü—ã –∏ –≤–∞–ª—é—Ç–∞ –≤—Å–µ–≥–¥–∞ —Ä—è–¥–æ–º —Å —á–∏—Å–ª–æ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, ¬´–º–ª—Ä–¥ ‚ÇΩ¬ª, ¬´$ –º–ª–Ω¬ª, ¬´—Ç—ã—Å. –ø–æ—Å–µ—â–µ–Ω–∏–π¬ª, ¬´–º¬≤¬ª).
‚Äî –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —á–∏—Å–ª–∞ ‚Äî —Ç–æ–ª—å–∫–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ–º—ã–µ —Ñ–∞–∫—Ç—ã —Å –ü–†–Ø–ú–´–ú–ò URL.
‚Äî –ë–µ–∑ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤/–∞–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤/–ò–ù–ù/–û–ì–†–ù.
""".strip()

def market_evidence_report(
    market: str,
    country: str = "–†–æ—Å—Å–∏—è",
    min_sources: int = 6,
    model: str = "sonar",
    recency: Optional[str] = None,
    max_tokens: int = 1800,
) -> str:
    assert isinstance(min_sources, int) and 3 <= min_sources <= 15, "min_sources ‚àà [3, 15]"
    prompt = build_market_evidence_prompt(market, country=country, min_sources=min_sources)
    raw = _call_pplx(prompt, model=model, recency=recency, max_tokens=max_tokens)
    return _sanitize_evidence(raw)


def _split_evidence_blocks(raw_text: str) -> Tuple[str, str, str]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (plain_text_without_code, money_block, natural_block).
    –ò—â–µ–º –¥–≤–∞ fenced-–±–ª–æ–∫–∞: text ...
    """
    if not raw_text:
        return "", "", ""
    code_blocks = re.findall(r"text\s*(.*?)\s*", raw_text, flags=re.S | re.I)
    money_block = code_blocks[0].strip() if len(code_blocks) >= 1 else ""
    natural_block = code_blocks[1].strip() if len(code_blocks) >= 2 else ""
    # –í—ã—Ä–µ–∑–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –±–ª–æ–∫–∏ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    plain = raw_text
    for blk in code_blocks[:2]:
        plain = plain.replace(f"text\n{blk}\n", "")
        plain = plain.replace(f"text\r\n{blk}\r\n", "")
    return plain.strip(), money_block, natural_block


def _linkify(text: str) -> str:
    """
    –ó–∞–º–µ–Ω—è–µ—Ç http/https —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ <a>.
    –ù–µ —Ç—Ä–æ–≥–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω–æ–π HTML.
    """
    url_re = re.compile(r"(https?://[^\s<>)\"']+)")
    return url_re.sub(r'<a href="\1" target="_blank" rel="noopener noreferrer">\1</a>', text)


@st.cache_data(ttl=86_400, show_spinner="üîé –°–æ–±–∏—Ä–∞–µ–º —Ä—ã–Ω–æ—á–Ω—ã–µ EVIDENCE‚Ä¶")
def get_market_evidence(
    market: str,
    country: str = "–†–æ—Å—Å–∏—è",
    min_sources: int = 6,
    model: str = "sonar",
    recency: Optional[str] = None,
    max_tokens: int = 1800,
) -> Dict[str, str]:
    """
    Streamlit-–∫—ç—à: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å:
    ‚Ä¢ text_html ‚Äî –≤–µ—Å—å —Ç–µ–∫—Å—Ç (–∞–±–∑–∞—Ü—ã-–∏—Å—Ç–æ—á–Ω–∏–∫–∏ + —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –∞–±–∑–∞—Ü), —Å—Å—ã–ª–∫–∏ –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã
    ‚Ä¢ money_block / natural_block ‚Äî —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–≤—É—Ö –º–∞—Ç—Ä–∏—Ü –¥–ª—è st.code(..., language="text")
    ‚Ä¢ raw_text ‚Äî –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
    """
    try:
        raw = market_evidence_report(
            market=market,
            country=country,
            min_sources=min_sources,
            model=model,
            recency=recency,
            max_tokens=max_tokens,
        )
    except PPLXError as e:
        return {
            "text_html": f"<i>–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å MARKET EVIDENCE: {str(e)}</i>",
            "money_block": "",
            "natural_block": "",
            "raw_text": "",
        }

    plain, money, natural = _split_evidence_blocks(raw)
    text_html = linkify_keep_url(plain).replace("\n", "<br>")

    return {"text_html": text_html, "money_block": money, "natural_block": natural, "raw_text": raw}




# ‚ï≠‚îÄüåê  Leaders & Interviews (context-aware)  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
import aiohttp, asyncio, re, html, logging, openai, streamlit as st, tldextract

HEADERS = {"User-Agent": "Mozilla/5.0"}
_URL_PAT = re.compile(r"https?://[^\s)]+")
def _linkify(txt:str)->str:
    return _URL_PAT.sub(lambda m:f'<a href="{html.escape(m.group(0))}" target="_blank">—Å—Å—ã–ª–∫–∞</a>', txt)

# --- –±—ã—Å—Ç—Ä—ã–π —Å–Ω–∏–ø–ø–µ—Ç Google ---------------------------------------
async def _snip(sess: aiohttp.ClientSession, query:str, n:int=4):
    q = re.sub(r'[\"\'‚Äú‚Äù]', '', query)[:90]
    params = {"key": KEYS["GOOGLE_API_KEY"], "cx": KEYS["GOOGLE_CX"],
              "q": q, "num": n, "hl": "ru", "gl": "ru"}
    try:
        async with sess.get("https://www.googleapis.com/customsearch/v1",
                             params=params, headers=HEADERS, timeout=8) as r:
            if r.status!=200:
                logging.warning(f"[Google] {r.status}"); return []
            js = await r.json()
            return [(it["link"], it.get("snippet",""))
                    for it in js.get("items",[]) if not _bad(it["link"])]
    except asyncio.TimeoutError:
        logging.warning("[Google] timeout"); return []

# --- –∫–æ–Ω—Ç–µ–∫—Å—Ç-—Å–Ω–∏–ø–ø–µ—Ç –ø–æ –¥–æ–º–µ–Ω—É -----------------------------------
async def _site_snip(sess, domain:str)->str:
    if not domain: return ""
    res = await _snip(sess, f"site:{domain}", n=1)
    return res[0][1] if res else ""

class FastLeadersInterviews:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict(summary, names, queries, snippets).

    company_info –∂–¥—ë—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É Checko/FNS:
       ‚Ä¢ general_director / managers / ¬´–†—É–∫–æ–≤–æ–¥¬ª
       ‚Ä¢ founders        / ¬´–£—á—Ä–µ–¥_–§–õ¬ª
    """
    def __init__(self, company: str, *,
                 website: str = "",
                 market:  str = "",
                 company_info: dict | None = None,
                 model: str = "gpt-4o-mini"):

        self.c        = company.strip()
        self.site     = website.strip()
        self.market   = market.strip()
        self.cinfo    = company_info or {}
        self.model    = model

    # ---------- helpers ------------------------------------------------
    def _domain(self) -> str:
        import tldextract
        return tldextract.extract(self.site).registered_domain if self.site else ""

    @staticmethod
    def _fmt_person(p: dict | list | None, default_role: str) -> str | None:
        # –î–û–ë–ê–í–ò–õ–ò –∫–ª—é—á–∏ '–§–ò–û' –∏ '–ò–ù–ù'
        if not p:
            return None
        if isinstance(p, list):
            p = next((d for d in p if isinstance(d, dict) and
                      (d.get("name") or d.get("fio") or d.get("–§–ò–û"))), None)
            if not p:
                return None
        fio  = p.get("name") or p.get("fio") or p.get("–§–ò–û")
        inn  = p.get("inn")  or p.get("–ò–ù–ù")
        role = p.get("type") or p.get("post") or default_role
        if not fio:
            return None
        inn_txt = f", –ò–ù–ù {inn}" if inn else ""
        return f"{fio} ({role}{inn_txt})"

    async def _llm_queries(self, prompt: str) -> list[str]:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç prompt –≤ GPT-4o (–∏–ª–∏ –ª—é–±—É—é self.model) –∏
        –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞  Q: <query>  –∏–∑ –æ—Ç–≤–µ—Ç–∞.
        """
        raw = await _gpt(
            [{"role": "system", "content": prompt},
             {"role": "user",   "content": ""}],
            model=self.model,
            T=0.14,
        )
        import re
        return re.findall(r"(?:Q|QUERY)[:\-]\s*(.+)", raw, flags=re.I)

    
    # ---------- 1. –†–£–ö–û–í–û–î–ò–¢–ï–õ–ò / –í–õ–ê–î–ï–õ–¨–¶–´ ---------------------------
    async def _leaders(self, sess):
        # 1) –±–µ—Ä—ë–º —É–∂–µ –æ—á–∏—â–µ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –∏–∑ self.cinfo
        names = []
        leaders_raw  = self.cinfo.get("leaders_raw")  or []
        founders_raw = self.cinfo.get("founders_raw") or []
        names.extend(leaders_raw)
        names.extend(founders_raw)
    
        # –µ—Å–ª–∏ —Å–ø–∏—Å–∫–∏ –Ω–∞—à–ª–∏—Å—å, –Ω–∏—á–µ–≥–æ –±–æ–ª—å—à–µ –Ω–µ –¥–µ–ª–∞–µ–º
        if names:
            return list(dict.fromkeys(names)), [], [] 

        # 1-B. –ï—Å–ª–∏ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –∏–º–µ–Ω–∞ —Ç–∞–∫ –∏ –Ω–µ –ø–æ—è–≤–∏–ª–∏—Å—å ‚Üí fallback –Ω–∞ Google
        if not names:
            # ----------------------------------------------------------- #
            # 1) —Ä–∞—Å—à–∏—Ä—è–µ–º —Å–ø–∏—Å–æ–∫ —Ä–æ–ª–µ–π
            roles_kw = [
                # founders / owners
                "–æ—Å–Ω–æ–≤–∞—Ç–µ–ª—å", "—Å–æ–æ—Å–Ω–æ–≤–∞—Ç–µ–ª—å", "owner", "founder",
                # top-management
                "–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä", "–≥–µ–Ω–¥–∏—Ä–µ–∫—Ç–æ—Ä", "CEO",
                "–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –¥–∏—Ä–µ–∫—Ç–æ—Ä", "CCO", "chief commercial officer",
                "–¥–∏—Ä–µ–∫—Ç–æ—Ä –ø–æ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥—É", "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä", "CMO",
                "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä", "CFO",
            ]

            # 2) —Å—Ç—Ä–æ–∏–º –∑–∞–ø—Ä–æ—Å—ã –¥–≤—É—Ö —Ç–∏–ø–æ–≤:
            #    –∞) ¬´–∫—Ç–æ {—Ä–æ–ª—å} "{–∫–æ–º–ø–∞–Ω–∏—è}" "{—Ä—ã–Ω–æ–∫}"¬ª
            #    –±) ¬´"{–∫–æ–º–ø–∞–Ω–∏—è}" {—Ä–æ–ª—å}¬ª (+ site:–¥–æ–º–µ–Ω, –µ—Å–ª–∏ –µ—Å—Ç—å)
            dom   = self._domain()
            mkt   = f' "{self.market}"' if self.market else ""
            g_queries, g_snips = [], []

            for kw in roles_kw:
                g_queries.append(f'–∫—Ç–æ {kw} "{self.c}"{mkt}')
                plain_q = f'"{self.c}" {kw}' + (f' OR site:{dom}' if dom else "")
                g_queries.append(plain_q)

            # 3) –≤—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ (‚â§3 –≤—ã–¥–∞—á–∏ –Ω–∞ –∑–∞–ø—Ä–æ—Å, —á—Ç–æ–±—ã –Ω–µ —à—É–º–µ—Ç—å)
            for q in g_queries:
                g_snips += await _google(sess, q, 3)

            # 4) –µ—Å–ª–∏ —Å–Ω–∏–ø–ø–µ—Ç—ã –µ—Å—Ç—å ‚Äî –ø—É—Å–∫–∞–µ–º –∏—Ö —á–µ—Ä–µ–∑ LLM-—Ñ–∏–ª—å—Ç—Ä
            if g_snips:
                sys = ("–¢—ã –ø—Ä–æ—Ñ-–∞–Ω–∞–ª–∏—Ç–∏–∫. –ü–æ —Å–Ω–∏–ø–ø–µ—Ç–∞–º —Å–æ—Å—Ç–∞–≤—å —Å–ø–∏—Å–æ–∫ "
                       "–¥–µ–π—Å—Ç–≤—É—é—â–∏—Ö —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤ "
                       "(–§–ò–û, –¥–æ–ª–∂–Ω–æ—Å—Ç—å).")
                llm_txt = await _gpt(
                    [{"role": "system", "content": sys},
                     {"role": "user",
                      "content": "\n".join(f'URL:{u}\nTXT:{t}'
                                           for u, t in g_snips)[:10_000]}],
                    model=self.model, T=0.12,
                )
                names += [ln.strip() for ln in llm_txt.splitlines() if ln.strip()]

        # dedup ---------------------------------------------------------
        uniq, seen = [], set()
        for n in names:
            k = n.lower()
            if k not in seen:
                seen.add(k); uniq.append(n)

        return uniq, g_queries, g_snips

    # ---------- 2. –ò–Ω—Ç–µ—Ä–≤—å—é (–æ—Å—Ç–∞–≤—å—Ç–µ –≤–∞—à—É —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é) -----------------
    async def _interviews(self, names: list[str], sess: aiohttp.ClientSession):
        if not names:
            return [], [], "–°–≤–µ–∂–∏—Ö –∏–Ω—Ç–µ—Ä–≤—å—é –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
    
        dom   = self._domain()
        sc    = await self._site_ctx(sess)
        base_ctx = (f"SITE_CONTEXT:\n{sc}\n—Ä—ã–Ω–æ–∫ –∫–æ–º–ø–∞–Ω–∏–∏ ‚Äì {self.market}\n\n"
                    if sc else "")
    
        all_queries, all_snips = [], []
        for fio_role in names:
            fio = fio_role.split("(")[0].strip()
            prompt = (f"–¢—ã ‚Äî –º–µ–¥–∏–∞-–∞–Ω–∞–ª–∏—Ç–∏–∫. –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π 4-6 Google-–∑–∞–ø—Ä–æ—Å–æ–≤, "
                      f"—á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –∏–Ω—Ç–µ—Ä–≤—å—é / –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ ¬´{fio}¬ª "
                      f"–∏–∑ –∫–æ–º–ø–∞–Ω–∏–∏ ¬´{self.c}¬ª. –§–æ—Ä–º–∞—Ç: Q: <query>")
            qlist = await self._llm_queries(prompt)
            for q in qlist:
                full_q = q + (f' OR site:{dom}' if dom and "site:" not in q.lower() else "")
                all_queries.append(full_q)
                all_snips += await _google(sess, full_q, 3)
    
        if not all_snips:
            return all_queries, [], "–°–≤–µ–∂–∏—Ö –∏–Ω—Ç–µ—Ä–≤—å—é –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
    
        ctx = base_ctx + "\n".join(f"URL:{u}\nTXT:{t}" for u, t in all_snips)[:16_000]
    
        sys = ("–¢—ã ‚Äî –∫–æ–Ω—Ç–µ–Ω—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫. –°–æ—Å—Ç–∞–≤—å –¥–∞–π–¥–∂–µ—Å—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤—å—é. "
               "–î–ª—è –∫–∞–∂–¥–æ–≥–æ: –§–ò–û, —Ä–æ–ª—å, –¥–∞—Ç–∞, 1-2 —Ñ—Ä–∞–∑—ã —Å—É—Ç–∏, —Å—Å—ã–ª–∫–∞.")
        digest = await _gpt([{"role": "system", "content": sys},
                             {"role": "user",   "content": ctx}],
                            model=self.model, T=0.18)
        return all_queries, all_snips, digest

    # ------------------------------------------------------------------
    # ---------- orchestrator ------------------------------------------------
    async def _run_async(self):
        async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=20)) as sess:
    
            names, q_lead, s_lead = await self._leaders(sess)
            q_int,  s_int, digest = await self._interviews(names, sess)
    
        # --- ‚ë† –≤–ª–∞–¥–µ–ª—å—Ü—ã / —Ç–æ–ø-–º–µ–Ω–µ–¥–∂–µ—Ä—ã ------------------------------------
        owners_block = ("–¢–æ–ø-–º–µ–Ω–µ–¥–∂–µ—Ä—ã –∏ –≤–ª–∞–¥–µ–ª—å—Ü—ã:\n" + "\n".join(names)
                        if names else "–¢–æ–ø-–º–µ–Ω–µ–¥–∂–µ—Ä—ã –∏ –≤–ª–∞–¥–µ–ª—å—Ü—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
    
        # --- ‚ë° –∫–æ–Ω—Ç–∞–∫—Ç—ã ------------------------------------------------------
        contacts_block = ""
        cdata = self.cinfo.get("–ö–æ–Ω—Ç–∞–∫—Ç—ã") or {}
        if cdata:
            phones = ", ".join(cdata.get("–¢–µ–ª", []))
            emails = ", ".join(cdata.get("–ï–º—ç–π–ª", []))
            site   = cdata.get("–í–µ–±–°–∞–π—Ç") or ""
            lines  = []
            if phones: lines.append(f"–¢–µ–ª: {phones}")
            if emails: lines.append(f"E-mail: {emails}")
            if site:   lines.append(f"–°–∞–π—Ç: {site}")
            if lines:
                contacts_block = "–ö–æ–Ω—Ç–∞–∫—Ç—ã:\n" + "\n".join(lines)
    
        # --- ‚ë¢ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ HTML -----------------------------------------------
        body = "\n\n".join([part for part in (owners_block, contacts_block, digest) if part])
        summary_html = linkify_as_word(body)
    
        return {
            "summary":  summary_html,
            "names":    names,
            "queries":  q_lead + q_int,
            "snippets": s_lead + s_int,
        }

    # ---------- –ø—É–±–ª–∏—á–Ω—ã–π sync-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å ------------------------------
    def run(self) -> dict:
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():                 # Jupyter / Streamlit-callback
                import nest_asyncio; nest_asyncio.apply()
                return loop.run_until_complete(self._run_async())
        except RuntimeError:
            pass
        return asyncio.run(self._run_async())

    async def _site_ctx(self, sess: aiohttp.ClientSession) -> str | None:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫—Ä–∞—Ç–∫–∏–π –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ –∫–æ–º–ø–∞–Ω–∏–∏ (–∏–ª–∏ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É,
        –µ—Å–ª–∏ self.site –Ω–µ —É–∫–∞–∑–∞–Ω). –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ,
        —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event-loop.
        """
        if not self.site:
            return ""

        loop = asyncio.get_running_loop()
        # _site_passport_sync –±–ª–æ–∫–∏—Ä—É—é—â–∏–π ‚áí –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ ThreadPool
        return await loop.run_in_executor(
            None,                              # default ThreadPoolExecutor
            partial(_site_passport_sync, self.site)
        )


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è –∫—ç—à–∞  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@st.cache_data(ttl=86_400,
               show_spinner="üîé –ò—â–µ–º —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏ –∏–Ω—Ç–µ—Ä–≤—å—é‚Ä¶")
def get_leaders_rag(company: str, *,
                    website: str = "",
                    market:  str = "",
                    company_info: dict | None = None) -> dict:
    """Streamlit-–∫—ç—à –≤–æ–∫—Ä—É–≥ FastLeadersInterviews."""
    return FastLeadersInterviews(
        company      = company,
        website      = website,
        market       = market,
        company_info = company_info,
    ).run()





# ---------- 1. –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç Checko ----------
@st.cache_data(ttl=3_600)
def ck_call(endpoint: str, inn: str):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ –∫ Checko API.

    endpoint : 'company', 'finances', 'analytics', ‚Ä¶
    inn      : —Å—Ç—Ä–æ–∫–∞ –ò–ù–ù
    """
    url = f"https://api.checko.ru/v2/{endpoint}"
    r = requests.get(
        url,
        params={"key": KEYS["CHECKO_API_KEY"], "inn": inn},
        timeout=10,
    )
    r.raise_for_status()
    return r.json()["data"]

# ---------- 2. –¢–æ–Ω–∫–∏–µ –æ–±—ë—Ä—Ç–∫–∏ (–ø–æ –∂–µ–ª–∞–Ω–∏—é) ----------
ck_company = functools.partial(ck_call, "company")
ck_fin     = functools.partial(ck_call, "finances")
# –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å ck_analytics = functools.partial(ck_call, "analytics")



# ---------- 4. –ü–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –ª–∏–¥–µ—Ä–æ–≤ / —É—á—Ä–µ–¥–∏—Ç–µ–ª–µ–π ----------
def extract_people(cell) -> list[str]:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —è—á–µ–π–∫—É ¬´–†—É–∫–æ–≤–æ–¥¬ª / ¬´–£—á—Ä–µ–¥_–§–õ¬ª –∏
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ ¬´–§–ò–û (–ò–ù–ù‚Ä¶, –¥–æ–ª—è ‚Ä¶%)¬ª.
    """
    # 0) —Å—Ä–∞–∑—É –æ—Ç—Å–µ–∫–∞–µ–º None / NaN
    if cell is None or (isinstance(cell, float) and pd.isna(cell)):
        return []

    # 1) –µ—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ ‚Üí –ø—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ Python-–ª–∏—Ç–µ—Ä–∞–ª
    if isinstance(cell, str):
        cell = cell.strip()
        if not cell:
            return []
        try:
            cell = ast.literal_eval(cell)  # '[{‚Ä¶}]' ‚Üí list | dict | str
        except (ValueError, SyntaxError):
            # –ø—Ä–æ—Å—Ç–æ —Å—Ç—Ä–æ–∫–∞ —Å –æ–¥–Ω–∏–º –§–ò–û
            return [cell]

    # 2) –æ–¥–∏–Ω–æ—á–Ω—ã–π dict ‚Üí –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ list
    if isinstance(cell, dict):
        cell = [cell]

    # 3) –µ—Å–ª–∏ —ç—Ç–æ —É–∂–µ list ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç
    if isinstance(cell, list):
        people = []
        for item in cell:
            if isinstance(item, str):
                people.append(item.strip())
            elif isinstance(item, dict):
                fio  = item.get("–§–ò–û") or item.get("fio") or ""
                inn  = item.get("–ò–ù–ù") or item.get("inn")
                share = item.get("–î–æ–ª—è", {}).get("–ü—Ä–æ—Ü–µ–Ω—Ç")
                line = fio
                if inn:
                    line += f" (–ò–ù–ù {inn}"
                    if share is not None:
                        line += f", –¥–æ–ª—è {share}%)"
                    else:
                        line += ")"
                people.append(line)
        return [p for p in people if p]      # –±–µ–∑ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
    # 4) –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø ‚Üí –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ —Å—Ç—Ä–æ–∫—É
    return [str(cell)]



def _safe_div(a: float | None, b: float | None) -> float | None:
    if a is None or b in (None, 0):
        return None
    try:
        return a / b
    except ZeroDivisionError:
        return None






import openai, asyncio, nest_asyncio, logging
nest_asyncio.apply()

# –∫–µ—à–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∫–ª–∏–∫–∞—Ö –Ω–µ –¥–µ—Ä–≥–∞—Ç—å LLM –∏ —Å–∞–π—Ç –∑–∞–Ω–æ–≤–æ
@st.cache_data(ttl=86_400, show_spinner=False)
def get_site_passport(url: str) -> dict:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –æ–±—ë—Ä—Ç–∫–∞ SiteRAG.run() —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
    if not url:
        return {"summary": "", "chunks_out": [], "html_size": "0", "url": url}
    try:
        return SiteRAG(url).run()
    except Exception as e:
        logging.warning(f"[SiteRAG] {url} ‚Üí {e}")
        return {"summary": f"(–Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Å–∞–π—Ç: {e})",
                "chunks_out": [], "html_size": "0", "url": url}









def run_ai_insight_tab() -> None:
        # ‚îÄ‚îÄ 1. ¬´–æ—á–∏—Å—Ç–∫–∞¬ª (–µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –æ—Ç—á—ë—Ç)
    if st.session_state.get("ai_result_ready"):
        rep = st.session_state["ai_report"]
    
        # --- –≤—ã–≤–æ–¥–∏–º –≤—Å—ë –∏–∑ session_state –≤–º–µ—Å—Ç–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ —Ä–∞—Å—á—ë—Ç–∞ ---
        st.markdown(rep["doc"]["summary_rendered_html"], unsafe_allow_html=True)
        st.dataframe(rep["tbl"], use_container_width=True)
        st.pyplot(rep["graphics"])
        # –∏ —Ç.–¥.
    
        # –∫–Ω–æ–ø–∫–∞ ¬´–°–±—Ä–æ—Å–∏—Ç—å –∏ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –∑–∞–Ω–æ–≤–æ¬ª
        if st.button("üîÑ –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –Ω–æ–≤—ã–π –æ—Ç—á—ë—Ç", type="primary"):
            st.session_state.pop("ai_result_ready", None)
            st.session_state.pop("ai_report", None)
            try:
                st.rerun()
            except AttributeError:
                st.experimental_rerun()
        return   
        

    # ‚ï≠‚îÄüéõ  UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
    st.title("üìä AI Company Insight")
    st.markdown("–í–≤–µ–¥–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ (–∫–∞–∂–¥–∞—è –∫–æ–º–ø–∞–Ω–∏—è ‚Äî –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ).")
    
    c1, c2, c3, c4 = st.columns(4)
    with c1: inns_raw  = st.text_area("–ò–ù–ù")          # ‚úÖ –±–µ–∑ key=* ‚Äî –Ω–∞–º –Ω–µ –Ω—É–∂–Ω—ã –¥–≤–µ –∫–æ–ø–∏–∏
    with c2: names_raw = st.text_area("–ù–∞–∑–≤–∞–Ω–∏–µ")
    with c3: mkts_raw  = st.text_area("–†—ã–Ω–æ–∫")
    with c4: sites_raw = st.text_area("–°–∞–π—Ç")
    
    aggregate_mode = st.checkbox("üßÆ –°—É–º–º–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏–Ω–∞–Ω—Å—ã –ø–æ –≤—Å–µ–º –ò–ù–ù")
    
    if st.button("üîç –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –æ—Ç—á—ë—Ç", key="ai_build"):
        with st.spinner("–°—á–∏—Ç–∞–µ–º –æ—Ç—á—ë—Ç‚Ä¶"):

    
            # ---------- –ø–∞—Ä—Å–∏–Ω–≥ ----------
            split = lambda s: [i.strip() for i in s.splitlines() if i.strip()]
            inns   = split(inns_raw)
            names  = split(names_raw)
            mkts   = split(mkts_raw)
            sites  = split(sites_raw)
            
            # ---------- –≤–∞–ª–∏–¥–∞—Ü–∏—è ----------
            if not inns:
                st.error("–£–∫–∞–∂–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ò–ù–ù."); st.stop()
            
            if aggregate_mode:            # Œ£-—Ä–µ–∂–∏–º
                # –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–∞—Å—Ç—è–≥–∏–≤–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                if len(names) == 1 and len(inns) > 1:  names *= len(inns)
                if len(mkts)  == 1 and len(inns) > 1:  mkts  *= len(inns)
                if len(sites) == 1 and len(inns) > 1:  sites *= len(inns)
            
                # —Ç–µ–ø–µ—Ä—å –≤—Å—ë –ª–∏–±–æ –ø—É—Å—Ç–æ–µ, –ª–∏–±–æ —Å–æ–≤–ø–∞–¥–∞–µ—Ç –ø–æ –¥–ª–∏–Ω–µ
                for lst, lbl in [(names, "–ù–∞–∑–≤–∞–Ω–∏–µ"), (mkts, "–†—ã–Ω–æ–∫")]:
                    if lst and len(lst) != len(inns):
                        st.error(f"–ß–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ ¬´{lbl}¬ª –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 1 –∏–ª–∏ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —á–∏—Å–ª–æ–º –ò–ù–ù."); st.stop()
            
            else:                         # –æ–¥–∏–Ω–æ—á–Ω—ã–π —Ä–µ–∂–∏–º
                if not (names and mkts):
                    st.error("–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –ø–æ–ª—è ‚Äî –ò–ù–ù, –ù–∞–∑–≤–∞–Ω–∏–µ –∏ –†—ã–Ω–æ–∫."); st.stop()
                if len({len(inns), len(names), len(mkts)}) != 1:
                    st.error("–ß–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ –≤–æ –≤—Å–µ—Ö —Ç—Ä—ë—Ö –ø–æ–ª—è—Ö –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å."); st.stop()
                if sites and len(sites) != len(inns):
                    st.error("–ß–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ ¬´–°–∞–π—Ç¬ª –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —á–∏—Å–ª–æ–º –ò–ù–ù."); st.stop()
            
            # ---------- –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—ã —Å–ø–∏—Å–∫–æ–≤ ----------
            pad = lambda lst: lst if lst else [""] * len(inns)
            names_full = pad(names)
            mkts_full  = pad(mkts)
            sites_full = pad(sites)
            YEARS = ["2022", "2023", "2024"]
            df_companies = pd.DataFrame([ck_company(i) for i in inns])

            
            def parse_people_cell(cell) -> list[str]:
                """
                –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —è—á–µ–π–∫–∏ ¬´–†—É–∫–æ–≤–æ–¥¬ª –∏–ª–∏ ¬´–£—á—Ä–µ–¥_–§–õ¬ª
                –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ ¬´–§–ò–û (–ò–ù–ù xxxx, –¥–æ–ª—è yy%)¬ª.
                –†–∞–±–æ—Ç–∞–µ—Ç –∏ –µ—Å–ª–∏ cell = NaN, '', —Å–ø–∏—Å–æ–∫, dict, —Å—Ç—Ä–æ–∫–∞-JSON.
                """
                # –ø—É—Å—Ç–æ / NaN
                if cell is None or (isinstance(cell, float) and pd.isna(cell)):
                    return []
            
                # –µ—Å–ª–∏ –ø—Ä–∏—à–ª–∞ —Å—Ç—Ä–æ–∫–∞ ‚Äî –ø—Ä–æ–±—É–µ–º –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –≤ –æ–±—ä–µ–∫—Ç
                if isinstance(cell, str):
                    cell = cell.strip()
                    if not cell:
                        return []
                    try:
                        cell = ast.literal_eval(cell)      # '[{‚Ä¶}]' -> python
                    except (ValueError, SyntaxError):
                        # –ø—Ä–æ—Å—Ç–æ —Å—Ç—Ä–æ–∫–∞ —Å –æ–¥–Ω–∏–º –§–ò–û
                        return [cell]
            
                # –æ–¥–∏–Ω–æ—á–Ω—ã–π dict
                if isinstance(cell, dict):
                    cell = [cell]
            
                # list
                if isinstance(cell, list):
                    out = []
                    for item in cell:
                        if isinstance(item, str):          # —É–∂–µ –≥–æ—Ç–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞
                            out.append(item.strip())
                        elif isinstance(item, dict):       # –Ω–∞—à –æ—Å–Ω–æ–≤–Ω–æ–π —Å–ª—É—á–∞–π
                            fio   = item.get("–§–ò–û") or ""
                            inn   = item.get("–ò–ù–ù") or ""
                            share = item.get("–î–æ–ª—è", {}).get("–ü—Ä–æ—Ü–µ–Ω—Ç")
                            line  = fio
                            if inn:
                                line += f" (–ò–ù–ù {inn}"
                                line += f", –¥–æ–ª—è {int(share)}%)" if share is not None else ")"
                            out.append(line)
                    return [s for s in out if s]
                # fallback
                return [str(cell)]
            
            def row_people_json(row: pd.Series) -> dict:
                """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç {'leaders_raw': [...], 'founders_raw': [...]}."""
                # ‚îÄ‚îÄ 1. —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                leaders = parse_people_cell(row.get("–†—É–∫–æ–≤–æ–¥"))
            
                # ‚îÄ‚îÄ 2. —É—á—Ä–µ–¥–∏—Ç–µ–ª–∏: –∫–æ–ª–æ–Ω–∫–∞ '–£—á—Ä–µ–¥' ‚Üí dict ‚Üí –∫–ª—é—á '–§–õ' ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                founders_cell = None
                uc = row.get("–£—á—Ä–µ–¥")
                if isinstance(uc, dict):
                    founders_cell = uc.get("–§–õ")          # —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
                else:
                    founders_cell = uc                    # fallback (–µ—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –¥—Ä—É–≥–æ–π)
            
                founders = parse_people_cell(founders_cell)
            
                return {"leaders_raw": leaders, "founders_raw": founders}
            
            people_cols = df_companies.apply(row_people_json, axis=1, result_type="expand")
            df_companies = pd.concat([df_companies, people_cols], axis=1)

            

            
            PNL_CODES = [                       # –≤—Å—ë, —á—Ç–æ —Ö–æ—Ç–∏–º –≤–∏–¥–µ—Ç—å –≤ –¥–ª–∏–Ω–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ
                ("–í—ã—Ä—É—á–∫–∞ (‚ÇΩ –º–ª–Ω)",                "2110"),
                ("–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å –ø—Ä–æ–¥–∞–∂ (‚ÇΩ –º–ª–Ω)",   "2120"),
                ("–í–∞–ª–æ–≤–∞—è –ø—Ä–∏–±—ã–ª—å (‚ÇΩ –º–ª–Ω)",        "2200"),
                ("–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ —Ä–∞—Å—Ö–æ–¥—ã (‚ÇΩ –º–ª–Ω)",   "2210"),
                ("–£–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–∏–µ —Ä–∞—Å—Ö–æ–¥—ã (‚ÇΩ –º–ª–Ω)", "2220"),
                ("–ü—Ä–∏–±—ã–ª—å –æ—Ç –ø—Ä–æ–¥–∞–∂ (‚ÇΩ –º–ª–Ω)",      "2300"),
                ("–î–æ—Ö–æ–¥—ã –æ—Ç —É—á–∞—Å—Ç–∏—è (‚ÇΩ –º–ª–Ω)",      "2310"),
                ("–ü—Ä–æ—Ü–µ–Ω—Ç—ã –∫ –ø–æ–ª—É—á–µ–Ω–∏—é (‚ÇΩ –º–ª–Ω)",   "2320"),
                ("–ü—Ä–æ—Ü–µ–Ω—Ç—ã –∫ —É–ø–ª–∞—Ç–µ (‚ÇΩ –º–ª–Ω)",      "2330"),
                ("–ü—Ä–æ—á–∏–µ –¥–æ—Ö–æ–¥—ã (‚ÇΩ –º–ª–Ω)",          "2340"),
                ("–ü—Ä–æ—á–∏–µ —Ä–∞—Å—Ö–æ–¥—ã (‚ÇΩ –º–ª–Ω)",         "2350"),
                ("–ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å (‚ÇΩ –º–ª–Ω)",         "2400"),
                ("–°–æ–≤–æ–∫—É–ø–Ω—ã–π –¥–æ–ª–≥ (‚ÇΩ –º–ª–Ω)",        "_total_debt"),
                ("–î–µ–Ω–µ–∂–Ω—ã–µ —Å—Ä–µ–¥—Å—Ç–≤–∞ (‚ÇΩ –º–ª–Ω)",      "_cash"),
                ("–ö—Ä–µ–¥–∏—Ç–æ—Ä—Å–∫–∞—è –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç—å (‚ÇΩ –º–ª–Ω)", "1520"),
                ("–ß–∏—Å—Ç—ã–π –¥–æ–ª–≥ (‚ÇΩ –º–ª–Ω)",            "_net_debt"),
                ("EBIT margin (%)",                "_ebit_margin"),
                ("Net Debt / EBIT",                "_netdebt_ebit"),
            ]
            
            # ---------- ‚ë† —Å–≤–æ–¥–Ω–∞—è –≤–∫–ª–∞–¥–∫–∞, –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ ----------
            def build_agg_finances() -> dict[str, dict[str, float | None]]:
                """–°—É–º–º–∏—Ä—É–µ—Ç –≤—Å–µ –ò–ù–ù –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å agg[year][code]."""
                NUMERIC = {c for _, c in PNL_CODES if c.isdigit()} | {"1250", "1400", "1500"}
                raw = {y: defaultdict(float) for y in YEARS}
            
                for inn in inns:
                    fin = ck_fin(inn)
                    for y in YEARS:
                        for code in NUMERIC:
                            v = fin.get(y, {}).get(code)
                            if isinstance(v, (int, float)):
                                raw[y][code] += v / 1e6        # ‚Üí –º–ª–Ω
            
                agg = {}
                for y in YEARS:
                    rev   = raw[y]["2110"]
                    ebit  = raw[y]["2200"]
                    cash  = raw[y]["1250"]
                    debt  = raw[y]["1400"] + raw[y]["1500"]
                    pay   = raw[y]["1520"]
            
                    net_debt    = debt - cash - pay
                    ebit_margin = _safe_div(ebit, rev)
                    ebit_margin = ebit_margin * 100 if ebit_margin is not None else None
                    nd_ebit     = _safe_div(net_debt, ebit)
            
            
                    agg[y] = {**raw[y],
                              "_total_debt":  debt,
                              "_cash":        cash,
                              "_net_debt":    net_debt,
                              "_ebit_margin": ebit_margin,
                              "_netdebt_ebit":nd_ebit}
                return agg
            
            # —Å–æ–∑–¥–∞—ë–º –≤–∫–ª–∞–¥–∫–∏ –∑–∞—Ä–∞–Ω–µ–µ (—á—Ç–æ–±—ã –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è tabs –°–£–©–ï–°–¢–í–û–í–ê–õ–ê –≤—Å–µ–≥–¥–∞)
            if aggregate_mode:
                tabs = st.tabs(["Œ£ –°–≤–æ–¥–Ω–æ"] + ([] if len(inns) == 1 else
                                               [f"{n} ({inn})" for inn, n in zip(inns, names_full)]))
                # –±–ª–æ–∫ Œ£ –°–≤–æ–¥–Ω–æ ‚Äî –≤—Å–µ–≥–¥–∞ –ø–µ—Ä–≤—ã–π
                with tabs[0]:
                    agg = build_agg_finances()
            
                    st.header("Œ£ –°–≤–æ–¥–Ω–∞—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞")
                    tbl = pd.DataFrame({"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å": [n for n, _ in PNL_CODES]})
                    for y in YEARS:
                        tbl[y] = [agg[y].get(code) for _, code in PNL_CODES]
            
                    def _fmt(v, pct=False, d=1):
                        if v is None or (isinstance(v, float) and np.isnan(v)): return "‚Äî"
                        return f"{v:.{d}f}{'%' if pct else ''}".replace(".", ",")
                    for i, (nm, _) in enumerate(PNL_CODES):
                        pct  = nm.endswith("%")
                        digs = 2 if ("Net" in nm or pct) else 1
                        tbl.iloc[i, 1:] = [_fmt(v, pct, digs) for v in tbl.iloc[i, 1:]]
            
                    st.dataframe(tbl.set_index("–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å"),
                                 use_container_width=True,
                                 height=min(880, 40 * len(PNL_CODES)))
            
            
                    # –≥—Ä–∞—Ñ–∏–∫
                    # --- –≥—Ä–∞—Ñ–∏–∫: –≤—ã—Ä—É—á–∫–∞ / EBIT / —á–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å + EBIT-margin -----------
                    fig, ax1 = plt.subplots(figsize=(7, 3.5))
                    x = np.arange(len(YEARS)); w = 0.25
                    
                    bars_rev  = ax1.bar(x - w, [agg[y]["2110"] or 0 for y in YEARS],
                                        w, label="–í—ã—Ä—É—á–∫–∞")
                    bars_ebit = ax1.bar(x,     [agg[y]["2200"] or 0 for y in YEARS],
                                        w, label="EBIT")
                    bars_prof = ax1.bar(x + w, [agg[y]["2400"] or 0 for y in YEARS],
                                        w, label="–ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å")
                    
                    # –ø–æ–¥–ø–∏—Å–∏ –Ω–∞ —Å—Ç–æ–ª–±—Ü–∞—Ö
                    for b in (*bars_rev, *bars_ebit, *bars_prof):
                        h = b.get_height()
                        if h and not np.isnan(h):
                            ax1.annotate(f"{h:.1f}", xy=(b.get_x() + b.get_width()/2, h),
                                         xytext=(0, 3), textcoords="offset points",
                                         ha="center", fontsize=8)
                    
                    # –ª–∏–Ω–∏—è EBIT-margin (%)
                    ax2 = ax1.twinx()
                    margins = [agg[y]["_ebit_margin"] if agg[y]["_ebit_margin"] else np.nan for y in YEARS]
                    ax2.plot(x, margins, linestyle="--", marker="o", label="EBIT margin, %")
                    
                    # –ø–æ–¥–ø–∏—Å–∏ ¬´—Ö %¬ª –Ω–∞–¥ —Ç–æ—á–∫–∞–º–∏ –ª–∏–Ω–∏–∏
                    for xx, yy in zip(x, margins):
                        if not np.isnan(yy):
                            ax2.annotate(f"{yy:.1f}%", xy=(xx, yy),
                                         xytext=(0, 5), textcoords="offset points",
                                         ha="center", fontsize=8)
                    
                    # –æ–±—ä–µ–¥–∏–Ω—è–µ–º –ª–µ–≥–µ–Ω–¥—É
                    h1, l1 = ax1.get_legend_handles_labels()
                    h2, l2 = ax2.get_legend_handles_labels()
                    ax1.legend(h1 + h2, l1 + l2, loc="upper left", fontsize=9)
                    # ‚üµ  –ø—Ä—è—á–µ–º —à–∫–∞–ª—ã Y
                    ax1.set_yticks([]); ax2.set_yticks([])
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                    
                    for ax in (ax1, ax2):
                        ax.spines[:].set_visible(False)
                    
                    # –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ
                    ax1.set_xticks(x); ax1.set_xticklabels(YEARS, fontsize=10)
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                    for ax in (ax1, ax2):
                        ax.spines[:].set_visible(False)
                    
                    fig.tight_layout(pad=1.0)
                    st.pyplot(fig)
            
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ü—Ä–æ—Ñ–∏–ª—å / —Ä—ã–Ω–æ–∫ / —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ (+ –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    first_name = names_full[0] or "–ö–æ–º–ø–∞–Ω–∏—è"
                    first_mkt  = mkts_full[0]
                    first_site = sites_full[0]
                    first_inn = inns[0] if inns else None
                    
                    # --- –µ–¥–∏–Ω—ã–π RAG-–ø–∞–π–ø–ª–∞–π–Ω (Google-—Å–Ω–∏–ø–ø–µ—Ç—ã + —Å–∞–π—Ç) ---------------------
                    st.subheader("üìù –û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏")
                    desc_legacy = st.toggle("Legacy (Google/SiteRAG) description", value=False, key="desc_first")
                    
                    if desc_legacy:
                        with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ (Legacy)‚Ä¶"):
                            doc = RAG(first_name, website=first_site, market=first_mkt).run()
                    
                        html_main = linkify_as_word(doc["summary"]).replace("\n", "<br>")
                        st.markdown(
                            f"<div style='background:#F7F9FA;border:1px solid #ccc;"
                            f"border-radius:8px;padding:18px;line-height:1.55'>{html_main}</div>",
                            unsafe_allow_html=True,
                        )
                    
                        with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                            for i, q in enumerate(doc["queries"], 1):
                                st.markdown(f"**{i}.** {q}")
                    
                        with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                            st.dataframe(
                                pd.DataFrame(doc["snippets"], columns=["URL", "Snippet"]).head(15),
                                use_container_width=True,
                            )
                    
                        if doc.get("site_pass"):
                            with st.expander("üåê –ü–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞"):
                                st.markdown(
                                    f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                                    f"border-radius:8px;padding:18px;line-height:1.55'>"
                                    f"{_linkify(doc['site_pass']).replace(chr(10), '<br>')}</div>",
                                    unsafe_allow_html=True,
                                )
                        else:
                            st.info("–ü–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ –Ω–µ –ø–æ–ª—É—á–µ–Ω (–Ω–µ—Ç URL, –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ –∏—Å—Ç–µ–∫ —Ç–∞–π-–∞—É—Ç).")
                    
                    else:
                        with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º INVEST SNAPSHOT‚Ä¶"):
                            company_info_first = {
                                "leaders_raw":  (df_companies.loc[0, "leaders_raw"]  if "leaders_raw"  in df_companies.columns else []) or [],
                                "founders_raw": (df_companies.loc[0, "founders_raw"] if "founders_raw" in df_companies.columns else []) or [],
                            }
                            
                            inv = get_invest_snapshot_enriched(
                                first_name,
                                site_hint=first_site,
                                company_info=company_info_first,
                                market=first_mkt,
                                model="sonar",
                                recency=None,
                                max_tokens=1500
                            )
                    
                        # inv['md'] —É–∂–µ Markdown ‚Üí –±–µ–∑ _linkify
                        st.markdown(
                            f"<div style='background:#F7F9FA;border:1px solid #ccc;"
                            f"border-radius:8px;padding:18px;line-height:1.55'>{inv['md']}</div>",
                            unsafe_allow_html=True,
                        )
                        with st.expander("üîß –û—Ç–ª–∞–¥–∫–∞ (—Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç)"):
                            st.text(inv["raw"] or "‚Äî")
                    
                    # (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –∑–∞–ø–æ–º–∏–Ω–∞–µ–º ¬´doc¬ª –¥–ª—è session_state
                    doc = doc if desc_legacy else {"summary": inv["md"], "mode": "invest_snapshot"}
                    
                    # ----------- –†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç (MARKET EVIDENCE) ------------------------
                    if first_mkt:
                        st.subheader("üìà –†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç")
                        legacy = st.toggle("Legacy (Google/GPT) mode", value=False, key="legacy_first")
                    
                        if legacy:
                            with st.spinner("–°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ä—ã–Ω–∫—É (Legacy)‚Ä¶"):
                                mkt_res = get_market_rag(first_mkt)
                            mkt_html = _linkify(mkt_res["summary"]).replace("\n", "<br>")
                            st.markdown(
                                f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                                f"border-radius:8px;padding:18px;line-height:1.55'>{mkt_html}</div>",
                                unsafe_allow_html=True,
                            )
                            with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                                for i, q in enumerate(mkt_res["queries"], 1):
                                    st.markdown(f"**{i}.** {q}")
                            with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                                st.dataframe(
                                    pd.DataFrame(mkt_res["snippets"], columns=["URL", "Snippet"]).head(15),
                                    use_container_width=True,
                                )
                        else:
                            with st.spinner("–°–æ–±–∏—Ä–∞–µ–º MARKET EVIDENCE‚Ä¶"):
                                ev = get_market_evidence(first_mkt, country="–†–æ—Å—Å–∏—è", min_sources=8, recency=None)
                            st.markdown(
                                f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                                f"border-radius:8px;padding:18px;line-height:1.55'>{ev['text_html']}</div>",
                                unsafe_allow_html=True,
                            )
                            st.caption("–°–¢–†–£–ö–¢–£–†–ê –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º ‚Äî –¥–µ–Ω—å–≥–∏:")
                            st.code(ev["money_block"] or "‚Äî", language="text")
                            st.caption("–°–¢–†–£–ö–¢–£–†–ê –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º ‚Äî –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—ã–µ –æ–±—ä—ë–º—ã:")
                            st.code(ev["natural_block"] or "‚Äî", language="text")
                    
                            # (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –æ—Ç–ª–∞–¥–∫–∞ —Å—ã—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                            with st.expander("üîß –û—Ç–ª–∞–¥–∫–∞ (—Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç)"):
                                st.text(ev["raw_text"] or "‚Äî")

                        if "mkt_res" not in locals():
                            mkt_res = {}
                    
                    # ----------- –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é -----------------------------------
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    st.subheader("üë• –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é")
                    
                    use_legacy_leaders_first = st.toggle(
                        "–ü–æ–∫–∞–∑–∞—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∏–Ω—Ç–µ—Ä–≤—å—é (legacy)",
                        value=False,
                        key="leaders_global"  # <- –±—ã–ª–æ "leaders_first"
                    )
                    
                    if use_legacy_leaders_first:
                        with st.spinner("–°–æ–±–∏—Ä–∞–µ–º —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏ –∏–Ω—Ç–µ—Ä–≤—å—é (legacy)‚Ä¶"):
                            # –±–µ—Ä—ë–º –¢–û–õ–¨–ö–û –Ω—É–∂–Ω—ã–µ –∫–ª—é—á–∏ –∏–∑ df_companies (–∞ –Ω–µ –≤—Å—é –∫–∞—Ä—Ç–æ—á–∫—É)
                            company_info = {
                                "leaders_raw":  (df_companies.loc[0, "leaders_raw"]  if "leaders_raw"  in df_companies.columns else []) or [],
                                "founders_raw": (df_companies.loc[0, "founders_raw"] if "founders_raw" in df_companies.columns else []) or [],
                            }
                    
                            lead_res = get_leaders_rag(
                                first_name,
                                website=first_site,
                                market=first_mkt,
                                company_info=company_info,  # ‚Üê –∏–º–µ–Ω–∞ –∏–∑ Checko ‚Üí –¥–∞–ª—å—à–µ –ø–æ–∏—Å–∫ –∏–Ω—Ç–µ—Ä–≤—å—é –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
                            )
                    
                        st.markdown(
                            f"<div style='background:#F9FAFB;border:1px solid #ddd;"
                            f"border-radius:8px;padding:18px;line-height:1.55'>"
                            f"{lead_res['summary'].replace(chr(10), '<br>')}</div>",
                            unsafe_allow_html=True,
                        )
                    
                        with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                            for i, q in enumerate(lead_res["queries"], 1):
                                st.markdown(f"**{i}.** {q}")
                    
                        with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                            if lead_res["snippets"]:
                                df = (
                                    pd.DataFrame(lead_res["snippets"], columns=["URL", "Snippet"])
                                    .drop_duplicates(subset="URL")
                                    .head(15)
                                )
                                st.dataframe(df, use_container_width=True)
                            else:
                                st.info("–°–Ω–∏–ø–ø–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
                    else:
                        st.info("–ë–∞–∑–æ–≤—ã–µ –∏–Ω—Ç–µ—Ä–≤—å—é —É–∂–µ –µ—Å—Ç—å –≤ –±–ª–æ–∫–µ ¬´–ò–Ω—Ç–µ—Ä–≤—å—é (–≤–ª–∞–¥–µ–ª—å—Ü—ã/—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ)¬ª –≤–Ω—É—Ç—Ä–∏ INVEST SNAPSHOT –≤—ã—à–µ. "
                                "–í–∫–ª—é—á–∏—Ç–µ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å, —á—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∏–º–µ–Ω–∞–º –∏–∑ Checko –∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ.")

                    # --- —Å—Ç—Ä–∞—Ö–æ–≤–∫–∏, –µ—Å–ª–∏ —Ç—É–º–±–ª–µ—Ä –±—ã–ª –≤—ã–∫–ª—é—á–µ–Ω (–ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–µ —Å–æ–∑–¥–∞–ª–∏—Å—å) ---
                    if "lead_res" not in locals():
                        lead_res = {"summary": "", "queries": [], "snippets": []}
                    
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –∫–æ–Ω–µ—Ü –±–ª–æ–∫–∞, –¥–∞–ª—å—à–µ –≤–∞—à –∫–æ–¥ (–µ—Å–ª–∏ –±—ã–ª) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            
            # ---------- ‚ë° –≤–∫–ª–∞–¥–∫–∏ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –∫–æ–º–ø–∞–Ω–∏—è–º ----------
            if aggregate_mode and len(inns) > 1:
                tabs = st.tabs(["Œ£ –°–≤–æ–¥–Ω–æ"] + [f"{n} ({inn})"
                                               for inn, n in zip(inns, names_full)])
            else:                                   # –æ–¥–∏–Ω–æ—á–Ω—ã–π —Ä–µ–∂–∏–º
                tabs = st.tabs([f"{n} ({inn})" for inn, n
                                in zip(inns, names_full)])
            
            start_idx = 1 if (aggregate_mode and len(inns) > 1) else 0
            
            for idx, (tab, inn, name, mkt, site) in enumerate(
                    zip(
                        tabs[start_idx:],   # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º Œ£-–≤–∫–ª–∞–¥–∫—É –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                        inns,
                        names_full,
                        mkts_full,
                        sites_full,
                    )
            ):
                with tab:
                    st.header(f"{name} ‚Äî {inn}")
                    st.caption(f"–†—ã–Ω–æ–∫: **{mkt or '‚Äî'}**")
            
                    # ---------- –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –ø—Ä–æ—Ñ–∏–ª—å ----------
                    fin = ck_fin(inn)
                    calc = {y: {} for y in YEARS}
            
                    for y in YEARS:
                        yr = fin.get(y, {})
                        # –ø—Ä—è–º—ã–µ —Å—Ç—Ä–æ–∫–∏ –æ—Ç—á—ë—Ç–∞
                        for _, code in PNL_CODES:
                            if code.isdigit():
                                v = yr.get(code)
                                calc[y][code] = (v / 1e6) if isinstance(v, (int, float)) else None
            
                        # —Ä–∞—Å—á—ë—Ç–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
                        cash  = yr.get("1250", 0) / 1e6
                        debt  = (yr.get("1400", 0) + yr.get("1500", 0)) / 1e6
                        pay   = yr.get("1520", 0) / 1e6
                        rev   = calc[y].get("2110")
                        ebit  = calc[y].get("2200")
                        net_debt = debt - cash - pay
                        ebit_m = _safe_div(ebit, rev)
                        ebit_m = ebit_m * 100 if ebit_m is not None else None
                        nd_eb     = _safe_div(net_debt, ebit)
            
                        calc[y].update({
                            "_total_debt": debt or None,
                            "_cash":       cash or None,
                            "_net_debt":   net_debt if rev is not None else None,
                            "_ebit_margin":ebit_m,
                            "_netdebt_ebit":nd_eb,
                        })
            
                    # --- –¥–ª–∏–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ ---
                    tbl = pd.DataFrame({"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å": [n for n, _ in PNL_CODES]})
                    for y in YEARS:
                        tbl[y] = [calc[y].get(code) for _, code in PNL_CODES]
            
                    def fmt(v, pct=False, d=1):
                        if v is None or (isinstance(v, float) and np.isnan(v)): return "‚Äî"
                        return f"{v:.{d}f}{'%' if pct else ''}".replace(".", ",")
            
                    for i, (nm, _) in enumerate(PNL_CODES):
                        pct  = nm.endswith("%")
                        digs = 2 if ("Net" in nm or pct) else 1
                        tbl.iloc[i, 1:] = [fmt(v, pct, digs) for v in tbl.iloc[i, 1:]]
            
                    st.dataframe(tbl.set_index("–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å"),
                                 use_container_width=True,
                                 height=min(880, 40 * len(tbl)))
            
                    # --- –≥—Ä–∞—Ñ–∏–∫: –≤—ã—Ä—É—á–∫–∞ / EBIT / —á–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å + EBIT-margin ---
                    fig, ax1 = plt.subplots(figsize=(7, 3.5))
                    x = np.arange(len(YEARS)); w = 0.25
                    bars_r  = ax1.bar(x - w, [calc[y]["2110"] or 0 for y in YEARS], w, label="–í—ã—Ä—É—á–∫–∞")
                    bars_e  = ax1.bar(x,     [calc[y]["2200"] or 0 for y in YEARS], w, label="EBIT")
                    bars_p  = ax1.bar(x + w, [calc[y]["2400"] or 0 for y in YEARS], w, label="–ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å")
            
                    for b in (*bars_r, *bars_e, *bars_p):
                        h = b.get_height()
                        if h and not np.isnan(h):
                            ax1.annotate(f"{h:.1f}", xy=(b.get_x()+b.get_width()/2, h),
                                         xytext=(0,3), textcoords="offset points",
                                         ha="center", fontsize=8)
            
                    ax2 = ax1.twinx()
                    m_vals = [calc[y]["_ebit_margin"] if calc[y]["_ebit_margin"] else np.nan for y in YEARS]
                    ax2.plot(x, m_vals, linestyle="--", marker="o", label="EBIT margin, %")
                    # ----- –µ–¥–∏–Ω–∞—è –ª–µ–≥–µ–Ω–¥–∞ -----
                    h1, l1 = ax1.get_legend_handles_labels()   # bars
                    h2, l2 = ax2.get_legend_handles_labels()   # –ª–∏–Ω–∏—è margin
                    ax1.legend(h1 + h2, l1 + l2, loc='upper left', fontsize=9)
            
            
                    
                    for xx, yy in zip(x, m_vals):
                        if yy and not np.isnan(yy):
                            ax2.annotate(f"{yy:.1f}%", xy=(xx, yy),
                                         xytext=(0,5), textcoords="offset points",
                                         ha="center", fontsize=8)
            
                    ax1.set_xticks(x); ax1.set_xticklabels(YEARS, fontsize=10)
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                     # ‚üµ  –ø—Ä—è—á–µ–º —à–∫–∞–ª—ã Y
                    ax1.set_yticks([]); ax2.set_yticks([])
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                    
                    for ax in (ax1, ax2):
                        ax.spines[:].set_visible(False)
                    for ax in (ax1, ax2): ax.spines[:].set_visible(False)
                    ax1.legend(loc="upper left", fontsize=9)
                    fig.tight_layout(pad=1.0)
                    st.pyplot(fig)
            
                    
                    
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ (Google + —Å–∞–π—Ç) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    st.subheader("üìù –û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏")
                    desc_legacy_tab = st.toggle("Legacy (Google/SiteRAG) description", value=False, key=f"desc_{idx}")
                    
                    if desc_legacy_tab:
                        with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ (Legacy)‚Ä¶"):
                            doc = RAG(name, website=site, market=mkt).run()
                    
                        main_html = _linkify(doc["summary"]).replace("\n", "<br>")
                        st.markdown(
                            f"<div style='background:#F7F9FA;border:1px solid #ccc;"
                            f"border-radius:8px;padding:18px;line-height:1.55'>{main_html}</div>",
                            unsafe_allow_html=True
                        )
                    
                        with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                            for i, q in enumerate(doc["queries"], 1):
                                st.markdown(f"**{i}.** {q}")
                    
                        with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                            st.dataframe(
                                pd.DataFrame(doc["snippets"], columns=["URL", "Snippet"]).head(15),
                                use_container_width=True,
                            )
                    
                        if doc.get("site_pass"):
                            with st.expander("üåê –ü–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞"):
                                st.markdown(
                                    f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                                    f"border-radius:8px;padding:18px;line-height:1.55'>"
                                    f"{_linkify(doc['site_pass']).replace(chr(10), '<br>')}</div>",
                                    unsafe_allow_html=True,
                                )
                        else:
                            st.info("–ü–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ –Ω–µ –ø–æ–ª—É—á–µ–Ω (–Ω–µ—Ç URL, –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ –∏—Å—Ç–µ–∫ —Ç–∞–π-–∞—É—Ç).")
                    
                    else:
                        with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º INVEST SNAPSHOT‚Ä¶"):
                            inv = get_invest_snapshot(name, site_hint=site, model="sonar", recency=None, max_tokens=1500)
                    
                        st.markdown(
                            f"<div style='background:#F7F9FA;border:1px solid #ccc;"
                            f"border-radius:8px;padding:18px;line-height:1.55'>{inv['md']}</div>",
                            unsafe_allow_html=True
                        )
                        with st.expander("üîß –û—Ç–ª–∞–¥–∫–∞ (—Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç)"):
                            st.text(inv["raw"] or "‚Äî")
                    
                    # –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å session_state –Ω–∏–∂–µ
                    doc = doc if desc_legacy_tab else {"summary": inv["md"], "mode": "invest_snapshot"}
                    
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç (MARKET EVIDENCE) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    if mkt:
                        st.subheader("üìà –†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç")
                        legacy = st.toggle("Legacy (Google/GPT) mode", value=False, key=f"legacy_{idx}")
                    
                        if legacy:
                            with st.spinner("–°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ä—ã–Ω–∫—É (Legacy)‚Ä¶"):
                                mkt_res = get_market_rag(mkt)
                            mkt_html = _linkify(mkt_res["summary"]).replace("\n", "<br>")
                            st.markdown(
                                f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                                f"border-radius:8px;padding:18px;line-height:1.55'>{mkt_html}</div>",
                                unsafe_allow_html=True,
                            )
                            with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                                for i, q in enumerate(mkt_res["queries"], 1):
                                    st.markdown(f"**{i}.** {q}")
                            with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                                df_leg = pd.DataFrame(mkt_res["snippets"], columns=["URL", "Snippet"]).head(15)
                                st.dataframe(df_leg, use_container_width=True)
                        else:
                            with st.spinner("–°–æ–±–∏—Ä–∞–µ–º MARKET EVIDENCE‚Ä¶"):
                                ev = get_market_evidence(mkt, country="–†–æ—Å—Å–∏—è", min_sources=8, recency=None)
                            st.markdown(
                                f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                                f"border-radius:8px;padding:18px;line-height:1.55'>{ev['text_html']}</div>",
                                unsafe_allow_html=True,
                            )
                            st.caption("–°–¢–†–£–ö–¢–£–†–ê –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º ‚Äî –¥–µ–Ω—å–≥–∏:")
                            st.code(ev["money_block"] or "‚Äî", language="text")
                            st.caption("–°–¢–†–£–ö–¢–£–†–ê –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º ‚Äî –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—ã–µ –æ–±—ä—ë–º—ã:")
                            st.code(ev["natural_block"] or "‚Äî", language="text")
                    
                            with st.expander("üîß –û—Ç–ª–∞–¥–∫–∞ (—Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç)"):
                                st.text(ev["raw_text"] or "‚Äî")
                    
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    st.subheader("üë• –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é")
                    use_legacy_leaders = st.toggle(
                        "–ü–æ–∫–∞–∑–∞—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –∏–Ω—Ç–µ—Ä–≤—å—é (legacy)",
                        value=False,
                        key=f"leaders_{idx}"  # <- –≤–º–µ—Å—Ç–æ "leaders_first"
                    )
                    
                    if use_legacy_leaders:
                        with st.spinner("–°–æ–±–∏—Ä–∞–µ–º —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏ –∏–Ω—Ç–µ—Ä–≤—å—é (legacy)‚Ä¶"):
                            row_idx = inns.index(inn)  # –∏–Ω–¥–µ–∫—Å —Ç–µ–∫—É—â–µ–≥–æ –ò–ù–ù –≤ –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ–º —Å–ø–∏—Å–∫–µ
                            company_info = {
                                "leaders_raw":  (df_companies.loc[row_idx, "leaders_raw"]  if "leaders_raw"  in df_companies.columns else []) or [],
                                "founders_raw": (df_companies.loc[row_idx, "founders_raw"] if "founders_raw" in df_companies.columns else []) or [],
                            }
                        
                            lead_res = get_leaders_rag(
                                name,                # <- –±—ã–ª–æ first_name
                                website=site,        # <- –±—ã–ª–æ first_site
                                market=mkt,          # <- –±—ã–ª–æ first_mkt
                                company_info=company_info,
                            )
                        st.markdown(
                            f"<div style='background:#F9FAFB;border:1px solid #ddd;border-radius:8px;padding:18px;line-height:1.55'>"
                            f"{lead_res['summary'].replace(chr(10), '<br>')}</div>",
                            unsafe_allow_html=True,
                        )
                        with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                            for i, q in enumerate(lead_res["queries"], 1):
                                st.markdown(f"**{i}.** {q}")
                        with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                            if lead_res["snippets"]:
                                df = (
                                    pd.DataFrame(lead_res["snippets"], columns=["URL", "Snippet"])
                                    .drop_duplicates(subset="URL")
                                    .head(15)
                                )
                                st.dataframe(df, use_container_width=True)
                            else:
                                st.info("–°–Ω–∏–ø–ø–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
                    else:
                        st.info("–ò–Ω—Ç–µ—Ä–≤—å—é —Å–º–æ—Ç—Ä–∏—Ç–µ –≤ —Ä–∞–∑–¥–µ–ª–µ ¬´–ò–Ω—Ç–µ—Ä–≤—å—é (–≤–ª–∞–¥–µ–ª—å—Ü—ã/—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ)¬ª –≤–Ω—É—Ç—Ä–∏ INVEST SNAPSHOT –≤—ã—à–µ.")

        st.session_state["ai_report"] = {
            "doc":          doc,          # –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏
            "mkt_res":      mkt_res,      # —Ä—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç
            "lead_res":     lead_res,     # —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏/–∏–Ω—Ç–µ—Ä–≤—å—é
            "tbl":          tbl,          # —Ñ–∏–Ω. —Ç–∞–±–ª–∏—Ü–∞ DataFrame
            "graphics":     fig,          # –æ–±—ä–µ–∫—Ç matplotlib (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω –ø–æ–≤—Ç–æ—Ä–Ω—ã–π —Ä–µ–Ω–¥–µ—Ä)
            # ‚Ä¶ —á—Ç–æ-—É–≥–æ–¥–Ω–æ –µ—â—ë
        }
        st.session_state["ai_result_ready"] = True

def long_job(total_sec: int = 180, key_prog: str = "ai_prog"):
    """–§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞, –∫–∞–∂–¥—ã–µ 1 —Å –æ–±–Ω–æ–≤–ª—è–µ—Ç progress –≤ session_state."""
    for i in range(total_sec + 1):
        time.sleep(1)
        st.session_state[key_prog] = i / total_sec     # 0 ‚Ä¶ 1
    st.session_state["ai_done"] = True                 # –æ—Ç—á—ë—Ç –≥–æ—Ç–æ–≤

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 2. UI-—Ñ—É–Ω–∫—Ü–∏–∏ –¥–≤—É—Ö –≤–∫–ª–∞–¥–æ–∫
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def run_advance_eye_tab() -> None:
    st.header("üëÅÔ∏è Advance Eye")

    user_query = st.text_input("–í–≤–µ–¥–∏—Ç–µ –ò–ù–ù –∏–ª–∏ –§–ò–û")
    if st.button("üîç –ù–∞–π—Ç–∏ –∫–æ–Ω—Ç–∞–∫—Ç—ã") and user_query:
        with st.spinner("–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º Dyxless‚Ä¶"):
            res = dyxless_query(user_query, token=DYXLESS_TOKEN, max_rows=20_000)

        if res.get("status"):
            st.success(f"–ü–æ–∫–∞–∑–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: **{res['counts']}**")
            st.json(res["data"] or {"note": "–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"})
        else:
            st.warning(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {res.get('error', '–ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç')}")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ (–æ–¥–∏–Ω —Ä–∞–∑ –∑–∞ —Å–µ—Å—Å–∏—é)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.session_state.setdefault("ai_prog", None)   # float 0‚Ä¶1 –∏–ª–∏ None
st.session_state.setdefault("ai_done", False)  # –æ—Ç—á—ë—Ç –≥–æ—Ç–æ–≤?

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 4. –î–≤–µ –≤–∫–ª–∞–¥–∫–∏
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
tab_ai, tab_eye = st.tabs(["üìä AI-Insight", "üëÅÔ∏è Advance Eye"])

# === –≤–∫–ª–∞–¥–∫–∞ 1: AI-Insight =========================================

with tab_ai:
    run_ai_insight_tab()       # –≤—Å—è –ª–æ–≥–∏–∫–∞ –≤–Ω—É—Ç—Ä–∏

with tab_eye:
    run_advance_eye_tab()      # –ø–æ–∏—Å–∫ Dyxless



