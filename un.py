#!/usr/bin/env python
# coding: utf-8

# In[ ]:


#!/usr/bin/env python
# coding: utf-8

# In[6]:


# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º API-–∫–ª—é—á OpenAI
import os
import requests
import pandas as pd
import re
import numpy as np
import requests
from bs4 import BeautifulSoup
import openai
from typing import List, Dict, Any, Tuple
import json
import streamlit as st
from collections import defaultdict
import asyncio, aiohttp, re, textwrap, nest_asyncio, openai, tiktoken
from collections import defaultdict
from urllib.parse import urlparse
import tldextract, re, asyncio, aiohttp                      # –µ—Å–ª–∏ tldextract —É–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω ‚Äì —ç—Ç—É —Å—Ç—Ä–æ–∫—É –º–æ–∂–Ω–æ –∫–æ—Ä–æ—á–µ
from functools import partial  
import threading
import time
import functools
KEYS = {
    "OPENAI_API_KEY": st.secrets["OPENAI_API_KEY"],
    "GOOGLE_API_KEY": st.secrets["GOOGLE_API_KEY"],
    "GOOGLE_CX":      st.secrets["GOOGLE_CX"],
    "CHECKO_API_KEY": st.secrets["CHECKO_API_KEY"],
    "DYXLESS_TOKEN": st.secrets["DYXLESS_TOKEN"]
}

DYXLESS_TOKEN = KEYS["DYXLESS_TOKEN"]


# In[ ]:


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ app.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import os, re, asyncio, aiohttp, requests, nest_asyncio, logging
import streamlit as st
import pandas as pd, numpy as np, matplotlib.pyplot as plt

nest_asyncio.apply()
logging.basicConfig(level=logging.WARNING)
import os, re, html, textwrap, asyncio, logging, nest_asyncio
import aiohttp, requests, streamlit as st
import pandas as pd, numpy as np, matplotlib.pyplot as plt
import tldextract, openai


import html, re

_URL_PAT = re.compile(r"https?://[^\s)]+", flags=re.I)

def _linkify(text) -> str:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç URL –≤ <a ‚Ä¶>—Å—Å—ã–ª–∫–∞</a>."""
    if not isinstance(text, str):                      # << –≥–ª–∞–≤–Ω–æ–µ
        text = "" if text is None else str(text)

    def repl(m):
        u = html.escape(m.group(0))
        return f'<a href="{u}" target="_blank">—Å—Å—ã–ª–∫–∞</a>'
    return _URL_PAT.sub(repl, text)



def long_job(total: int, key: str):
    """–î–æ–ª–≥–∞—è –∑–∞–¥–∞—á–∞: –ø–∏—à–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ st.session_state[key]"""
    for i in range(total + 1):
        time.sleep(1)                         # –∑–¥–µ—Å—å –≤–∞—à–∞ —Ç—è–∂—ë–ª–∞—è –ª–æ–≥–∏–∫–∞
        st.session_state[key] = i / total     # –æ—Ç 0.0 –¥–æ 1.0
    st.session_state[key] = 1.0               # —Ñ–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º



# ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
async def _site_snippet(domain: str) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—ã–π Google-—Å–Ω–∏–ø–ø–µ—Ç –¥–ª—è site:domain (–∏–ª–∏ '')."""
    if not domain:
        return ""
    async with aiohttp.ClientSession() as sess:
        q = f"site:{domain}"
        snips = await _google(sess, q, n=1)
    return snips[0][1] if snips else ""



@st.cache_data(ttl=3_600, show_spinner=False)
def dyxless_query(query: str,
                  token: str,
                  max_rows: int = 20_000) -> Dict[str, Any]:
    """
    –û–±—ë—Ä—Ç–∫–∞ Dyxless. –ï—Å–ª–∏ –∑–∞–ø–∏—Å–µ–π > max_rows ‚Äì –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ max_rows,
    –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–∏—à–µ–º truncated=True –∏ original_counts.
    """
    url = "https://api-dyxless.cfd/query"
    try:
        r = requests.post(url, json={"query": query, "token": token}, timeout=15)
        r.raise_for_status()
        res = r.json()

        if res.get("status") and isinstance(res.get("data"), list):
            full = len(res["data"])
            if full > max_rows:
                res["data"] = res["data"][:max_rows]
                res.update(truncated=True, original_counts=full, counts=max_rows)
        return res
    except requests.RequestException as e:
        return {"status": False, "error": str(e)}












# ‚ï≠‚îÄüîß  –≤—Å–ø–æ–º–æ–≥–∞–ª–∫–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
_BAD = ("vk.com", "facebook.", ".pdf", ".jpg", ".png")
HEADERS = {"User-Agent": "Mozilla/5.0 (Win64) AppleWebKit/537.36 Chrome/125 Safari/537.36"}
def _bad(u: str) -> bool: return any(b in u.lower() for b in _BAD)

async def _google(sess, q, n=3):
    q = re.sub(r'[\"\'‚Äú‚Äù]', " ", q)[:80]
    params = {"key": KEYS["GOOGLE_API_KEY"], "cx": KEYS["GOOGLE_CX"],
              "q": q, "num": n, "hl": "ru", "gl": "ru"}
    async with sess.get("https://www.googleapis.com/customsearch/v1",
                         params=params, headers=HEADERS, timeout=8) as r:
        if r.status != 200:
            logging.warning(f"Google error {r.status}")
            return []
        js = await r.json()
        return [(i["link"], i.get("snippet", "")) for i in js.get("items", [])
                if not _bad(i["link"])]






async def _gpt(messages, *, model="gpt-4o-mini", T=0.2):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ OpenAI ChatCompletion ‚Üí str."""
    chat = await openai.ChatCompletion.acreate(
        model=model, temperature=T, messages=messages)
    return chat.choices[0].message.content.strip()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –æ—Å–Ω–æ–≤–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞ (–≤ —Å—Ç–∏–ª–µ –≤–∞—à–µ–≥–æ RAG) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class SiteRAG:
    """
    url        ‚Äì –∞–¥—Ä–µ—Å —Å–∞–π—Ç–∞ (–º–æ–∂–Ω–æ –±–µ–∑ http/https)
    max_chunk  ‚Äì –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ–¥–Ω–æ–≥–æ –∫—É—Å–∫–∞ HTML, –∫–æ—Ç–æ—Ä—ã–π —É–π–¥—ë—Ç LLM
    summary    ‚Äì –∏—Ç–æ–≥–æ–≤—ã–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç –∫–æ–º–ø–∞–Ω–∏–∏
    chunks_out ‚Äì —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—é–º–µ c –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏ HTML
    html_size  ‚Äì —Ä–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ HTML-—Ñ–∞–π–ª–∞, bytes
    """
    def __init__(self, url: str, *, model="gpt-4o-mini",
                 max_chunk: int = 6_000, T: float = 0.18):
        if not url.startswith(("http://", "https://")):
            url = "http://" + url
        self.url       = url
        self.model     = model
        self.max_chunk = max_chunk
        self.T         = T

    # ---------- 1. —Å–∫–∞—á–∏–≤–∞–µ–º HTML ------------------------------------------------
    async def _fetch(self) -> str:
        h = {"User-Agent": "Mozilla/5.0"}
        async with aiohttp.ClientSession(headers=h) as sess:
            async with sess.get(self.url, timeout=20) as r:
                if r.status == 200 and "text/html" in r.headers.get("Content-Type", ""):
                    return await r.text("utf-8", errors="ignore")
                raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å {self.url} (status={r.status})")

    # ---------- 2. –¥–µ–ª–∏–º HTML –Ω–∞ ¬´–±–µ–∑–æ–ø–∞—Å–Ω—ã–µ¬ª –∫—É—Å–∫–∏ -----------------------------
    def _split(self, html_raw: str) -> list[str]:
        # –ø—Ä–æ–±—É–µ–º —Ä–µ–∑–∞—Ç—å –ø–æ –∫—Ä—É–ø–Ω—ã–º —Ç–µ–≥–∞–º, —á—Ç–æ–±—ã –∫—É—Å–∫–∏ –±—ã–ª–∏ —Å–≤—è–∑–Ω—ã
        body = re.split(r"</?(?:body|div|section|article)[^>]*>", html_raw, flags=re.I)
        chunks, buf = [], ""
        for part in body:
            if len(buf) + len(part) > self.max_chunk:
                chunks.append(buf); buf = part
            else:
                buf += part
        if buf:
            chunks.append(buf)
        return chunks

    # ---------- 3. map-—Ñ–∞–∑–∞: –∫–æ–Ω—Å–ø–µ–∫—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∫—É—Å–æ–∫ -------------------------
    async def _summarise_chunk(self, n: int, total: int, chunk: str) -> str:
        sys = (
            "–¢—ã ‚Äì –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –ü—Ä–æ—á–∏—Ç–∞–π –¥–∞–Ω–Ω—ã–π HTML-—Ñ—Ä–∞–≥–º–µ–Ω—Ç –∏ "
            "–≤—ã–ø–∏—à–∏ –í–°–ï –∑–Ω–∞—á–∏–º—ã–µ —Ñ–∞–∫—Ç—ã –æ –∫–æ–º–ø–∞–Ω–∏–∏ (–ø—Ä–æ–¥—É–∫—Ç—ã, —É—Å–ª—É–≥–∏, –∏—Å—Ç–æ—Ä–∏—è, "
            "–≥–µ–æ–≥—Ä–∞—Ñ–∏—è, –∫–ª–∏–µ–Ω—Ç—ã, —Ü–∏—Ñ—Ä—ã, –∫–æ–º–∞–Ω–¥–∞, –∫–æ–Ω—Ç–∞–∫—Ç—ã –∏ –ø—Ä.). "
            "–£–¥–∞–ª–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏—é/footer/—Å–∫—Ä–∏–ø—Ç—ã. –°–æ—Ö—Ä–∞–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞–±–∑–∞—Ü–∞–º–∏."
        )
        return await _gpt([
            {"role": "system", "content": sys},
            {"role": "user",
             "content": f"HTML_CHUNK_{n}/{total} (len={len(chunk):,}):\n{chunk}"}],
            model=self.model, T=self.T)

    # ---------- 4. reduce-—Ñ–∞–∑–∞: –¥–µ–ª–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç ------------------------
    async def _summarise_overall(self, parts: list[str]) -> str:
        sys = (
            "–ù–∏–∂–µ —É–∂–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–Ω—Å–ø–µ–∫—Ç—ã —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–µ–π —Å–∞–π—Ç–∞. "
            "–ù–∞ –∏—Ö –æ—Å–Ω–æ–≤–µ —Å–æ—Å—Ç–∞–≤—å –æ–¥–∏–Ω –ü–û–õ–ù–´–ô –∏ —Å–≤—è–∑–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç –∫–æ–º–ø–∞–Ω–∏–∏: "
            "‚Ä¢ –∫—Ç–æ –æ–Ω–∏ –∏ —á–µ–º –∑–∞–Ω–∏–º–∞—é—Ç—Å—è; ‚Ä¢ –ø—Ä–æ–¥—É–∫—Ç—ã / —É—Å–ª—É–≥–∏; ‚Ä¢ —Ä—ã–Ω–æ–∫ –∏ –∫–ª–∏–µ–Ω—Ç—ã; "
            "‚Ä¢ –∏—Å—Ç–æ—Ä–∏—è –∏ –∫–ª—é—á–µ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è; ‚Ä¢ –≥–µ–æ–≥—Ä–∞—Ñ–∏—è –∏ –º–∞—Å—à—Ç–∞–±—ã; "
            "‚Ä¢ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ / –∫–æ–º–∞–Ω–¥–∞; ‚Ä¢ –ª—é–±—ã–µ —Ü–∏—Ñ—Ä—ã –∏ —Ñ–∞–∫—Ç—ã; "
            "‚Ä¢ –≤—ã–≤–æ–¥ –æ –ø–æ–∑–∏—Ü–∏–∏ –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞—Ö. –ù–∏—á–µ–≥–æ –≤–∞–∂–Ω–æ–≥–æ –Ω–µ —É–ø—É—Å—Ç–∏."
        )
        merged = "\n\n".join(parts)
        return await _gpt([
            {"role": "system", "content": sys},
            {"role": "user",   "content": merged}],
            model=self.model, T=self.T)

    # ---------- orchestrator ----------------------------------------------------
    async def _run_async(self):
        html_raw = await self._fetch()
        chunks   = self._split(html_raw)

        # map
        part_summaries = []
        for idx, ch in enumerate(chunks, 1):
            print(f"‚Üí LLM chunk {idx}/{len(chunks)} ‚Ä¶")
            part_summaries.append(await self._summarise_chunk(idx, len(chunks), ch))

        # reduce
        summary_final = await self._summarise_overall(part_summaries)

        return {"summary":    summary_final,
                "chunks_out": part_summaries,
                "html_size":  f"{len(html_raw):,} bytes",
                "url":        self.url}

    # ---------- –ø—É–±–ª–∏—á–Ω—ã–π —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å ----------------------------------
    def run(self) -> dict:
        loop = asyncio.get_event_loop()
        if loop and loop.is_running():
            return loop.run_until_complete(self._run_async())
        return asyncio.run(self._run_async())




# ---------- helper: —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –¥–æ—Å—Ç–∞—ë–º –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ -----------------
def _site_passport_sync(url: str, *, max_chunk: int = 6_000) -> str:
    """–í—ã–∑—ã–≤–∞–µ—Ç SiteRAG(url).run() –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ summary."""
    try:
        return SiteRAG(url, max_chunk=max_chunk).run()["summary"]
    except Exception as exc:
        return f"[site passport error: {exc}]"



class RAG:
    """
    summary    ‚Äì —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç (Google-—Å–Ω–∏–ø–ø–µ—Ç—ã + –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞)
    queries    ‚Äì –∑–∞–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ LLM —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞ –¥–ª—è Google
    snippets   ‚Äì —Å–ø–∏—Å–æ–∫ (url, text) –∏–∑ Google
    site_ctx   ‚Äì –∫–æ—Ä–æ—Ç–∫–∏–π —Å–Ω–∏–ø–ø–µ—Ç ¬´site:<–¥–æ–º–µ–Ω> ‚Ä¶¬ª
    site_pass  ‚Äì –ø–æ–¥—Ä–æ–±–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ (–≥–æ—Ç–æ–≤—ã–π summary –æ—Ç SiteRAG)
    """
    def __init__(self, company: str, *, website: str = "", market: str = "",
                 years=(2022, 2023, 2024), country: str = "–†–æ—Å—Å–∏—è",
                 steps: int = 3, snips: int = 4,
                 llm_model: str = "gpt-4o-mini",company_info: dict | None = None,):
        self.company   = company.strip()
        self.website   = website.strip()
        self.market    = market.strip()
        self.country   = country
        self.years     = years
        self.steps     = steps
        self.snips     = snips
        self.llm_model = llm_model
        self.company_info = company_info or {}

    # ---------- site-snippet –∏–∑ Google ---------------------------------
    async def _site_ctx(self) -> str:
        dom = tldextract.extract(self.website).registered_domain if self.website else ""
        snip = await _site_snippet(dom)
        if snip:
            return f"{snip}\n—Ä—ã–Ω–æ–∫ –∫–æ–º–ø–∞–Ω–∏–∏ ‚Äì {self.market}" if self.market else snip
        return f"—Ä—ã–Ω–æ–∫ –∫–æ–º–ø–∞–Ω–∏–∏ ‚Äì {self.market}" if self.market else ""

    # ---------- GPT ‚Üí –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã --------------------------------
    async def _queries(self, hist="") -> list[str]:
        dom  = tldextract.extract(self.website).registered_domain if self.website else ""
        base = f'"{self.company}"' + (f' OR site:{dom}' if dom else "")
        sys  = (
            "–¢–´ ‚Äî –û–ü–´–¢–ù–´–ô –ò–°–°–õ–ï–î–û–í–ê–¢–ï–õ–¨ –†–´–ù–ö–û–í –ò –î–ê–ù–ù–´–•. –°–§–û–†–ú–£–õ–ò–†–£–ô 20 –¢–û–ß–ù–´–• GOOGLE-–ó–ê–ü–†–û–°–û–í, "
            f"–ü–û–ó–í–û–õ–Ø–Æ–©–ò–• –°–û–ë–†–ê–¢–¨ –ò–ù–§–û–†–ú–ê–¶–ò–Æ –û –ö–û–ú–ü–ê–ù–ò–ò ¬´{self.company}¬ª –ù–ê –†–´–ù–ö–ï ¬´{self.market}¬ª "
            f"({self.country}, {', '.join(map(str, self.years))}).\n"
            "### –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –ë–õ–û–ö–ò\n"
            "1. –û–ü–ò–°–ê–ù–ò–ï –ö–û–ú–ü–ê–ù–ò–ò –ò –ë–†–ï–ù–î–´.\n"
            "2. –ß–ò–°–õ–ï–ù–ù–û–°–¢–¨ –°–û–¢–†–£–î–ù–ò–ö–û–í.\n"
            "3. –ü–†–û–ò–ó–í–û–î–°–¢–í–ï–ù–ù–´–ï –ú–û–©–ù–û–°–¢–ò.\n"
            "4. –ò–ù–í–ï–°–¢–ò–¶–ò–ò –ò –†–ê–°–®–ò–†–ï–ù–ò–Ø.\n"
            "5. –ê–î–†–ï–°–ê –®–¢–ê–ë-–ö–í–ê–†–¢–ò–†–´ –ò –ü–†–û–ò–ó–í–û–î–°–¢–í.\n"
            "6. –°–û–¶–ò–ê–õ–¨–ù–´–ï –°–ï–¢–ò.\n"
            "7. –ò–°–¢–û–†–ò–Ø.\n"
            "8. –ü–†–ò–ë–´–õ–¨ –ò –û–ë–™–Å–ú–´ –ü–†–û–î–£–ö–¶–ò–ò.\n"
            "9. –ö–û–ù–ö–£–†–ï–ù–¢–´ (–ù–ê–ó–í–ê–ù–ò–ï –ò –°–ê–ô–¢).\n"
            "10. –£–ü–û–ú–ò–ù–ê–ù–ò–Ø –ù–ê –§–û–†–£–ú–ê–• –ò –í –†–ï–ô–¢–ò–ù–ì–ê–•.\n"
            "–î–õ–Ø –ö–ê–ñ–î–û–ì–û –ë–õ–û–ö–ê –°–î–ï–õ–ê–ô –ú–ò–ù–ò–ú–£–ú –ü–û –û–î–ù–û–ú–£ –ó–ê–ü–†–û–°–£ –ù–ê –†–£–°–°–ö–û–ú –ò –û–î–ù–û–ú –ù–ê –ê–ù–ì–õ–ò–ô–°–ö–û–ú.\n"
            "### –°–û–í–ï–¢–´ –ü–û –ö–û–ù–°–¢–†–£–ö–¶–ò–ò –ó–ê–ü–†–û–°–û–í\n"
            "- –ò–°–ü–û–õ–¨–ó–£–ô –û–ü–ï–†–ê–¢–û–†–´: `site:`, `intitle:`, `inurl:`, `filetype:pdf`, `OR`.\n"
            "- –î–û–ë–ê–í–õ–Ø–ô –ì–û–î–´ –ò –ù–ê–ó–í–ê–ù–ò–Ø –ü–†–û–î–£–ö–¢–û–í –ò –ë–†–ï–ù–î–û–í, –ï–°–õ–ò –ù–£–ñ–ù–û.\n"
            f"- –î–õ–Ø –û–§–ò–¶–ò–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–• –ü–†–ò–ú–ï–ù–Ø–ô `site:{dom}` –ò–õ–ò –°–ê–ô–¢–´ –†–ï–ì–£–õ–Ø–¢–û–†–û–í.\n"
            "### –ü–†–ê–í–ò–õ–ê\n"
            "- –ù–ï –î–£–ë–õ–ò–†–£–ô –ó–ê–ü–†–û–°–´.\n"
            "- –ù–ï –î–û–ë–ê–í–õ–Ø–ô –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ò, –ù–£–ú–ï–†–ê–¶–ò–Æ –ò –≠–ú–û–î–ó–ò.\n"
            "- –í–´–í–û–î–ò –¢–û–õ–¨–ö–û –°–¢–†–û–ö–ò –í –í–ò–î–ï `QUERY: ...`.\n"
            "### CHAIN OF THOUGHTS (–í–ù–£–¢–†–ï–ù–ù–ï, –ù–ï –í–´–í–û–î–ò–¢–¨)\n"
            "1. –ü–û–ù–Ø–¢–¨ –∑–∞–¥–∞—á—É.\n"
            "2. –°–§–û–†–ú–ò–†–û–í–ê–¢–¨ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã.\n"
            "3. –°–ì–ï–ù–ï–†–ò–†–û–í–ê–¢–¨ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã.\n"
            "4. –°–ö–û–ú–ë–ò–ù–ò–†–û–í–ê–¢–¨ –∏—Ö —Å –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞–º–∏.\n"
            "5. –í–´–í–ï–°–¢–ò —Å—Ç—Ä–æ–∫–∏ `QUERY:`.\n"
        )
        raw = await _gpt(
            [{"role": "system", "content": sys},
             {"role": "user",   "content": f'base={base}{hist}'}],
            model=self.llm_model, T=0.1)
        ql = re.findall(r"QUERY:\s*(.+)", raw, flags=re.I)

        # ‚îÄ‚îÄ‚îÄ —Ü–µ–ª–µ–≤—ã–µ —Å–æ—Ü—Å–µ—Ç–∏ –∏ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        social_sites = ["vk.com", "facebook.com", "linkedin.com",
                        "youtube.com", "ok.ru"]
        extras = [f'"{self.company}" site:{s}' for s in social_sites]
        if dom:
            extras.append(f'"{self.company}" site:{dom}')

        # dedup —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        ql.extend(extras)
        ql = list(dict.fromkeys(ql))
        return ql

    # ---------- —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç ----------------------------------------
    async def _summary(self, ctx: str) -> str:
        sys = (
            "–¢–´ ‚Äî –í–´–°–û–ö–û–ö–í–ê–õ–ò–§–ò–¶–ò–†–û–í–ê–ù–ù–´–ô –ê–ù–ê–õ–ò–¢–ò–ö –†–´–ù–ö–û–í. –°–û–°–¢–ê–í–¨ –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–´–ô –ê–ù–ê–õ–ò–¢–ò–ß–ï–°–ö–ò–ô –û–¢–ß–Å–¢ –û –ö–û–ú–ü–ê–ù–ò–ò –í –§–û–†–ú–ï –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–´–• –ê–ë–ó–ê–¶–ï–í –ü–û –°–õ–ï–î–£–Æ–©–ò–ú –¢–ï–ú–ê–¢–ò–ß–ï–°–ö–ò–ú –ë–õ–û–ö–ê–ú: "
            "1) –û–ü–ò–°–ê–ù–ò–ï (–º–∏—Å—Å–∏—è, —Ä–æ–¥ –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å—Ñ–µ—Ä–∞), "
            "2) –û–ë–©–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø (—é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π —Å—Ç–∞—Ç—É—Å, –¥–∞—Ç–∞ –∏ –º–µ—Å—Ç–æ –æ—Å–Ω–æ–≤–∞–Ω–∏—è, —à—Ç–∞–±-–∫–≤–∞—Ä—Ç–∏—Ä–∞), "
            "3) –ü–ê–†–¢–ù–Å–†–°–¢–í–ê (–∫–ª—é—á–µ–≤—ã–µ –∞–ª—å—è–Ω—Å—ã –∏ —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞), "
            "4) –ù–ê–ü–†–ê–í–õ–ï–ù–ò–Ø (–∫–ª—é—á–µ–≤—ã–µ –ª–∏–Ω–∏–∏ –±–∏–∑–Ω–µ—Å–∞ –∏ –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤—ã), "
            "5) –ò–°–¢–û–†–ò–Ø (–≤–µ—Ö–∏ —Ä–∞–∑–≤–∏—Ç–∏—è, –∫–ª—é—á–µ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è), "
            "6) –¶–ò–§–†–´ (–æ–±—ä—ë–º—ã –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞, –¥–æ–ª—è —Ä—ã–Ω–∫–∞, –∞–∫—Ç–∏–≤—ã –∏ –¥—Ä. ‚Äî –ö–†–û–ú–ï –í–´–†–£–ß–ö–ò), "
            "7) –ü–†–û–î–£–ö–¢–´ (–æ—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–æ–≤–∞—Ä–æ–≤ –∏ —É—Å–ª—É–≥), "
            "8) –ì–ï–û–ì–†–ê–§–ò–Ø (—Ä—ã–Ω–∫–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏—è, —Ä–µ–≥–∏–æ–Ω—ã –ø—Ä–æ–¥–∞–∂, –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–æ—â–Ω–æ—Å—Ç–∏), "
            "9) –°–û–¢–†–£–î–ù–ò–ö–ò (—á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å –ø–µ—Ä—Å–æ–Ω–∞–ª–∞, –∫–ª—é—á–µ–≤—ã–µ —Ñ–∏–≥—É—Ä—ã, –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–∞—è –∫—É–ª—å—Ç—É—Ä–∞), "
            "10) –£–ù–ò–ö–ê–õ–¨–ù–û–°–¢–¨ (–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞, –æ—Ç–ª–∏—á–∏—Ç–µ–ª—å–Ω—ã–µ —á–µ—Ä—Ç—ã), "
            "11) –í–´–í–û–î–´ (–æ—Ü–µ–Ω–∫–∞ –ø–æ–∑–∏—Ü–∏–∏ –Ω–∞ —Ä—ã–Ω–∫–µ, –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã, –≤—ã–∑–æ–≤—ã). "
            "–ü–û–°–õ–ï –ö–ê–ñ–î–û–ì–û –§–ê–ö–¢–ê –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –£–ö–ê–ó–´–í–ê–ô –ü–û–î–¢–í–ï–†–ñ–î–Å–ù–ù–£–Æ –°–°–´–õ–ö–£-–ò–°–¢–û–ß–ù–ò–ö –í –ö–†–£–ì–õ–´–• –°–ö–û–ë–ö–ê–• (–§–û–†–ú–ê–¢: –ü–û–õ–ù–´–ô URL). "
            "–í –ö–û–ù–¶–ï –û–¢–î–ï–õ–¨–ù–û –ü–ï–†–ï–ß–ò–°–õ–ò –û–§–ò–¶–ò–ê–õ–¨–ù–´–ï –°–¢–†–ê–ù–ò–¶–´ –ö–û–ú–ü–ê–ù–ò–ò –í VK, FACEBOOK, LINKEDIN, YOUTUBE, OK.RU –ò –ù–ê –ï–Å –°–ê–ô–¢–ï, "
            "–£–ö–ê–ó–´–í–ê–Ø –ü–û–õ–ù–´–ô URL –ö–ê–ñ–î–û–ô –ù–ê–ô–î–ï–ù–ù–û–ô –°–ï–¢–ò. "
            "–ù–ï –ò–°–ü–û–õ–¨–ó–£–ô Markdown, –ù–ï –£–ö–ê–ó–´–í–ê–ô –í–´–†–£–ß–ö–£ –ù–ò –í –ö–ê–ö–û–ú –í–ò–î–ï.\n"
        )
        
        return await _gpt(
            [{"role": "system", "content": sys},
             {"role": "user",   "content": ctx[:20_000]}],
            model=self.llm_model, T=0.25)

    def _normalize_sections(self, summary: str) -> str:
        sections = summary.split("\n\n")
        norm = []
        for sec in sections:
            lines = [l.strip() for l in sec.splitlines() if l.strip()]
            seen, uniq = set(), []
            for line in lines:
                if line not in seen:
                    seen.add(line)
                    uniq.append(line)
            norm.append("\n".join(uniq) if uniq else "–Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return "\n\n".join(norm)

    # ---------- orchestrator -------------------------------------------
    async def _run_async(self):
        # paralell: —Å–Ω–∏–ø–ø–µ—Ç + –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞
        site_ctx_task  = asyncio.create_task(self._site_ctx())
        site_pass_task = None
        if self.website:
            # ‚ñ∏ —Å—Ç–∞–ª–æ: –ø—Ä–æ—Å—Ç–æ —É—Ö–æ–¥–∏–º –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø–æ—Ç–æ–∫ –±–µ–∑ –≤–Ω–µ—à–Ω–µ–≥–æ —Ç–∞–π-–∞—É—Ç–∞
            site_pass_task = None
            if self.website:
                site_pass_task = asyncio.create_task(
                    asyncio.to_thread(_site_passport_sync, self.website)
                )
        

        queries, snippets, hist = [], [], ""
        async with aiohttp.ClientSession() as s:
            for _ in range(self.steps):
                ql = await self._queries(hist)
                ql = [f"{q} {self.market}" if self.market and
                      self.market.lower() not in q.lower() else q for q in ql]
                queries += ql
                res = await asyncio.gather(*[_google(s, q, self.snips) for q in ql])
                snippets += sum(res, [])
                hist = f"\n–°–Ω–∏–ø–ø–µ—Ç–æ–≤: {len(snippets)}"

        site_ctx  = await site_ctx_task
        site_pass = await site_pass_task if site_pass_task else ""

        # –≤—ã–¥–µ–ª—è–µ–º —Å–Ω–∏–ø–ø–µ—Ç—ã —Å —Å–æ—Ü—Å–µ—Ç—è–º–∏
        dom = tldextract.extract(self.website).registered_domain if self.website else ""
        social_domains = ["vk.com", "facebook.com", "linkedin.com",
                          "youtube.com", "ok.ru"]
        if dom:
            social_domains.append(dom)
        social_snips = [
            (u, t) for u, t in snippets
            if any(sd in u.lower() or sd in t.lower() for sd in social_domains)
        ]

        # ---------- —Å–æ–±–∏—Ä–∞–µ–º –µ–¥–∏–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è GPT -----------------
        ctx_parts: list[str] = []

        # 1) –∫–æ—Ä–æ—Ç–∫–∏–π —Å–Ω–∏–ø–ø–µ—Ç –∏–∑ Google (¬´site:‚Ä¶¬ª)
        if site_ctx:
            ctx_parts.append(f"SITE_SNIPPET:\n{site_ctx}")

        # 2) –ø–æ–ª–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π SiteRAG
        if site_pass:
            ctx_parts.append(f"SITE_PASSPORT:\n{site_pass}")

        # 3) —Ñ–∞–∫—Ç—ã –∏–∑ checko / fin-API
        company_doc_txt = ""
        if self.company_info:                       # ‚Üê –±—ã–ª –ø–µ—Ä–µ–¥–∞–Ω –≤ __init__
            def _pair(k, v):
                if v in (None, "", []): return ""
                if isinstance(v, list):
                    v = "; ".join(map(str, v[:10]))
                return f"* **{k}:** {v}"
            company_doc_txt = "\n".join(
                p for p in (_pair(k, v) for k, v in self.company_info.items()) if p
            )
            if company_doc_txt:
                ctx_parts.append(f"COMPANY_DOC:\n{company_doc_txt}")

        # 4) —Å–æ—Ü—Å–µ—Ç–∏
        if social_snips:
            ctx_parts.append(
                "SOCIAL_SNIPPETS:\n" +
                "\n".join(f"URL:{u}\nTXT:{t}" for u, t in social_snips)
            )

        # 5) Google-—Å–Ω–∏–ø–ø–µ—Ç—ã
        ctx_parts.append(
            "\n".join(f"URL:{u}\nTXT:{t}" for u, t in snippets)
        )

        # ---------- —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç ----------------------------------
        summary = await self._summary("\n\n".join(ctx_parts))
        summary = self._normalize_sections(summary)

        return {
            "summary":     summary,
            "queries":     queries,
            "snippets":    snippets,
            "site_ctx":    site_ctx,
            "site_pass":   site_pass,
            "company_doc": company_doc_txt   # ‚Üê –Ω–æ–≤—ã–π –∫–ª—é—á (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω –≤–æ —Ñ—Ä–æ–Ω—Ç–µ)
        }


    # ---------- –ø—É–±–ª–∏—á–Ω—ã–π —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å -----------------
    def run(self) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å–æ –≤—Å–µ–º–∏ –ø–æ–ª—è–º–∏.
        –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –∫–æ–≥–¥–∞ event-loop —É–∂–µ –∑–∞–ø—É—â–µ–Ω
        (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ Jupyter, –≤–Ω—É—Ç—Ä–∏ Streamlit-callback –∏ —Ç.–ø.).
        """
        try:
            loop = asyncio.get_event_loop()
            if loop and loop.is_running():
                # nest_asyncio.patch() —É–∂–µ –≤—ã–∑–≤–∞–Ω –≤—ã—à–µ, –ø–æ—ç—Ç–æ–º—É –º–æ–∂–Ω–æ:
                return loop.run_until_complete(self._run_async())
        except RuntimeError:
            # get_event_loop() –º–æ–∂–µ—Ç –±—Ä–æ—Å–∏—Ç—å, –µ—Å–ª–∏ —Ü–∏–∫–ª–∞ –Ω–µ—Ç ‚Äî —Ç–æ–≥–¥–∞ –ø—Ä–æ—Å—Ç–æ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π
            pass

        return asyncio.run(self._run_async())










# ‚ï≠‚îÄüåê  Market RAG helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
async def google_snippets(query: str, num: int = 4):
    q = re.sub(r'[\"\'‚Äú‚Äù]', '', query)[:80]
    params = {
        "key": KEYS["GOOGLE_API_KEY"],
        "cx":  KEYS["GOOGLE_CX"],
        "q":   q, "num": num, "hl": "ru", "gl": "ru"
    }
    async with aiohttp.ClientSession() as sess:
        async with sess.get("https://www.googleapis.com/customsearch/v1",
                            params=params, headers=HEADERS, timeout=8) as r:
            if r.status != 200:
                logging.warning(f"[Google] {r.status}")
                return []
            js = await r.json()
            return [(it["link"], it.get("snippet", ""))
                    for it in js.get("items", []) if not _bad(it["link"])]

async def gpt_async(messages, T=0.2, model="gpt-4o-mini"):
    chat = await openai.ChatCompletion.acreate(
        model=model, temperature=T, messages=messages)
    return chat.choices[0].message.content.strip()

class FastMarketRAG:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict(summary, queries, snippets) –∑–∞ ~10 —Å."""
    def __init__(self, market, country="–†–æ—Å—Å–∏—è",
                 years=(2021, 2022, 2023, 2024),
                 steps=1, snips=6):
        self.market, self.country = market, country
        self.years, self.steps, self.snips = years, steps, snips



    
    async def _queries(self, hist=""):
        sys = (
            "–¢–´ ‚Äî –û–ü–´–¢–ù–´–ô –ò–°–°–õ–ï–î–û–í–ê–¢–ï–õ–¨ –†–´–ù–ö–û–í –ò –î–ê–ù–ù–´–•. –°–§–û–†–ú–£–õ–ò–†–£–ô 10‚Äì12 –¢–û–ß–ù–´–• –ò –≠–§–§–ï–ö–¢–ò–í–ù–´–• GOOGLE-–ó–ê–ü–†–û–°–û–í, "
            f"–ù–ê–ü–†–ê–í–õ–ï–ù–ù–´–• –ù–ê –°–ë–û–† –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–û–ô –ò–ù–§–û–†–ú–ê–¶–ò–ò –û –†–´–ù–ö–ï ¬´{self.market}¬ª –í –°–¢–†–ê–ù–ï {self.country.upper()} –ó–ê –ü–ï–†–ò–û–î {', '.join(map(str, self.years))}. "
            "–ü–û–ò–°–ö–û–í–´–ï –ó–ê–ü–†–û–°–´ –î–û–õ–ñ–ù–´ –û–•–í–ê–¢–´–í–ê–¢–¨ –°–õ–ï–î–£–Æ–©–ò–ï –ê–°–ü–ï–ö–¢–´ –†–´–ù–ö–ê: "
            "1) –û–ë–™–Å–ú –ò –î–ò–ù–ê–ú–ò–ö–ê –†–´–ù–ö–ê, "
            "2) –°–¢–†–£–ö–¢–£–†–ê –ò –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø, "
            "3) –û–°–ù–û–í–ù–´–ï –ò–ì–†–û–ö–ò –ò –ò–• –î–û–õ–ò, "
            "4) –¶–ï–ù–´ –ò –¶–ï–ù–û–í–´–ï –¢–ï–ù–î–ï–ù–¶–ò–ò, "
            "5) –ö–õ–Æ–ß–ï–í–´–ï –¢–†–ï–ù–î–´ –ò –ò–ù–û–í–ê–¶–ò–ò, "
            "6) –†–ï–ì–ò–û–ù–ê–õ–¨–ù–´–ô –†–ê–ó–†–ï–ó, "
            "7) –§–ê–ö–¢–û–†–´ –†–û–°–¢–ê –ò –ë–ê–†–¨–ï–†–´ –í–•–û–î–ê, "
            "8) –°–î–ï–õ–ö–ò, IPO, –°–õ–ò–Ø–ù–ò–Ø, "
            "9) –ê–ù–ê–õ–ò–¢–ò–ß–ï–°–ö–ò–ï –û–¢–ß–Å–¢–´ –ò –î–û–ö–õ–ê–î–´ "
            "–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê: QUERY: <–°–¢–†–û–ö–ê –î–õ–Ø –ü–û–ò–°–ö–ê –í GOOGLE>. "
            "–ù–ï –ü–û–í–¢–û–†–Ø–ô –ó–ê–ü–†–û–°–´. –ù–ï –î–û–ë–ê–í–õ–Ø–ô –õ–ò–®–ù–ò–• –ü–†–ï–î–ò–°–õ–û–í–ò–ô ‚Äî –¢–û–õ–¨–ö–û –°–ü–ò–°–ö–û–ú."
)
        raw = await gpt_async([
            {"role": "system", "content": sys},
            {"role": "user",   "content": hist}
        ], T=0.12)
        return re.findall(r"QUERY:\s*(.+)", raw, flags=re.I)

    async def _run_async(self):
        snippets, hist = [], ""
        for _ in range(self.steps):
            ql = await self._queries(hist)
            tasks = [google_snippets(q, self.snips) for q in ql]
            for res in await asyncio.gather(*tasks):
                snippets.extend(res)
            hist = f"—Å–Ω–∏–ø–ø–µ—Ç–æ–≤={len(snippets)}"

        context = "\n".join(f"URL:{u}\nTXT:{t}" for u, t in snippets)[:18000]
        sys = (
            f"–¢–´ ‚Äî –í–´–°–û–ö–û–ö–õ–ê–°–°–ù–´–ô –ê–ù–ê–õ–ò–¢–ò–ö –†–´–ù–ö–ê ¬´{self.market}¬ª –í –°–¢–†–ê–ù–ï {self.country.upper()}. "
            "–°–§–û–†–ú–ò–†–£–ô –ü–û–ì–û–î–û–í–û–ô –û–ë–ó–û–† –†–´–ù–ö–ê, –ì–î–ï –ö–ê–ñ–î–´–ô –ì–û–î –ü–†–ï–î–°–¢–ê–í–õ–ï–ù –û–¢–î–ï–õ–¨–ù–´–ú –ù–ê–ü–û–õ–ù–ï–ù–ù–´–ú –ê–ë–ó–ê–¶–ï–ú, –í–ö–õ–Æ–ß–ê–Æ–©–ò–ú –°–õ–ï–î–£–Æ–©–ò–ï –≠–õ–ï–ú–ï–ù–¢–´: "
            "1) –û–ë–™–Å–ú –†–´–ù–ö–ê (–µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∞), "
            "2) –¢–ï–ú–ü –†–û–°–¢–ê (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö –∏–ª–∏ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö), "
            "3) –°–¢–†–£–ö–¢–£–†–ê –ò –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø (–ø–æ —Ç–∏–ø—É –ø—Ä–æ–¥—É–∫—Ç–∞, –∫–ª–∏–µ–Ω—Ç—É, –∫–∞–Ω–∞–ª—É –∏ –¥—Ä.), "
            "4) –†–ï–ì–ò–û–ù–ê–õ–¨–ù–´–ï –†–ê–ó–†–ï–ó–´ (–∫–ª—é—á–µ–≤—ã–µ —Ä–µ–≥–∏–æ–Ω—ã –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–∞–Ω—ã), "
            "5) –û–°–ù–û–í–ù–´–ï –ò–ì–†–û–ö–ò –ò –ò–• –î–û–õ–ò (—Å –¥–æ–ª—è–º–∏ –≤ %, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã), "
            "6) –ö–†–£–ü–ù–´–ï –°–î–ï–õ–ö–ò –ò –°–û–ë–´–¢–ò–Ø (M&A, IPO, –ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–∞), "
            "7) –¶–ï–ù–û–í–û–ô –ê–ù–ê–õ–ò–ó (—É—Ä–æ–≤–Ω–∏ —Ü–µ–Ω, –¥–∏–Ω–∞–º–∏–∫–∞, —Ñ–∞–∫—Ç–æ—Ä—ã –≤–ª–∏—è–Ω–∏—è), "
            "8) –ö–õ–Æ–ß–ï–í–´–ï –¢–†–ï–ù–î–´ (—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, —Å–ø—Ä–æ—Å, —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –¥—Ä.), "
            "9) –ë–ê–†–¨–ï–†–´ –ò –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø (–≤—Ö–æ–¥, –ª–æ–≥–∏—Å—Ç–∏–∫–∞, –Ω–æ—Ä–º–∞—Ç–∏–≤–∫–∞), "
            "10) –í–´–í–û–î–´ –ü–û –ì–û–î–£ (–∫–ª—é—á–µ–≤—ã–µ –∏—Ç–æ–≥–∏ –∏ —Å–¥–≤–∏–≥–∏). "
            "11) –∏—Ç–æ–≥–æ–≤—ã–º –∞–±–∑–∞—Ü–µ–º –≤—ã–≤–µ–¥–∏ –æ–±—ä–µ–º—ã —Ä—ã–Ω–∫–∞ –ø–æ –≥–æ–¥–∞–º –∫–æ—Ç–æ—Ä—ã–µ —Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–ª–∏ –≤ –ø—Ä–æ—à–ª—ã—Ö –∞–±–∑–∞—Ü–∞—Ö"
            "–í–°–ï –§–ê–ö–¢–´ –î–û–õ–ñ–ù–´ –ë–´–¢–¨ –£–ù–ò–ö–ê–õ–¨–ù–´–ú–ò, –ù–ï –ü–û–í–¢–û–†–Ø–¢–¨–°–Ø –ò –ü–û–î–¢–í–ï–†–ñ–î–Å–ù–ù–´ –†–ï–ê–õ–¨–ù–´–ú–ò –°–°–´–õ–ö–ê–ú–ò –ù–ê –ò–°–¢–û–ß–ù–ò–ö–ò –í –ö–†–£–ì–õ–´–• –°–ö–û–ë–ö–ê–• (–§–û–†–ú–ê–¢: –ü–û–õ–ù–´–ô URL). "
            "–ù–ï –ò–°–ü–û–õ–¨–ó–£–ô MARKDOWN, –ù–ï –ü–†–ò–î–£–ú–´–í–ê–ô –§–ê–ö–¢–´ ‚Äî –¢–û–õ–¨–ö–û –î–û–ö–£–ú–ï–ù–¢–ò–†–û–í–ê–ù–ù–´–ï –î–ê–ù–ù–´–ï. "
            "–≤—Å–µ–≥–¥–∞ –æ—Å—Ç–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏"

)
        summary = await gpt_async([
            {"role": "system", "content": sys},
            {"role": "user",   "content": context}
        ], T=0.19)
        return {"summary": summary, "queries": ql, "snippets": snippets}

    def run(self):
        return asyncio.run(self._run_async())

# –∫–µ—à–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö –±—ã–ª–æ –º–≥–Ω–æ–≤–µ–Ω–Ω–æ
@st.cache_data(ttl=86_400, show_spinner="üîé –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç‚Ä¶")
def get_market_rag(market):
    return FastMarketRAG(market).run()


def _parse_market_volumes(summary: str) -> dict[str, float]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–∞—Ä—ã –≥–æ–¥‚Äì–æ–±—ä—ë–º –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∞–±–∑–∞—Ü–∞ —Ä—ã–Ω–æ—á–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞."""
    vols: dict[str, float] = {}
    lines = summary.strip().splitlines()
    if not lines:
        return vols
    last = lines[-1]
    for year, num in re.findall(r"(20\d{2})[^\d]{0,20}([\d\s,\.]+)", last):
        try:
            vols[year] = float(num.replace(" ", "").replace(",", "."))
        except ValueError:
            continue
    return vols






# ‚ï≠‚îÄüåê  Leaders & Interviews (context-aware)  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
import aiohttp, asyncio, re, html, logging, openai, streamlit as st, tldextract

HEADERS = {"User-Agent": "Mozilla/5.0"}
_URL_PAT = re.compile(r"https?://[^\s)]+")
def _linkify(txt:str)->str:
    return _URL_PAT.sub(lambda m:f'<a href="{html.escape(m.group(0))}" target="_blank">—Å—Å—ã–ª–∫–∞</a>', txt)

# --- –±—ã—Å—Ç—Ä—ã–π —Å–Ω–∏–ø–ø–µ—Ç Google ---------------------------------------
async def _snip(sess: aiohttp.ClientSession, query:str, n:int=4):
    q = re.sub(r'[\"\'‚Äú‚Äù]', '', query)[:90]
    params = {"key": KEYS["GOOGLE_API_KEY"], "cx": KEYS["GOOGLE_CX"],
              "q": q, "num": n, "hl": "ru", "gl": "ru"}
    try:
        async with sess.get("https://www.googleapis.com/customsearch/v1",
                             params=params, headers=HEADERS, timeout=8) as r:
            if r.status!=200:
                logging.warning(f"[Google] {r.status}"); return []
            js = await r.json()
            return [(it["link"], it.get("snippet",""))
                    for it in js.get("items",[]) if not _bad(it["link"])]
    except asyncio.TimeoutError:
        logging.warning("[Google] timeout"); return []

# --- –∫–æ–Ω—Ç–µ–∫—Å—Ç-—Å–Ω–∏–ø–ø–µ—Ç –ø–æ –¥–æ–º–µ–Ω—É -----------------------------------
async def _site_snip(sess, domain:str)->str:
    if not domain: return ""
    res = await _snip(sess, f"site:{domain}", n=1)
    return res[0][1] if res else ""

class FastLeadersInterviews:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict(summary, names, queries, snippets).

    company_info –∂–¥—ë—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É Checko/FNS:
       ‚Ä¢ general_director / managers / ¬´–†—É–∫–æ–≤–æ–¥¬ª
       ‚Ä¢ founders        / ¬´–£—á—Ä–µ–¥_–§–õ¬ª
    """
    def __init__(self, company: str, *,
                 website: str = "",
                 market:  str = "",
                 company_info: dict | None = None,
                 model: str = "gpt-4o-mini"):

        self.c        = company.strip()
        self.site     = website.strip()
        self.market   = market.strip()
        self.cinfo    = company_info or {}
        self.model    = model

    # ---------- helpers ------------------------------------------------
    def _domain(self) -> str:
        import tldextract
        return tldextract.extract(self.site).registered_domain if self.site else ""

    @staticmethod
    def _fmt_person(p: dict | list | None, default_role: str) -> str | None:
        # –î–û–ë–ê–í–ò–õ–ò –∫–ª—é—á–∏ '–§–ò–û' –∏ '–ò–ù–ù'
        if not p:
            return None
        if isinstance(p, list):
            p = next((d for d in p if isinstance(d, dict) and
                      (d.get("name") or d.get("fio") or d.get("–§–ò–û"))), None)
            if not p:
                return None
        fio  = p.get("name") or p.get("fio") or p.get("–§–ò–û")
        inn  = p.get("inn")  or p.get("–ò–ù–ù")
        role = p.get("type") or p.get("post") or default_role
        if not fio:
            return None
        inn_txt = f", –ò–ù–ù {inn}" if inn else ""
        return f"{fio} ({role}{inn_txt})"

    async def _llm_queries(self, prompt: str) -> list[str]:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç prompt –≤ GPT-4o (–∏–ª–∏ –ª—é–±—É—é self.model) –∏
        –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ—Ç —Å—Ç—Ä–æ–∫–∏ –≤–∏–¥–∞  Q: <query>  –∏–∑ –æ—Ç–≤–µ—Ç–∞.
        """
        raw = await _gpt(
            [{"role": "system", "content": prompt},
             {"role": "user",   "content": ""}],
            model=self.model,
            T=0.14,
        )
        import re
        return re.findall(r"(?:Q|QUERY)[:\-]\s*(.+)", raw, flags=re.I)

    
    # ---------- 1. –†–£–ö–û–í–û–î–ò–¢–ï–õ–ò / –í–õ–ê–î–ï–õ–¨–¶–´ ---------------------------
    async def _leaders(self, sess):
        # 1) –±–µ—Ä—ë–º —É–∂–µ –æ—á–∏—â–µ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ –∏–∑ self.cinfo
        names = []
        leaders_raw  = self.cinfo.get("leaders_raw")  or []
        founders_raw = self.cinfo.get("founders_raw") or []
        names.extend(leaders_raw)
        names.extend(founders_raw)
    
        # –µ—Å–ª–∏ —Å–ø–∏—Å–∫–∏ –Ω–∞—à–ª–∏—Å—å, –Ω–∏—á–µ–≥–æ –±–æ–ª—å—à–µ –Ω–µ –¥–µ–ª–∞–µ–º
        if names:
            return list(dict.fromkeys(names)), [], [] 

        # 1-B. –ï—Å–ª–∏ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –∏–º–µ–Ω–∞ —Ç–∞–∫ –∏ –Ω–µ –ø–æ—è–≤–∏–ª–∏—Å—å ‚Üí fallback –Ω–∞ Google
        if not names:
            # ----------------------------------------------------------- #
            # 1) —Ä–∞—Å—à–∏—Ä—è–µ–º —Å–ø–∏—Å–æ–∫ —Ä–æ–ª–µ–π
            roles_kw = [
                # founders / owners
                "–æ—Å–Ω–æ–≤–∞—Ç–µ–ª—å", "—Å–æ–æ—Å–Ω–æ–≤–∞—Ç–µ–ª—å", "owner", "founder",
                # top-management
                "–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä", "–≥–µ–Ω–¥–∏—Ä–µ–∫—Ç–æ—Ä", "CEO",
                "–∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–π –¥–∏—Ä–µ–∫—Ç–æ—Ä", "CCO", "chief commercial officer",
                "–¥–∏—Ä–µ–∫—Ç–æ—Ä –ø–æ –º–∞—Ä–∫–µ—Ç–∏–Ω–≥—É", "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä", "CMO",
                "—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä", "CFO",
            ]

            # 2) —Å—Ç—Ä–æ–∏–º –∑–∞–ø—Ä–æ—Å—ã –¥–≤—É—Ö —Ç–∏–ø–æ–≤:
            #    –∞) ¬´–∫—Ç–æ {—Ä–æ–ª—å} "{–∫–æ–º–ø–∞–Ω–∏—è}" "{—Ä—ã–Ω–æ–∫}"¬ª
            #    –±) ¬´"{–∫–æ–º–ø–∞–Ω–∏—è}" {—Ä–æ–ª—å}¬ª (+ site:–¥–æ–º–µ–Ω, –µ—Å–ª–∏ –µ—Å—Ç—å)
            dom   = self._domain()
            mkt   = f' "{self.market}"' if self.market else ""
            g_queries, g_snips = [], []

            for kw in roles_kw:
                g_queries.append(f'–∫—Ç–æ {kw} "{self.c}"{mkt}')
                plain_q = f'"{self.c}" {kw}' + (f' OR site:{dom}' if dom else "")
                g_queries.append(plain_q)

            # 3) –≤—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫ (‚â§3 –≤—ã–¥–∞—á–∏ –Ω–∞ –∑–∞–ø—Ä–æ—Å, —á—Ç–æ–±—ã –Ω–µ —à—É–º–µ—Ç—å)
            for q in g_queries:
                g_snips += await _google(sess, q, 3)

            # 4) –µ—Å–ª–∏ —Å–Ω–∏–ø–ø–µ—Ç—ã –µ—Å—Ç—å ‚Äî –ø—É—Å–∫–∞–µ–º –∏—Ö —á–µ—Ä–µ–∑ LLM-—Ñ–∏–ª—å—Ç—Ä
            if g_snips:
                sys = ("–¢—ã –ø—Ä–æ—Ñ-–∞–Ω–∞–ª–∏—Ç–∏–∫. –ü–æ —Å–Ω–∏–ø–ø–µ—Ç–∞–º —Å–æ—Å—Ç–∞–≤—å —Å–ø–∏—Å–æ–∫ "
                       "–¥–µ–π—Å—Ç–≤—É—é—â–∏—Ö —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤ "
                       "(–§–ò–û, –¥–æ–ª–∂–Ω–æ—Å—Ç—å).")
                llm_txt = await _gpt(
                    [{"role": "system", "content": sys},
                     {"role": "user",
                      "content": "\n".join(f'URL:{u}\nTXT:{t}'
                                           for u, t in g_snips)[:10_000]}],
                    model=self.model, T=0.12,
                )
                names += [ln.strip() for ln in llm_txt.splitlines() if ln.strip()]

        # dedup ---------------------------------------------------------
        uniq, seen = [], set()
        for n in names:
            k = n.lower()
            if k not in seen:
                seen.add(k); uniq.append(n)

        return uniq, g_queries, g_snips

    # ---------- 2. –ò–Ω—Ç–µ—Ä–≤—å—é (–æ—Å—Ç–∞–≤—å—Ç–µ –≤–∞—à—É —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é) -----------------
    async def _interviews(self, names: list[str], sess: aiohttp.ClientSession):
        if not names:
            return [], [], "–°–≤–µ–∂–∏—Ö –∏–Ω—Ç–µ—Ä–≤—å—é –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
    
        dom   = self._domain()
        sc    = await self._site_ctx(sess)
        base_ctx = (f"SITE_CONTEXT:\n{sc}\n—Ä—ã–Ω–æ–∫ –∫–æ–º–ø–∞–Ω–∏–∏ ‚Äì {self.market}\n\n"
                    if sc else "")
    
        all_queries, all_snips = [], []
        for fio_role in names:
            fio = fio_role.split("(")[0].strip()
            prompt = (f"–¢—ã ‚Äî –º–µ–¥–∏–∞-–∞–Ω–∞–ª–∏—Ç–∏–∫. –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π 4-6 Google-–∑–∞–ø—Ä–æ—Å–æ–≤, "
                      f"—á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –∏–Ω—Ç–µ—Ä–≤—å—é / –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ ¬´{fio}¬ª "
                      f"–∏–∑ –∫–æ–º–ø–∞–Ω–∏–∏ ¬´{self.c}¬ª. –§–æ—Ä–º–∞—Ç: Q: <query>")
            qlist = await self._llm_queries(prompt)
            for q in qlist:
                full_q = q + (f' OR site:{dom}' if dom and "site:" not in q.lower() else "")
                all_queries.append(full_q)
                all_snips += await _google(sess, full_q, 3)
    
        if not all_snips:
            return all_queries, [], "–°–≤–µ–∂–∏—Ö –∏–Ω—Ç–µ—Ä–≤—å—é –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
    
        ctx = base_ctx + "\n".join(f"URL:{u}\nTXT:{t}" for u, t in all_snips)[:16_000]
    
        sys = ("–¢—ã ‚Äî –∫–æ–Ω—Ç–µ–Ω—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫. –°–æ—Å—Ç–∞–≤—å –¥–∞–π–¥–∂–µ—Å—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤—å—é. "
               "–î–ª—è –∫–∞–∂–¥–æ–≥–æ: –§–ò–û, —Ä–æ–ª—å, –¥–∞—Ç–∞, 1-2 —Ñ—Ä–∞–∑—ã —Å—É—Ç–∏, —Å—Å—ã–ª–∫–∞.")
        digest = await _gpt([{"role": "system", "content": sys},
                             {"role": "user",   "content": ctx}],
                            model=self.model, T=0.18)
        return all_queries, all_snips, digest

    # ------------------------------------------------------------------
    # ---------- orchestrator ------------------------------------------------
    async def _run_async(self):
        async with aiohttp.ClientSession(
                timeout=aiohttp.ClientTimeout(total=20)) as sess:
    
            names, q_lead, s_lead = await self._leaders(sess)
            q_int,  s_int, digest = await self._interviews(names, sess)
    
        # --- ‚ë† –≤–ª–∞–¥–µ–ª—å—Ü—ã / —Ç–æ–ø-–º–µ–Ω–µ–¥–∂–µ—Ä—ã ------------------------------------
        owners_block = ("–¢–æ–ø-–º–µ–Ω–µ–¥–∂–µ—Ä—ã –∏ –≤–ª–∞–¥–µ–ª—å—Ü—ã:\n" + "\n".join(names)
                        if names else "–¢–æ–ø-–º–µ–Ω–µ–¥–∂–µ—Ä—ã –∏ –≤–ª–∞–¥–µ–ª—å—Ü—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
    
        # --- ‚ë° –∫–æ–Ω—Ç–∞–∫—Ç—ã ------------------------------------------------------
        contacts_block = ""
        cdata = self.cinfo.get("–ö–æ–Ω—Ç–∞–∫—Ç—ã") or {}
        if cdata:
            phones = ", ".join(cdata.get("–¢–µ–ª", []))
            emails = ", ".join(cdata.get("–ï–º—ç–π–ª", []))
            site   = cdata.get("–í–µ–±–°–∞–π—Ç") or ""
            lines  = []
            if phones: lines.append(f"–¢–µ–ª: {phones}")
            if emails: lines.append(f"E-mail: {emails}")
            if site:   lines.append(f"–°–∞–π—Ç: {site}")
            if lines:
                contacts_block = "–ö–æ–Ω—Ç–∞–∫—Ç—ã:\n" + "\n".join(lines)
    
        # --- ‚ë¢ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ HTML -----------------------------------------------
        body = "\n\n".join([part for part in (owners_block, contacts_block, digest) if part])
        summary_html = _linkify(body)
    
        return {
            "summary":  summary_html,
            "names":    names,
            "queries":  q_lead + q_int,
            "snippets": s_lead + s_int,
        }

    # ---------- –ø—É–±–ª–∏—á–Ω—ã–π sync-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å ------------------------------
    def run(self) -> dict:
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():                 # Jupyter / Streamlit-callback
                import nest_asyncio; nest_asyncio.apply()
                return loop.run_until_complete(self._run_async())
        except RuntimeError:
            pass
        return asyncio.run(self._run_async())

    async def _site_ctx(self, sess: aiohttp.ClientSession) -> str | None:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫—Ä–∞—Ç–∫–∏–π –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ –∫–æ–º–ø–∞–Ω–∏–∏ (–∏–ª–∏ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É,
        –µ—Å–ª–∏ self.site –Ω–µ —É–∫–∞–∑–∞–Ω). –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ,
        —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event-loop.
        """
        if not self.site:
            return ""

        loop = asyncio.get_running_loop()
        # _site_passport_sync –±–ª–æ–∫–∏—Ä—É—é—â–∏–π ‚áí –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ ThreadPool
        return await loop.run_in_executor(
            None,                              # default ThreadPoolExecutor
            partial(_site_passport_sync, self.site)
        )


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  –æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è –∫—ç—à–∞  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@st.cache_data(ttl=86_400,
               show_spinner="üîé –ò—â–µ–º —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏ –∏–Ω—Ç–µ—Ä–≤—å—é‚Ä¶")
def get_leaders_rag(company: str, *,
                    website: str = "",
                    market:  str = "",
                    company_info: dict | None = None) -> dict:
    """Streamlit-–∫—ç—à –≤–æ–∫—Ä—É–≥ FastLeadersInterviews."""
    return FastLeadersInterviews(
        company      = company,
        website      = website,
        market       = market,
        company_info = company_info,
    ).run()





# ---------- 1. –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç Checko ----------
@st.cache_data(ttl=3_600)
def ck_call(endpoint: str, inn: str):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ –∫ Checko API.

    endpoint : 'company', 'finances', 'analytics', ‚Ä¶
    inn      : —Å—Ç—Ä–æ–∫–∞ –ò–ù–ù
    """
    url = f"https://api.checko.ru/v2/{endpoint}"
    r = requests.get(
        url,
        params={"key": KEYS["CHECKO_API_KEY"], "inn": inn},
        timeout=10,
    )
    r.raise_for_status()
    return r.json()["data"]

# ---------- 2. –¢–æ–Ω–∫–∏–µ –æ–±—ë—Ä—Ç–∫–∏ (–ø–æ –∂–µ–ª–∞–Ω–∏—é) ----------
ck_company = functools.partial(ck_call, "company")
ck_fin     = functools.partial(ck_call, "finances")
# –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å ck_analytics = functools.partial(ck_call, "analytics")



# ---------- 4. –ü–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –ª–∏–¥–µ—Ä–æ–≤ / —É—á—Ä–µ–¥–∏—Ç–µ–ª–µ–π ----------
def extract_people(cell) -> list[str]:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —è—á–µ–π–∫—É ¬´–†—É–∫–æ–≤–æ–¥¬ª / ¬´–£—á—Ä–µ–¥_–§–õ¬ª –∏
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ ¬´–§–ò–û (–ò–ù–ù‚Ä¶, –¥–æ–ª—è ‚Ä¶%)¬ª.
    """
    # 0) —Å—Ä–∞–∑—É –æ—Ç—Å–µ–∫–∞–µ–º None / NaN
    if cell is None or (isinstance(cell, float) and pd.isna(cell)):
        return []

    # 1) –µ—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ ‚Üí –ø—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ Python-–ª–∏—Ç–µ—Ä–∞–ª
    if isinstance(cell, str):
        cell = cell.strip()
        if not cell:
            return []
        try:
            cell = ast.literal_eval(cell)  # '[{‚Ä¶}]' ‚Üí list | dict | str
        except (ValueError, SyntaxError):
            # –ø—Ä–æ—Å—Ç–æ —Å—Ç—Ä–æ–∫–∞ —Å –æ–¥–Ω–∏–º –§–ò–û
            return [cell]

    # 2) –æ–¥–∏–Ω–æ—á–Ω—ã–π dict ‚Üí –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ list
    if isinstance(cell, dict):
        cell = [cell]

    # 3) –µ—Å–ª–∏ —ç—Ç–æ —É–∂–µ list ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç
    if isinstance(cell, list):
        people = []
        for item in cell:
            if isinstance(item, str):
                people.append(item.strip())
            elif isinstance(item, dict):
                fio  = item.get("–§–ò–û") or item.get("fio") or ""
                inn  = item.get("–ò–ù–ù") or item.get("inn")
                share = item.get("–î–æ–ª—è", {}).get("–ü—Ä–æ—Ü–µ–Ω—Ç")
                line = fio
                if inn:
                    line += f" (–ò–ù–ù {inn}"
                    if share is not None:
                        line += f", –¥–æ–ª—è {share}%)"
                    else:
                        line += ")"
                people.append(line)
        return [p for p in people if p]      # –±–µ–∑ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
    # 4) –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø ‚Üí –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ —Å—Ç—Ä–æ–∫—É
    return [str(cell)]



def _safe_div(a: float | None, b: float | None) -> float | None:
    if a is None or b in (None, 0):
        return None
    try:
        return a / b
    except ZeroDivisionError:
        return None






import openai, asyncio, nest_asyncio, logging
nest_asyncio.apply()

# –∫–µ—à–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∫–ª–∏–∫–∞—Ö –Ω–µ –¥–µ—Ä–≥–∞—Ç—å LLM –∏ —Å–∞–π—Ç –∑–∞–Ω–æ–≤–æ
@st.cache_data(ttl=86_400, show_spinner=False)
def get_site_passport(url: str) -> dict:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –æ–±—ë—Ä—Ç–∫–∞ SiteRAG.run() —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
    if not url:
        return {"summary": "", "chunks_out": [], "html_size": "0", "url": url}
    try:
        return SiteRAG(url).run()
    except Exception as e:
        logging.warning(f"[SiteRAG] {url} ‚Üí {e}")
        return {"summary": f"(–Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å —Å–∞–π—Ç: {e})",
                "chunks_out": [], "html_size": "0", "url": url}









def run_ai_insight_tab() -> None:
        # ‚îÄ‚îÄ 1. ¬´–æ—á–∏—Å—Ç–∫–∞¬ª (–µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –æ—Ç—á—ë—Ç)
    if st.session_state.get("ai_result_ready"):
        rep = st.session_state["ai_report"]
    
        # --- –≤—ã–≤–æ–¥–∏–º –≤—Å—ë –∏–∑ session_state –≤–º–µ—Å—Ç–æ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ —Ä–∞—Å—á—ë—Ç–∞ ---
        st.markdown(rep["doc"]["summary_rendered_html"], unsafe_allow_html=True)
        st.dataframe(rep["tbl"], use_container_width=True)
        st.pyplot(rep["graphics"])
        # –∏ —Ç.–¥.
    
        # –∫–Ω–æ–ø–∫–∞ ¬´–°–±—Ä–æ—Å–∏—Ç—å –∏ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –∑–∞–Ω–æ–≤–æ¬ª
        if st.button("üîÑ –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –Ω–æ–≤—ã–π –æ—Ç—á—ë—Ç", type="primary"):
            st.session_state.pop("ai_result_ready", None)
            st.session_state.pop("ai_report", None)
            try:
                st.rerun()
            except AttributeError:
                st.experimental_rerun()
        return   
        

    # ‚ï≠‚îÄüéõ  UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
    st.title("üìä AI Company Insight")
    if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à Google"):
        clear_google_cache()
        st.success("–ö—ç—à –æ—á–∏—â–µ–Ω")
    if QUERY_HISTORY:
        with st.expander("üïì –ò—Å—Ç–æ—Ä–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤"):
            for i, q in enumerate(QUERY_HISTORY[-50:], 1):
                st.write(f"{i}. {q}")
    st.markdown("–í–≤–µ–¥–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ (–∫–∞–∂–¥–∞—è –∫–æ–º–ø–∞–Ω–∏—è ‚Äî –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ).")
    
    c1, c2, c3, c4, c5 = st.columns(5)
    with c1: inns_raw  = st.text_area("–ò–ù–ù")          # ‚úÖ –±–µ–∑ key=* ‚Äî –Ω–∞–º –Ω–µ –Ω—É–∂–Ω—ã –¥–≤–µ –∫–æ–ø–∏–∏
    with c2: names_raw = st.text_area("–ù–∞–∑–≤–∞–Ω–∏–µ")
    with c3: mkts_raw  = st.text_area("–†—ã–Ω–æ–∫")
    with c4: sites_raw = st.text_area("–°–∞–π—Ç")
    with c5: group_sel = st.selectbox("–ì—Ä—É–ø–ø–∞", GROUPS)
    
    aggregate_mode = st.checkbox("üßÆ –°—É–º–º–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏–Ω–∞–Ω—Å—ã –ø–æ –≤—Å–µ–º –ò–ù–ù")
    
    if st.button("üîç –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –æ—Ç—á—ë—Ç", key="ai_build"):
        with st.spinner("–°—á–∏—Ç–∞–µ–º –æ—Ç—á—ë—Ç‚Ä¶"):

    
            # ---------- –ø–∞—Ä—Å–∏–Ω–≥ ----------
            split = lambda s: [i.strip() for i in s.splitlines() if i.strip()]
            inns   = split(inns_raw)
            names  = split(names_raw)
            mkts   = split(mkts_raw)
            sites  = split(sites_raw)
            groups = [group_sel] * len(inns)
            
            # ---------- –≤–∞–ª–∏–¥–∞—Ü–∏—è ----------
            if not inns:
                st.error("–£–∫–∞–∂–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ò–ù–ù."); st.stop()
            
            if aggregate_mode:            # Œ£-—Ä–µ–∂–∏–º
                # –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–∞—Å—Ç—è–≥–∏–≤–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                if len(names) == 1 and len(inns) > 1:  names *= len(inns)
                if len(mkts)  == 1 and len(inns) > 1:  mkts  *= len(inns)
                if len(sites) == 1 and len(inns) > 1:  sites *= len(inns)
                if len(groups) == 1 and len(inns) > 1: groups *= len(inns)
            
                # —Ç–µ–ø–µ—Ä—å –≤—Å—ë –ª–∏–±–æ –ø—É—Å—Ç–æ–µ, –ª–∏–±–æ —Å–æ–≤–ø–∞–¥–∞–µ—Ç –ø–æ –¥–ª–∏–Ω–µ
                for lst, lbl in [(names, "–ù–∞–∑–≤–∞–Ω–∏–µ"), (mkts, "–†—ã–Ω–æ–∫")]:
                    if lst and len(lst) != len(inns):
                        st.error(f"–ß–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ ¬´{lbl}¬ª –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 1 –∏–ª–∏ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —á–∏—Å–ª–æ–º –ò–ù–ù."); st.stop()
            
            else:                         # –æ–¥–∏–Ω–æ—á–Ω—ã–π —Ä–µ–∂–∏–º
                if not (names and mkts):
                    st.error("–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –ø–æ–ª—è ‚Äî –ò–ù–ù, –ù–∞–∑–≤–∞–Ω–∏–µ –∏ –†—ã–Ω–æ–∫."); st.stop()
                if len({len(inns), len(names), len(mkts)}) != 1:
                    st.error("–ß–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ –≤–æ –≤—Å–µ—Ö —Ç—Ä—ë—Ö –ø–æ–ª—è—Ö –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å."); st.stop()
                if sites and len(sites) != len(inns):
                    st.error("–ß–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ ¬´–°–∞–π—Ç¬ª –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —á–∏—Å–ª–æ–º –ò–ù–ù."); st.stop()
                if groups and len(groups) != len(inns):
                    st.error("–ß–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ ¬´–ì—Ä—É–ø–ø–∞¬ª –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —á–∏—Å–ª–æ–º –ò–ù–ù."); st.stop()
            
            # ---------- –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—ã —Å–ø–∏—Å–∫–æ–≤ ----------
            pad = lambda lst: lst if lst else [""] * len(inns)
            names_full = pad(names)
            mkts_full  = pad(mkts)
            sites_full = pad(sites)
            groups_full = pad(groups)
            YEARS = ["2022", "2023", "2024"]
            df_companies = pd.DataFrame([ck_company(i) for i in inns])

            
            def parse_people_cell(cell) -> list[str]:
                """
                –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —è—á–µ–π–∫–∏ ¬´–†—É–∫–æ–≤–æ–¥¬ª –∏–ª–∏ ¬´–£—á—Ä–µ–¥_–§–õ¬ª
                –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ ¬´–§–ò–û (–ò–ù–ù xxxx, –¥–æ–ª—è yy%)¬ª.
                –†–∞–±–æ—Ç–∞–µ—Ç –∏ –µ—Å–ª–∏ cell = NaN, '', —Å–ø–∏—Å–æ–∫, dict, —Å—Ç—Ä–æ–∫–∞-JSON.
                """
                # –ø—É—Å—Ç–æ / NaN
                if cell is None or (isinstance(cell, float) and pd.isna(cell)):
                    return []
            
                # –µ—Å–ª–∏ –ø—Ä–∏—à–ª–∞ —Å—Ç—Ä–æ–∫–∞ ‚Äî –ø—Ä–æ–±—É–µ–º –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –≤ –æ–±—ä–µ–∫—Ç
                if isinstance(cell, str):
                    cell = cell.strip()
                    if not cell:
                        return []
                    try:
                        cell = ast.literal_eval(cell)      # '[{‚Ä¶}]' -> python
                    except (ValueError, SyntaxError):
                        # –ø—Ä–æ—Å—Ç–æ —Å—Ç—Ä–æ–∫–∞ —Å –æ–¥–Ω–∏–º –§–ò–û
                        return [cell]
            
                # –æ–¥–∏–Ω–æ—á–Ω—ã–π dict
                if isinstance(cell, dict):
                    cell = [cell]
            
                # list
                if isinstance(cell, list):
                    out = []
                    for item in cell:
                        if isinstance(item, str):          # —É–∂–µ –≥–æ—Ç–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞
                            out.append(item.strip())
                        elif isinstance(item, dict):       # –Ω–∞—à –æ—Å–Ω–æ–≤–Ω–æ–π —Å–ª—É—á–∞–π
                            fio   = item.get("–§–ò–û") or ""
                            inn   = item.get("–ò–ù–ù") or ""
                            share = item.get("–î–æ–ª—è", {}).get("–ü—Ä–æ—Ü–µ–Ω—Ç")
                            line  = fio
                            if inn:
                                line += f" (–ò–ù–ù {inn}"
                                line += f", –¥–æ–ª—è {int(share)}%)" if share is not None else ")"
                            out.append(line)
                    return [s for s in out if s]
                # fallback
                return [str(cell)]
            
            def row_people_json(row: pd.Series) -> dict:
                """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç {'leaders_raw': [...], 'founders_raw': [...]}."""
                # ‚îÄ‚îÄ 1. —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                leaders = parse_people_cell(row.get("–†—É–∫–æ–≤–æ–¥"))
            
                # ‚îÄ‚îÄ 2. —É—á—Ä–µ–¥–∏—Ç–µ–ª–∏: –∫–æ–ª–æ–Ω–∫–∞ '–£—á—Ä–µ–¥' ‚Üí dict ‚Üí –∫–ª—é—á '–§–õ' ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                founders_cell = None
                uc = row.get("–£—á—Ä–µ–¥")
                if isinstance(uc, dict):
                    founders_cell = uc.get("–§–õ")          # —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
                else:
                    founders_cell = uc                    # fallback (–µ—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –¥—Ä—É–≥–æ–π)
            
                founders = parse_people_cell(founders_cell)
            
                return {"leaders_raw": leaders, "founders_raw": founders}
            
            people_cols = df_companies.apply(row_people_json, axis=1, result_type="expand")
            df_companies = pd.concat([df_companies, people_cols], axis=1)

            

            
            PNL_CODES = [                       # –≤—Å—ë, —á—Ç–æ —Ö–æ—Ç–∏–º –≤–∏–¥–µ—Ç—å –≤ –¥–ª–∏–Ω–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ
                ("–í—ã—Ä—É—á–∫–∞ (‚ÇΩ –º–ª–Ω)",                "2110"),
                ("–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å –ø—Ä–æ–¥–∞–∂ (‚ÇΩ –º–ª–Ω)",   "2120"),
                ("–í–∞–ª–æ–≤–∞—è –ø—Ä–∏–±—ã–ª—å (‚ÇΩ –º–ª–Ω)",        "2200"),
                ("–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ —Ä–∞—Å—Ö–æ–¥—ã (‚ÇΩ –º–ª–Ω)",   "2210"),
                ("–£–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–∏–µ —Ä–∞—Å—Ö–æ–¥—ã (‚ÇΩ –º–ª–Ω)", "2220"),
                ("–ü—Ä–∏–±—ã–ª—å –æ—Ç –ø—Ä–æ–¥–∞–∂ (‚ÇΩ –º–ª–Ω)",      "2300"),
                ("–î–æ—Ö–æ–¥—ã –æ—Ç —É—á–∞—Å—Ç–∏—è (‚ÇΩ –º–ª–Ω)",      "2310"),
                ("–ü—Ä–æ—Ü–µ–Ω—Ç—ã –∫ –ø–æ–ª—É—á–µ–Ω–∏—é (‚ÇΩ –º–ª–Ω)",   "2320"),
                ("–ü—Ä–æ—Ü–µ–Ω—Ç—ã –∫ —É–ø–ª–∞—Ç–µ (‚ÇΩ –º–ª–Ω)",      "2330"),
                ("–ü—Ä–æ—á–∏–µ –¥–æ—Ö–æ–¥—ã (‚ÇΩ –º–ª–Ω)",          "2340"),
                ("–ü—Ä–æ—á–∏–µ —Ä–∞—Å—Ö–æ–¥—ã (‚ÇΩ –º–ª–Ω)",         "2350"),
                ("–ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å (‚ÇΩ –º–ª–Ω)",         "2400"),
                ("–°–æ–≤–æ–∫—É–ø–Ω—ã–π –¥–æ–ª–≥ (‚ÇΩ –º–ª–Ω)",        "_total_debt"),
                ("–î–µ–Ω–µ–∂–Ω—ã–µ —Å—Ä–µ–¥—Å—Ç–≤–∞ (‚ÇΩ –º–ª–Ω)",      "_cash"),
                ("–ö—Ä–µ–¥–∏—Ç–æ—Ä—Å–∫–∞—è –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç—å (‚ÇΩ –º–ª–Ω)", "1520"),
                ("–ß–∏—Å—Ç—ã–π –¥–æ–ª–≥ (‚ÇΩ –º–ª–Ω)",            "_net_debt"),
                ("EBIT margin (%)",                "_ebit_margin"),
                ("Net Debt / EBIT",                "_netdebt_ebit"),
            ]
            
            # ---------- ‚ë† —Å–≤–æ–¥–Ω–∞—è –≤–∫–ª–∞–¥–∫–∞, –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ ----------
            def build_agg_finances() -> dict[str, dict[str, float | None]]:
                """–°—É–º–º–∏—Ä—É–µ—Ç –≤—Å–µ –ò–ù–ù –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å agg[year][code]."""
                NUMERIC = {c for _, c in PNL_CODES if c.isdigit()} | {"1250", "1400", "1500"}
                raw = {y: defaultdict(float) for y in YEARS}
            
                for inn in inns:
                    fin = ck_fin(inn)
                    for y in YEARS:
                        for code in NUMERIC:
                            v = fin.get(y, {}).get(code)
                            if isinstance(v, (int, float)):
                                raw[y][code] += v / 1e6        # ‚Üí –º–ª–Ω
            
                agg = {}
                for y in YEARS:
                    rev   = raw[y]["2110"]
                    ebit  = raw[y]["2200"]
                    cash  = raw[y]["1250"]
                    debt  = raw[y]["1400"] + raw[y]["1500"]
                    pay   = raw[y]["1520"]
            
                    net_debt    = debt - cash - pay
                    ebit_margin = _safe_div(ebit, rev)
                    ebit_margin = ebit_margin * 100 if ebit_margin is not None else None
                    nd_ebit     = _safe_div(net_debt, ebit)
            
            
                    agg[y] = {**raw[y],
                              "_total_debt":  debt,
                              "_cash":        cash,
                              "_net_debt":    net_debt,
                              "_ebit_margin": ebit_margin,
                              "_netdebt_ebit":nd_ebit}
                return agg
            
            # —Å–æ–∑–¥–∞—ë–º –≤–∫–ª–∞–¥–∫–∏ –∑–∞—Ä–∞–Ω–µ–µ (—á—Ç–æ–±—ã –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è tabs –°–£–©–ï–°–¢–í–û–í–ê–õ–ê –≤—Å–µ–≥–¥–∞)
            if aggregate_mode:
                tabs = st.tabs(["Œ£ –°–≤–æ–¥–Ω–æ"] + ([] if len(inns) == 1 else
                                               [f"{n} ({inn})" for inn, n in zip(inns, names_full)]))
                # –±–ª–æ–∫ Œ£ –°–≤–æ–¥–Ω–æ ‚Äî –≤—Å–µ–≥–¥–∞ –ø–µ—Ä–≤—ã–π
                with tabs[0]:
                    agg = build_agg_finances()
            
                    st.header("Œ£ –°–≤–æ–¥–Ω–∞—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞")
                    tbl = pd.DataFrame({"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å": [n for n, _ in PNL_CODES]})
                    for y in YEARS:
                        tbl[y] = [agg[y].get(code) for _, code in PNL_CODES]
            
                    def _fmt(v, pct=False, d=1):
                        if v is None or (isinstance(v, float) and np.isnan(v)): return "‚Äî"
                        return f"{v:.{d}f}{'%' if pct else ''}".replace(".", ",")
                    for i, (nm, _) in enumerate(PNL_CODES):
                        pct  = nm.endswith("%")
                        digs = 2 if ("Net" in nm or pct) else 1
                        tbl.iloc[i, 1:] = [_fmt(v, pct, digs) for v in tbl.iloc[i, 1:]]
            
                    st.dataframe(tbl.set_index("–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å"),
                                 use_container_width=True,
                                 height=min(880, 40 * len(PNL_CODES)))
            
            
                    # –≥—Ä–∞—Ñ–∏–∫
                    # --- –≥—Ä–∞—Ñ–∏–∫: –≤—ã—Ä—É—á–∫–∞ / EBIT / —á–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å + EBIT-margin -----------
                    fig, ax1 = plt.subplots(figsize=(7, 3.5))
                    x = np.arange(len(YEARS)); w = 0.25
                    
                    bars_rev  = ax1.bar(x - w, [agg[y]["2110"] or 0 for y in YEARS],
                                        w, label="–í—ã—Ä—É—á–∫–∞")
                    bars_ebit = ax1.bar(x,     [agg[y]["2200"] or 0 for y in YEARS],
                                        w, label="EBIT")
                    bars_prof = ax1.bar(x + w, [agg[y]["2400"] or 0 for y in YEARS],
                                        w, label="–ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å")
                    
                    # –ø–æ–¥–ø–∏—Å–∏ –Ω–∞ —Å—Ç–æ–ª–±—Ü–∞—Ö
                    for b in (*bars_rev, *bars_ebit, *bars_prof):
                        h = b.get_height()
                        if h and not np.isnan(h):
                            ax1.annotate(f"{h:.1f}", xy=(b.get_x() + b.get_width()/2, h),
                                         xytext=(0, 3), textcoords="offset points",
                                         ha="center", fontsize=8)
                    
                    # –ª–∏–Ω–∏—è EBIT-margin (%)
                    ax2 = ax1.twinx()
                    margins = [agg[y]["_ebit_margin"] if agg[y]["_ebit_margin"] else np.nan for y in YEARS]
                    ax2.plot(x, margins, linestyle="--", marker="o", label="EBIT margin, %")
                    
                    # –ø–æ–¥–ø–∏—Å–∏ ¬´—Ö %¬ª –Ω–∞–¥ —Ç–æ—á–∫–∞–º–∏ –ª–∏–Ω–∏–∏
                    for xx, yy in zip(x, margins):
                        if not np.isnan(yy):
                            ax2.annotate(f"{yy:.1f}%", xy=(xx, yy),
                                         xytext=(0, 5), textcoords="offset points",
                                         ha="center", fontsize=8)
                    
                    # –æ–±—ä–µ–¥–∏–Ω—è–µ–º –ª–µ–≥–µ–Ω–¥—É
                    h1, l1 = ax1.get_legend_handles_labels()
                    h2, l2 = ax2.get_legend_handles_labels()
                    ax1.legend(h1 + h2, l1 + l2, loc="upper left", fontsize=9)
                    # ‚üµ  –ø—Ä—è—á–µ–º —à–∫–∞–ª—ã Y
                    ax1.set_yticks([]); ax2.set_yticks([])
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                    
                    for ax in (ax1, ax2):
                        ax.spines[:].set_visible(False)
                    
                    # –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ
                    ax1.set_xticks(x); ax1.set_xticklabels(YEARS, fontsize=10)
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                    for ax in (ax1, ax2):
                        ax.spines[:].set_visible(False)
                    
                    fig.tight_layout(pad=1.0)
                    st.pyplot(fig)
            
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ü—Ä–æ—Ñ–∏–ª—å / —Ä—ã–Ω–æ–∫ / —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ (+ –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    first_name = names_full[0] or "–ö–æ–º–ø–∞–Ω–∏—è"
                    first_mkt  = mkts_full[0]
                    first_site = sites_full[0]
                    first_inn = inns[0] if inns else None
                    
                    # --- –µ–¥–∏–Ω—ã–π RAG-–ø–∞–π–ø–ª–∞–π–Ω (Google-—Å–Ω–∏–ø–ø–µ—Ç—ã + —Å–∞–π—Ç) ---------------------
                    st.subheader("üìù –û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏")
                    with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏‚Ä¶"):
                        doc = RAG(first_name, website=first_site, market=first_mkt, group=groups_full[0]).run()
                    
                    # ----------- –≤—ã–≤–æ–¥ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞ -----------------------------------
                    html_main = _linkify(doc["summary"]).replace("\n", "<br>")
                    st.markdown(
                        f"<div style='background:#F7F9FA;border:1px solid #ccc;"
                        f"border-radius:8px;padding:18px;line-height:1.55'>{html_main}</div>",
                        unsafe_allow_html=True,
                    )
                    
                    with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                        for i, q in enumerate(doc["queries"], 1):
                            st.markdown(f"**{i}.** {q}")
                    
                    with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                        st.dataframe(
                            pd.DataFrame(doc["snippets"], columns=["URL", "Snippet"]).head(15),
                            use_container_width=True,
                        )
                    
                    # ----------- –æ—Ç–¥–µ–ª—å–Ω–∞—è –ø–ª–∞—à–∫–∞ ¬´–ü–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞¬ª (–µ—Å–ª–∏ –µ—Å—Ç—å) --------------
                    if doc.get("site_pass"):
                        with st.expander("üåê –ü–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞"):
                            st.markdown(
                                f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                                f"border-radius:8px;padding:18px;line-height:1.55'>"
                                f"{_linkify(doc['site_pass']).replace(chr(10), '<br>')}</div>",
                                unsafe_allow_html=True,
                            )
                    else:
                        st.info("–ü–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ –Ω–µ –ø–æ–ª—É—á–µ–Ω (–Ω–µ—Ç URL, –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ –∏—Å—Ç–µ–∫ —Ç–∞–π-–∞—É—Ç).")
                    
                    # ----------- –†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç -------------------------------------------
                    if first_mkt:
                        st.subheader("üìà –†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç")
                        with st.spinner("–°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ä—ã–Ω–∫—É –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑‚Ä¶"):
                            mkt_res = get_market_rag(first_mkt)
                    
                        mkt_html = _linkify(mkt_res["summary"]).replace("\n", "<br>")
                        st.markdown(
                            f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                            f"border-radius:8px;padding:18px;line-height:1.55'>{mkt_html}</div>",
                            unsafe_allow_html=True,
                        )

                        vols = _parse_market_volumes(mkt_res["summary"])
                        if vols:
                            fig, ax = plt.subplots(figsize=(4, 2))
                            years = list(vols.keys())
                            vals = list(vols.values())
                            bars = ax.bar(range(len(years)), vals, color="#4C72B0")
                            ax.set_xticks(range(len(years)))
                            ax.set_xticklabels(years)
                            ax.set_yticks([])
                            for spine in ax.spines.values():
                                spine.set_visible(False)
                            for i, b in enumerate(bars):
                                ax.text(b.get_x() + b.get_width() / 2, b.get_height(),
                                        f"{vals[i]:.1f}", ha="center", va="bottom", fontsize=8)
                            st.pyplot(fig)

                        with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                            for i, q in enumerate(mkt_res["queries"], 1):
                                st.markdown(f"**{i}.** {q}")
                    
                        with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                            st.dataframe(
                                pd.DataFrame(mkt_res["snippets"], columns=["URL", "Snippet"]).head(15),
                                use_container_width=True,
                            )
                    
                    # ----------- –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é -----------------------------------
                    st.subheader("üë• –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é")
                    with st.spinner("–°–æ–±–∏—Ä–∞–µ–º —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏ –∏–Ω—Ç–µ—Ä–≤—å—é‚Ä¶"):
                        # –±–µ—Ä—ë–º Checko-–∫–∞—Ä—Ç–æ—á–∫—É –ø–µ—Ä–≤–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ –≥–æ—Ç–æ–≤–æ–≥–æ DataFrame
                        company_info = df_companies.iloc[0].to_dict()
                    
                        lead_res = get_leaders_rag(
                            first_name,
                            website=first_site,
                            market=first_mkt,
                            company_info=company_info,      # ‚Üê –ø–µ—Ä–µ–¥–∞—ë–º dict —Å leaders_raw / founders_raw
                        )
                    
                    st.markdown(
                        f"<div style='background:#F9FAFB;border:1px solid #ddd;"
                        f"border-radius:8px;padding:18px;line-height:1.55'>"
                        f"{lead_res['summary'].replace(chr(10), '<br>')}</div>",
                        unsafe_allow_html=True,
                    )
                    
                    with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                        for i, q in enumerate(lead_res["queries"], 1):
                            st.markdown(f"**{i}.** {q}")
                    
                    with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                        if lead_res["snippets"]:
                            df = (
                                pd.DataFrame(lead_res["snippets"], columns=["URL", "Snippet"])
                                .drop_duplicates(subset="URL")
                                .head(15)
                            )
                            st.dataframe(df, use_container_width=True)
                        else:
                            st.info("–°–Ω–∏–ø–ø–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
                    
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –∫–æ–Ω–µ—Ü –±–ª–æ–∫–∞, –¥–∞–ª—å—à–µ –≤–∞—à –∫–æ–¥ (–µ—Å–ª–∏ –±—ã–ª) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            
            # ---------- ‚ë° –≤–∫–ª–∞–¥–∫–∏ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –∫–æ–º–ø–∞–Ω–∏—è–º ----------
            if aggregate_mode and len(inns) > 1:
                tabs = st.tabs(["Œ£ –°–≤–æ–¥–Ω–æ"] + [f"{n} ({inn})"
                                               for inn, n in zip(inns, names_full)])
            else:                                   # –æ–¥–∏–Ω–æ—á–Ω—ã–π —Ä–µ–∂–∏–º
                tabs = st.tabs([f"{n} ({inn})" for inn, n
                                in zip(inns, names_full)])
            
            start_idx = 1 if (aggregate_mode and len(inns) > 1) else 0
            
            for idx, (tab, inn, name, mkt, site) in enumerate(
                    zip(
                        tabs[start_idx:],   # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º Œ£-–≤–∫–ª–∞–¥–∫—É –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                        inns,
                        names_full,
                        mkts_full,
                        sites_full,
                    )
            ):
                with tab:
                    st.header(f"{name} ‚Äî {inn}")
                    st.caption(f"–†—ã–Ω–æ–∫: **{mkt or '‚Äî'}**")
            
                    # ---------- –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –ø—Ä–æ—Ñ–∏–ª—å ----------
                    fin = ck_fin(inn)
                    calc = {y: {} for y in YEARS}
            
                    for y in YEARS:
                        yr = fin.get(y, {})
                        # –ø—Ä—è–º—ã–µ —Å—Ç—Ä–æ–∫–∏ –æ—Ç—á—ë—Ç–∞
                        for _, code in PNL_CODES:
                            if code.isdigit():
                                v = yr.get(code)
                                calc[y][code] = (v / 1e6) if isinstance(v, (int, float)) else None
            
                        # —Ä–∞—Å—á—ë—Ç–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
                        cash  = yr.get("1250", 0) / 1e6
                        debt  = (yr.get("1400", 0) + yr.get("1500", 0)) / 1e6
                        pay   = yr.get("1520", 0) / 1e6
                        rev   = calc[y].get("2110")
                        ebit  = calc[y].get("2200")
                        net_debt = debt - cash - pay
                        ebit_m = _safe_div(ebit, rev)
                        ebit_m = ebit_m * 100 if ebit_m is not None else None
                        nd_eb     = _safe_div(net_debt, ebit)
            
                        calc[y].update({
                            "_total_debt": debt or None,
                            "_cash":       cash or None,
                            "_net_debt":   net_debt if rev is not None else None,
                            "_ebit_margin":ebit_m,
                            "_netdebt_ebit":nd_eb,
                        })
            
                    # --- –¥–ª–∏–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ ---
                    tbl = pd.DataFrame({"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å": [n for n, _ in PNL_CODES]})
                    for y in YEARS:
                        tbl[y] = [calc[y].get(code) for _, code in PNL_CODES]
            
                    def fmt(v, pct=False, d=1):
                        if v is None or (isinstance(v, float) and np.isnan(v)): return "‚Äî"
                        return f"{v:.{d}f}{'%' if pct else ''}".replace(".", ",")
            
                    for i, (nm, _) in enumerate(PNL_CODES):
                        pct  = nm.endswith("%")
                        digs = 2 if ("Net" in nm or pct) else 1
                        tbl.iloc[i, 1:] = [fmt(v, pct, digs) for v in tbl.iloc[i, 1:]]
            
                    st.dataframe(tbl.set_index("–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å"),
                                 use_container_width=True,
                                 height=min(880, 40 * len(tbl)))
            
                    # --- –≥—Ä–∞—Ñ–∏–∫: –≤—ã—Ä—É—á–∫–∞ / EBIT / —á–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å + EBIT-margin ---
                    fig, ax1 = plt.subplots(figsize=(7, 3.5))
                    x = np.arange(len(YEARS)); w = 0.25
                    bars_r  = ax1.bar(x - w, [calc[y]["2110"] or 0 for y in YEARS], w, label="–í—ã—Ä—É—á–∫–∞")
                    bars_e  = ax1.bar(x,     [calc[y]["2200"] or 0 for y in YEARS], w, label="EBIT")
                    bars_p  = ax1.bar(x + w, [calc[y]["2400"] or 0 for y in YEARS], w, label="–ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å")
            
                    for b in (*bars_r, *bars_e, *bars_p):
                        h = b.get_height()
                        if h and not np.isnan(h):
                            ax1.annotate(f"{h:.1f}", xy=(b.get_x()+b.get_width()/2, h),
                                         xytext=(0,3), textcoords="offset points",
                                         ha="center", fontsize=8)
            
                    ax2 = ax1.twinx()
                    m_vals = [calc[y]["_ebit_margin"] if calc[y]["_ebit_margin"] else np.nan for y in YEARS]
                    ax2.plot(x, m_vals, linestyle="--", marker="o", label="EBIT margin, %")
                    # ----- –µ–¥–∏–Ω–∞—è –ª–µ–≥–µ–Ω–¥–∞ -----
                    h1, l1 = ax1.get_legend_handles_labels()   # bars
                    h2, l2 = ax2.get_legend_handles_labels()   # –ª–∏–Ω–∏—è margin
                    ax1.legend(h1 + h2, l1 + l2, loc='upper left', fontsize=9)
            
            
                    
                    for xx, yy in zip(x, m_vals):
                        if yy and not np.isnan(yy):
                            ax2.annotate(f"{yy:.1f}%", xy=(xx, yy),
                                         xytext=(0,5), textcoords="offset points",
                                         ha="center", fontsize=8)
            
                    ax1.set_xticks(x); ax1.set_xticklabels(YEARS, fontsize=10)
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                     # ‚üµ  –ø—Ä—è—á–µ–º —à–∫–∞–ª—ã Y
                    ax1.set_yticks([]); ax2.set_yticks([])
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                    
                    for ax in (ax1, ax2):
                        ax.spines[:].set_visible(False)
                    for ax in (ax1, ax2): ax.spines[:].set_visible(False)
                    ax1.legend(loc="upper left", fontsize=9)
                    fig.tight_layout(pad=1.0)
                    st.pyplot(fig)
            
                    
                    
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ (Google + —Å–∞–π—Ç) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    st.subheader("üìù –û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏")
                    with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏‚Ä¶"):
                        doc = RAG(name, website=site, market=mkt, group=groups_full[idx]).run()     # ‚Üê –Ω–æ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
                    
                    # –æ—Å–Ω–æ–≤–Ω–æ–π –æ—Ç—á—ë—Ç
                    main_html = _linkify(doc["summary"]).replace("\n", "<br>")
                    st.markdown(
                        f"<div style='background:#F7F9FA;border:1px solid #ccc;"
                        f"border-radius:8px;padding:18px;line-height:1.55'>{main_html}</div>",
                        unsafe_allow_html=True
                    )
                    
                    with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                        for i, q in enumerate(doc["queries"], 1):
                            st.markdown(f"**{i}.** {q}")
                    
                    with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                        st.dataframe(
                            pd.DataFrame(doc["snippets"], columns=["URL", "Snippet"]).head(15),
                            use_container_width=True,
                        )
                    
                    # üåê –ü–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ (–µ—Å–ª–∏ –ø–æ–ª—É—á–∏–ª—Å—è)
                    if doc.get("site_pass"):
                        with st.expander("üåê –ü–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞"):
                            st.markdown(
                                f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                                f"border-radius:8px;padding:18px;line-height:1.55'>"
                                f"{_linkify(doc['site_pass']).replace(chr(10), '<br>')}</div>",
                                unsafe_allow_html=True,
                            )
                    else:
                        st.info("–ü–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ –Ω–µ –ø–æ–ª—É—á–µ–Ω (–Ω–µ—Ç URL, –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ –∏—Å—Ç–µ–∫ —Ç–∞–π-–∞—É—Ç).")
                    
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    if mkt:
                        st.subheader("üìà –†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç")
                        with st.spinner("–°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ä—ã–Ω–∫—É –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑‚Ä¶"):
                            mkt_res = get_market_rag(mkt)

                        mkt_html = _linkify(mkt_res["summary"]).replace("\n", "<br>")
                        st.markdown(
                            f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                            f"border-radius:8px;padding:18px;line-height:1.55'>{mkt_html}</div>",
                            unsafe_allow_html=True,
                        )

                        vols = _parse_market_volumes(mkt_res["summary"])
                        if vols:
                            fig, ax = plt.subplots(figsize=(4, 2))
                            years = list(vols.keys())
                            vals = list(vols.values())
                            bars = ax.bar(range(len(years)), vals, color="#4C72B0")
                            ax.set_xticks(range(len(years)))
                            ax.set_xticklabels(years)
                            ax.set_yticks([])
                            for spine in ax.spines.values():
                                spine.set_visible(False)
                            for i, b in enumerate(bars):
                                ax.text(b.get_x() + b.get_width() / 2, b.get_height(),
                                        f"{vals[i]:.1f}", ha="center", va="bottom", fontsize=8)
                            st.pyplot(fig)

                        with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                            for i, q in enumerate(mkt_res["queries"], 1):
                                st.markdown(f"**{i}.** {q}")
                    
                        with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                            st.dataframe(
                                pd.DataFrame(mkt_res["snippets"], columns=["URL", "Snippet"]).head(15),
                                use_container_width=True,
                            )
                    
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    st.subheader("üë• –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é")
                    with st.spinner("–°–æ–±–∏—Ä–∞–µ–º —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏ –∏–Ω—Ç–µ—Ä–≤—å—é‚Ä¶"):
                    
                        # ‚ë† –±–µ—Ä—ë–º —Å—ã—Ä—ã–µ —Å–ø–∏—Å–∫–∏ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π / —É—á—Ä–µ–¥–∏—Ç–µ–ª–µ–π –∏–∑ –≥–æ—Ç–æ–≤–æ–≥–æ df_companies
                        company_info = {
                            "leaders_raw":  df_companies.loc[idx, "leaders_raw"]  or [],
                            "founders_raw": df_companies.loc[idx, "founders_raw"] or [],
                        }
                    
                        # ‚ë° –∑–∞–ø—É—Å–∫–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
                        lead_res = get_leaders_rag(
                            name,
                            website=site,
                            market=mkt,
                            company_info=company_info,   # ‚Üê —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–ª—é—á–∏
                        )
                    
                    # –≤—ã–≤–æ–¥
                    st.markdown(
                        f"<div style='background:#F9FAFB;border:1px solid #ddd;"
                        f"border-radius:8px;padding:18px;line-height:1.55'>"
                        f"{lead_res['summary'].replace(chr(10), '<br>')}</div>",
                        unsafe_allow_html=True,
                    )
                    
                    with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                        for i, q in enumerate(lead_res["queries"], 1):
                            st.markdown(f"**{i}.** {q}")
                    
                    with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                        if lead_res["snippets"]:
                            df = (
                                pd.DataFrame(lead_res["snippets"], columns=["URL", "Snippet"])
                                .drop_duplicates(subset="URL")
                                .head(15)
                            )
                            st.dataframe(df, use_container_width=True)
                        else:
                            st.info("–°–Ω–∏–ø–ø–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")

        st.session_state["ai_report"] = {
            "doc":          doc,          # –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏
            "mkt_res":      mkt_res,      # —Ä—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç
            "lead_res":     lead_res,     # —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏/–∏–Ω—Ç–µ—Ä–≤—å—é
            "tbl":          tbl,          # —Ñ–∏–Ω. —Ç–∞–±–ª–∏—Ü–∞ DataFrame
            "graphics":     fig,          # –æ–±—ä–µ–∫—Ç matplotlib (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω –ø–æ–≤—Ç–æ—Ä–Ω—ã–π —Ä–µ–Ω–¥–µ—Ä)
            # ‚Ä¶ —á—Ç–æ-—É–≥–æ–¥–Ω–æ –µ—â—ë
        }
        st.session_state["ai_result_ready"] = True

def long_job(total_sec: int = 180, key_prog: str = "ai_prog"):
    """–§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞, –∫–∞–∂–¥—ã–µ 1 —Å –æ–±–Ω–æ–≤–ª—è–µ—Ç progress –≤ session_state."""
    for i in range(total_sec + 1):
        time.sleep(1)
        st.session_state[key_prog] = i / total_sec     # 0 ‚Ä¶ 1
    st.session_state["ai_done"] = True                 # –æ—Ç—á—ë—Ç –≥–æ—Ç–æ–≤

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 2. UI-—Ñ—É–Ω–∫—Ü–∏–∏ –¥–≤—É—Ö –≤–∫–ª–∞–¥–æ–∫
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def run_advance_eye_tab() -> None:
    st.header("üëÅÔ∏è Advance Eye")

    user_query = st.text_input("–í–≤–µ–¥–∏—Ç–µ –ò–ù–ù –∏–ª–∏ –§–ò–û")
    if st.button("üîç –ù–∞–π—Ç–∏ –∫–æ–Ω—Ç–∞–∫—Ç—ã") and user_query:
        with st.spinner("–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º Dyxless‚Ä¶"):
            res = dyxless_query(user_query, token=DYXLESS_TOKEN, max_rows=20_000)

        if res.get("status"):
            st.success(f"–ü–æ–∫–∞–∑–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: **{res['counts']}**")
            st.json(res["data"] or {"note": "–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"})
        else:
            st.warning(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {res.get('error', '–ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç')}")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ (–æ–¥–∏–Ω —Ä–∞–∑ –∑–∞ —Å–µ—Å—Å–∏—é)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.session_state.setdefault("ai_prog", None)   # float 0‚Ä¶1 –∏–ª–∏ None
st.session_state.setdefault("ai_done", False)  # –æ—Ç—á—ë—Ç –≥–æ—Ç–æ–≤?

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 4. –î–≤–µ –≤–∫–ª–∞–¥–∫–∏
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
tab_ai, tab_eye = st.tabs(["üìä AI-Insight", "üëÅÔ∏è Advance Eye"])

# === –≤–∫–ª–∞–¥–∫–∞ 1: AI-Insight =========================================

with tab_ai:
    run_ai_insight_tab()       # –≤—Å—è –ª–æ–≥–∏–∫–∞ –≤–Ω—É—Ç—Ä–∏

with tab_eye:
    run_advance_eye_tab()      # –ø–æ–∏—Å–∫ Dyxless


# In[6]:





# In[14]:





# In[13]:





# In[ ]:




