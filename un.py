#!/usr/bin/env python
# coding: utf-8

# In[ ]:


#!/usr/bin/env python
# coding: utf-8

# In[6]:
#from timesheet_tab import render_timesheet_tab, ensure_db
#ensure_db()  # —Å–æ–∑–¥–∞—Å—Ç —Ç–∞–±–ª–∏—Ü—ã –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –ë–î –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º API-–∫–ª—é—á OpenAI
import os
import requests
import pandas as pd
import re
import numpy as np
import requests
from bs4 import BeautifulSoup
import openai
from typing import List, Dict, Any, Tuple
import json
import streamlit as st
from collections import defaultdict
import asyncio, aiohttp, re, textwrap, nest_asyncio, openai, tiktoken
from collections import defaultdict
from urllib.parse import urlparse
import tldextract, re, asyncio, aiohttp                      # –µ—Å–ª–∏ tldextract —É–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω ‚Äì —ç—Ç—É —Å—Ç—Ä–æ–∫—É –º–æ–∂–Ω–æ –∫–æ—Ä–æ—á–µ
from functools import partial  
import threading
import time
import functools
import ast
from timesheet_tab import render_timesheet_tab, ensure_db
ensure_db()  # –±–µ–∑–æ–ø–∞—Å–Ω–æ –¥–µ—Ä–≥–∞—Ç—å –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ (—Å–æ–∑–¥–∞—Å—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â–µ–µ)

KEYS = {
    "OPENAI_API_KEY": st.secrets["OPENAI_API_KEY"],
    "GOOGLE_API_KEY": st.secrets["GOOGLE_API_KEY"],
    "GOOGLE_CX":      st.secrets["GOOGLE_CX"],
    "CHECKO_API_KEY": st.secrets["CHECKO_API_KEY"],
    "DYXLESS_TOKEN": st.secrets["DYXLESS_TOKEN"],
    "SONAR_API_KEY": st.secrets["SONAR_API_KEY"],
}

DYXLESS_TOKEN = KEYS["DYXLESS_TOKEN"]

try:
    st.cache_data.clear()
except Exception:
    pass

def _no_cache(*args, **kwargs):
    def _decorator(func):
        return func
    return _decorator

st.cache_data = _no_cache

# --- –µ–¥–∏–Ω–∞—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞ –≥–æ–¥–æ–≤ (–≤ –º–æ–¥—É–ª–µ, –Ω–µ –≤–Ω—É—Ç—Ä–∏ —Ñ—É–Ω–∫—Ü–∏–π!) ---
YEARS = ["2022", "2023", "2024"]

# ‚îÄ‚îÄ –û–±—â–∏–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã (–µ–¥–∏–Ω—ã–µ –¥–ª—è –≤—Å–µ–≥–æ —Ñ–∞–π–ª–∞)
HEADERS = {"User-Agent": "Mozilla/5.0 (Win64) AppleWebKit/537.36 Chrome/125 Safari/537.36"}
_URL_PAT = re.compile(r"https?://[^\s)<>\"']+", flags=re.I)

# –î–≤–µ —á—ë—Ç–∫–∏–µ –≤–µ—Ä—Å–∏–∏ linkify, —á—Ç–æ–±—ã –Ω–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞—Ç—å:
def linkify_as_word(text: str, label: str = "—Å—Å—ã–ª–∫–∞") -> str:
    """–ó–∞–º–µ–Ω—è–µ—Ç URL –Ω–∞ <a>label</a> (–∫—Ä–∞—Ç–∫–∞—è –≤–µ—Ä—Å–∏—è)."""
    if not isinstance(text, str):
        text = "" if text is None else str(text)
    return _URL_PAT.sub(lambda m: f'<a href="{html.escape(m.group(0))}" target="_blank" rel="noopener">{label}</a>', text)

def linkify_keep_url(text: str) -> str:
    """–ó–∞–º–µ–Ω—è–µ—Ç URL –Ω–∞ <a>—Å–∞–º URL</a> (–ø–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è)."""
    if not isinstance(text, str):
        text = "" if text is None else str(text)
    return _URL_PAT.sub(lambda m: f'<a href="{html.escape(m.group(0))}" target="_blank" rel="noopener">{html.escape(m.group(0))}</a>', text)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ app.py ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import os, re, asyncio, aiohttp, requests, nest_asyncio, logging
import streamlit as st
import pandas as pd, numpy as np, matplotlib.pyplot as plt

nest_asyncio.apply()
logging.basicConfig(level=logging.WARNING)
import os, re, html, textwrap, asyncio, logging, nest_asyncio
import aiohttp, requests, streamlit as st
import pandas as pd, numpy as np, matplotlib.pyplot as plt
import tldextract, openai


import html, re

_URL_PAT = re.compile(r"https?://[^\s)]+", flags=re.I)

def _linkify(text) -> str:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç URL –≤ <a ‚Ä¶>—Å—Å—ã–ª–∫–∞</a>."""
    if not isinstance(text, str):                      # << –≥–ª–∞–≤–Ω–æ–µ
        text = "" if text is None else str(text)

    def repl(m):
        u = html.escape(m.group(0))
        return f'<a href="{u}" target="_blank">—Å—Å—ã–ª–∫–∞</a>'
    return _URL_PAT.sub(repl, text)





# ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
async def _site_snippet(domain: str) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—ã–π Google-—Å–Ω–∏–ø–ø–µ—Ç –¥–ª—è site:domain (–∏–ª–∏ '')."""
    if not domain:
        return ""
    async with aiohttp.ClientSession() as sess:
        q = f"site:{domain}"
        snips = await _google(sess, q, n=1)
    return snips[0][1] if snips else ""



@st.cache_data(ttl=3_600, show_spinner=False)
def dyxless_query(query: str,
                  token: str,
                  max_rows: int = 20_000) -> Dict[str, Any]:
    """
    –û–±—ë—Ä—Ç–∫–∞ Dyxless. –ï—Å–ª–∏ –∑–∞–ø–∏—Å–µ–π > max_rows ‚Äì –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ max_rows,
    –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø–∏—à–µ–º truncated=True –∏ original_counts.
    """
    url = "https://api-dyxless.cfd/query"
    try:
        r = requests.post(url, json={"query": query, "token": token}, timeout=15)
        r.raise_for_status()
        res = r.json()

        if res.get("status") and isinstance(res.get("data"), list):
            full = len(res["data"])
            if full > max_rows:
                res["data"] = res["data"][:max_rows]
                res.update(truncated=True, original_counts=full, counts=max_rows)
        return res
    except requests.RequestException as e:
        return {"status": False, "error": str(e)}












# ‚ï≠‚îÄüîß  –≤—Å–ø–æ–º–æ–≥–∞–ª–∫–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
_BAD = ("vk.com", "facebook.", ".pdf", ".jpg", ".png")
HEADERS = {"User-Agent": "Mozilla/5.0 (Win64) AppleWebKit/537.36 Chrome/125 Safari/537.36"}
def _bad(u: str) -> bool: return any(b in u.lower() for b in _BAD)

async def _google(sess, q, n=3):
    q = re.sub(r'[\"\'‚Äú‚Äù]', " ", q)[:80]
    params = {"key": KEYS["GOOGLE_API_KEY"], "cx": KEYS["GOOGLE_CX"],
              "q": q, "num": n, "hl": "ru", "gl": "ru"}
    async with sess.get("https://www.googleapis.com/customsearch/v1",
                         params=params, headers=HEADERS, timeout=8) as r:
        if r.status != 200:
            logging.warning(f"Google error {r.status}")
            return []
        js = await r.json()
        return [(i["link"], i.get("snippet", "")) for i in js.get("items", [])
                if not _bad(i["link"])]






async def _gpt(messages, *, model="gpt-4o-mini", T=0.2):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ OpenAI ChatCompletion ‚Üí str."""
    chat = await openai.ChatCompletion.acreate(
        model=model, temperature=T, messages=messages)
    return chat.choices[0].message.content.strip()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –æ—Å–Ω–æ–≤–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞ (–≤ —Å—Ç–∏–ª–µ –≤–∞—à–µ–≥–æ RAG) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class SiteRAG:
    """
    url        ‚Äì –∞–¥—Ä–µ—Å —Å–∞–π—Ç–∞ (–º–æ–∂–Ω–æ –±–µ–∑ http/https)
    max_chunk  ‚Äì –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ–¥–Ω–æ–≥–æ –∫—É—Å–∫–∞ HTML, –∫–æ—Ç–æ—Ä—ã–π —É–π–¥—ë—Ç LLM
    summary    ‚Äì –∏—Ç–æ–≥–æ–≤—ã–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç –∫–æ–º–ø–∞–Ω–∏–∏
    chunks_out ‚Äì —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—é–º–µ c –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏ HTML
    html_size  ‚Äì —Ä–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–≥–æ HTML-—Ñ–∞–π–ª–∞, bytes
    """
    def __init__(self, url: str, *, model="gpt-4o-mini",
                 max_chunk: int = 6_000, T: float = 0.18):
        if not url.startswith(("http://", "https://")):
            url = "http://" + url
        self.url       = url
        self.model     = model
        self.max_chunk = max_chunk
        self.T         = T

    # ---------- 1. —Å–∫–∞—á–∏–≤–∞–µ–º HTML ------------------------------------------------
    async def _fetch(self) -> str:
        h = {"User-Agent": "Mozilla/5.0"}
        async with aiohttp.ClientSession(headers=h) as sess:
            async with sess.get(self.url, timeout=20) as r:
                if r.status == 200 and "text/html" in r.headers.get("Content-Type", ""):
                    return await r.text("utf-8", errors="ignore")
                raise RuntimeError(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å {self.url} (status={r.status})")

    # ---------- 2. –¥–µ–ª–∏–º HTML –Ω–∞ ¬´–±–µ–∑–æ–ø–∞—Å–Ω—ã–µ¬ª –∫—É—Å–∫–∏ -----------------------------
    def _split(self, html_raw: str) -> list[str]:
        # –ø—Ä–æ–±—É–µ–º —Ä–µ–∑–∞—Ç—å –ø–æ –∫—Ä—É–ø–Ω—ã–º —Ç–µ–≥–∞–º, —á—Ç–æ–±—ã –∫—É—Å–∫–∏ –±—ã–ª–∏ —Å–≤—è–∑–Ω—ã
        body = re.split(r"</?(?:body|div|section|article)[^>]*>", html_raw, flags=re.I)
        chunks, buf = [], ""
        for part in body:
            if len(buf) + len(part) > self.max_chunk:
                chunks.append(buf); buf = part
            else:
                buf += part
        if buf:
            chunks.append(buf)
        return chunks

    # ---------- 3. map-—Ñ–∞–∑–∞: –∫–æ–Ω—Å–ø–µ–∫—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∫—É—Å–æ–∫ -------------------------
    async def _summarise_chunk(self, n: int, total: int, chunk: str) -> str:
        sys = (
            "–¢—ã ‚Äì –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –ü—Ä–æ—á–∏—Ç–∞–π –¥–∞–Ω–Ω—ã–π HTML-—Ñ—Ä–∞–≥–º–µ–Ω—Ç –∏ "
            "–≤—ã–ø–∏—à–∏ –í–°–ï –∑–Ω–∞—á–∏–º—ã–µ —Ñ–∞–∫—Ç—ã –æ –∫–æ–º–ø–∞–Ω–∏–∏ (–ø—Ä–æ–¥—É–∫—Ç—ã, —É—Å–ª—É–≥–∏, –∏—Å—Ç–æ—Ä–∏—è, "
            "–≥–µ–æ–≥—Ä–∞—Ñ–∏—è, –∫–ª–∏–µ–Ω—Ç—ã, —Ü–∏—Ñ—Ä—ã, –∫–æ–º–∞–Ω–¥–∞, –∫–æ–Ω—Ç–∞–∫—Ç—ã –∏ –ø—Ä.). "
            "–£–¥–∞–ª–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏—é/footer/—Å–∫—Ä–∏–ø—Ç—ã. –°–æ—Ö—Ä–∞–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞–±–∑–∞—Ü–∞–º–∏."
        )
        return await _gpt([
            {"role": "system", "content": sys},
            {"role": "user",
             "content": f"HTML_CHUNK_{n}/{total} (len={len(chunk):,}):\n{chunk}"}],
            model=self.model, T=self.T)

    # ---------- 4. reduce-—Ñ–∞–∑–∞: –¥–µ–ª–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç ------------------------
    async def _summarise_overall(self, parts: list[str]) -> str:
        sys = (
            "–ù–∏–∂–µ —É–∂–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–Ω—Å–ø–µ–∫—Ç—ã —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–µ–π —Å–∞–π—Ç–∞. "
            "–ù–∞ –∏—Ö –æ—Å–Ω–æ–≤–µ —Å–æ—Å—Ç–∞–≤—å –æ–¥–∏–Ω –ü–û–õ–ù–´–ô –∏ —Å–≤—è–∑–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç –∫–æ–º–ø–∞–Ω–∏–∏: "
            "‚Ä¢ –∫—Ç–æ –æ–Ω–∏ –∏ —á–µ–º –∑–∞–Ω–∏–º–∞—é—Ç—Å—è; ‚Ä¢ –ø—Ä–æ–¥—É–∫—Ç—ã / —É—Å–ª—É–≥–∏; ‚Ä¢ —Ä—ã–Ω–æ–∫ –∏ –∫–ª–∏–µ–Ω—Ç—ã; "
            "‚Ä¢ –∏—Å—Ç–æ—Ä–∏—è –∏ –∫–ª—é—á–µ–≤—ã–µ —Å–æ–±—ã—Ç–∏—è; ‚Ä¢ –≥–µ–æ–≥—Ä–∞—Ñ–∏—è –∏ –º–∞—Å—à—Ç–∞–±—ã; "
            "‚Ä¢ —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ / –∫–æ–º–∞–Ω–¥–∞; ‚Ä¢ –ª—é–±—ã–µ —Ü–∏—Ñ—Ä—ã –∏ —Ñ–∞–∫—Ç—ã; "
            "‚Ä¢ –≤—ã–≤–æ–¥ –æ –ø–æ–∑–∏—Ü–∏–∏ –∏ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞—Ö. –ù–∏—á–µ–≥–æ –≤–∞–∂–Ω–æ–≥–æ –Ω–µ —É–ø—É—Å—Ç–∏."
        )
        merged = "\n\n".join(parts)
        return await _gpt([
            {"role": "system", "content": sys},
            {"role": "user",   "content": merged}],
            model=self.model, T=self.T)

    # ---------- orchestrator ----------------------------------------------------
    async def _run_async(self):
        html_raw = await self._fetch()
        chunks   = self._split(html_raw)

        # map
        part_summaries = []
        for idx, ch in enumerate(chunks, 1):
            print(f"‚Üí LLM chunk {idx}/{len(chunks)} ‚Ä¶")
            part_summaries.append(await self._summarise_chunk(idx, len(chunks), ch))

        # reduce
        summary_final = await self._summarise_overall(part_summaries)

        return {"summary":    summary_final,
                "chunks_out": part_summaries,
                "html_size":  f"{len(html_raw):,} bytes",
                "url":        self.url}

    # ---------- –ø—É–±–ª–∏—á–Ω—ã–π —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å ----------------------------------
    def run(self) -> dict:
        loop = asyncio.get_event_loop()
        if loop and loop.is_running():
            return loop.run_until_complete(self._run_async())
        return asyncio.run(self._run_async())




# ---------- helper: —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –¥–æ—Å—Ç–∞—ë–º –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ -----------------
def _site_passport_sync(url: str, *, max_chunk: int = 6_000) -> str:
    """–í—ã–∑—ã–≤–∞–µ—Ç SiteRAG(url).run() –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ summary."""
    try:
        return SiteRAG(url, max_chunk=max_chunk).run()["summary"]
    except Exception as exc:
        return f"[site passport error: {exc}]"

# ‚ï≠‚îÄüßæ  INVEST SNAPSHOT (cheap, 1 call) ‚Äî –∞–¥—Ä–µ—Å–∞ –º–æ—â–Ω–æ—Å—Ç–µ–π, —Å–æ—Ü—Å–µ—Ç–∏, –Ω–æ–≤–æ—Å—Ç–∏, –∏–Ω—Ç–µ—Ä–≤—å—é, –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã, headcount ‚îÄ‚ïÆ
import json, requests, re
from typing import Optional

API_URL_INVEST = "https://api.perplexity.ai/chat/completions"

class PPLXError(Exception):
    ...

def _pplx_call_invest(
    prompt: str,
    model: str = "sonar",            # –¥–µ—à—ë–≤–∞—è –º–æ–¥–µ–ª—å
    recency: Optional[str] = None,   # None = —à–∏—Ä–æ–∫–∏–π –æ—Ö–≤–∞—Ç (5 –ª–µ—Ç)
    temperature: float = 0.0,
    max_tokens: int = 1500,
    timeout: int = 60,
) -> str:
    key = (os.getenv("SONAR_API_KEY") or os.getenv("PPLX_API_KEY") or os.getenv("PERPLEXITY_API_KEY") or "").strip()
    assert key.startswith("pplx-"), "–£—Å—Ç–∞–Ω–æ–≤–∏ SONAR_API_KEY = pplx-..."
    headers = {"Authorization": f"Bearer {key}", "Content-Type": "application/json", "Accept": "application/json"}
    payload = {
        "model": model,
        "messages": [
            {"role":"system","content":(
                "–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ M&A. –û—Ö–≤–∞—Ç 5 –ª–µ—Ç. –°—Ç—Ä–æ–≥–æ —Ñ–∞–∫—Ç—ã –∏–∑ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤. "
                "–ó–∞–ø—Ä–µ—â–µ–Ω–æ —É–ø–æ–º–∏–Ω–∞—Ç—å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ (–≤—ã—Ä—É—á–∫–∞, –ø—Ä–∏–±—ã–ª—å, EBITDA –∏ —Ç.–ø.), "
                "–ò–ù–ù/–û–ì–†–ù –∏ –ª—é–±—ã–µ –≤—ã–≤–æ–¥—ã –Ω–∞ –∏—Ö –æ—Å–Ω–æ–≤–µ. –ú–æ–∂–Ω–æ —É–ø–æ–º–∏–Ω–∞—Ç—å –∏–º–µ–Ω–∞ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤/—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π "
                "–¢–û–õ–¨–ö–û –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏—Ö –∏–Ω—Ç–µ—Ä–≤—å—é. "
                "–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –ø–æ –ø—É–Ω–∫—Ç—É –Ω–µ—Ç ‚Äî –ø–∏—à–∏ '–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö'. –ù–µ –ø–æ–≤—Ç–æ—Ä—è–π –æ–¥–Ω—É –∏ —Ç—É –∂–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–∞—Ö. "
                "–í –∫–æ–Ω—Ü–µ —É–∫–∞–∂–∏ –ø—Ä—è–º—ã–µ URL –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."
            )},
            {"role":"user","content": prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    if recency in {"hour","day","week","month","year"}:
        payload["search_recency_filter"] = recency

    r = requests.post(API_URL_INVEST, headers=headers, json=payload, timeout=timeout)
    if r.status_code != 200:
        try: err = r.json()
        except Exception: err = {"error": r.text}
        raise PPLXError(f"HTTP {r.status_code}: {json.dumps(err, ensure_ascii=False)[:900]}")
    return r.json()["choices"][0]["message"]["content"]

# —Ñ–∏–ª—å—Ç—Ä –∑–∞–ø—Ä–µ—Ç–Ω—ã—Ö —Ç–µ–º: —Ñ–∏–Ω–ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –∏ —Ä–µ–≥.–Ω–æ–º–µ—Ä–∞ (—Ä–∞–∑—Ä–µ—à–∞–µ–º –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤ —Ç–æ–ª—å–∫–æ –≤ –∏–Ω—Ç–µ—Ä–≤—å—é)
_FORBIDDEN_INVEST = re.compile(
    r"(\b–≤—ã—Ä—É—á–∫|\b–ø—Ä–∏–±—ã–ª|\bebit(?:da)?\b|–º–∞—Ä–∂–∏–Ω–∞–ª|—Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω|—Ñ–∏–Ω–∞–Ω—Å|\b–∏–Ω–Ω\b|\b–æ–≥—Ä–Ω\b|—É—Å—Ç–∞–≤–Ω\w*\s+–∫–∞–ø–∏—Ç–∞–ª)",
    re.IGNORECASE
)

def _dedup_lines_invest(text: str) -> str:
    """–£–¥–∞–ª–∏—Ç—å —Ç–æ—á–Ω—ã–µ –ø–æ–≤—Ç–æ—Ä—ã —Å—Ç—Ä–æ–∫ –∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è URL –≤ —Å—Ç—Ä–æ–∫–∞—Ö, —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Ä—è–¥–æ–∫."""
    seen_lines, seen_urls, out = set(), set(), []
    url_re = re.compile(r'https?://\S+')
    for raw in text.splitlines():
        ln = raw.strip()
        if not ln:
            if out and out[-1] != "":
                out.append("")
            continue
        urls = url_re.findall(ln)
        for u in urls:
            if u in seen_urls:
                ln = ln.replace(u, "")
            else:
                seen_urls.add(u)
        ln = re.sub(r'\s{2,}', ' ', ln).strip()
        if ln and ln not in seen_lines:
            out.append(ln); seen_lines.add(ln)
    cleaned = []
    for i, ln in enumerate(out):
        if ln == "" and (i == 0 or (i+1 < len(out) and out[i+1] == "")):
            continue
        cleaned.append(ln)
    return "\n".join(cleaned).strip()

def sanitize_invest(text: str) -> str:
    """–£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å —Ñ–∏–Ω–ø–æ–∫–∞–∑–∞—Ç–µ–ª—è–º–∏/–ò–ù–ù/–û–ì–†–ù/—É—Å—Ç–∞–≤–Ω—ã–º –∫–∞–ø–∏—Ç–∞–ª–æ–º –∏ —á–∏—Å—Ç–∏–º –ø–æ–≤—Ç–æ—Ä—ã."""
    keep = []
    for ln in text.splitlines():
        if _FORBIDDEN_INVEST.search(ln):
            continue
        keep.append(ln)
    return _dedup_lines_invest("\n".join(keep))

def build_invest_prompt(company: str, site_hint: Optional[str] = None) -> str:
    site = f"\n–í–æ–∑–º–æ–∂–Ω—ã–π –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç: {site_hint}." if site_hint else ""
    return f"""
–°–¥–µ–ª–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á—ë—Ç –ø–æ –∫–æ–º–ø–∞–Ω–∏–∏ ¬´{company}¬ª –Ω–∞ —Ä—É—Å—Å–∫–æ–º (–æ—Ö–≤–∞—Ç: 5 –ª–µ—Ç).{site}
–§–æ—Ä–º–∞—Ç —Å—Ç—Ä–æ–≥–æ Markdown —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ —É—Ä–æ–≤–Ω—è ### –∏ –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –∞–±–∑–∞—Ü–∞–º–∏ (–±–µ–∑ —Å–ø–∏—Å–∫–æ–≤). –ù–µ –¥—É–±–ª–∏—Ä—É–π —Ñ–∞–∫—Ç—ã –º–µ–∂–¥—É —Ä–∞–∑–¥–µ–ª–∞–º–∏.

### –ü—Ä–æ—Ñ–∏–ª—å
1‚Äì2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: —á—Ç–æ –∑–∞ –∫–æ–º–ø–∞–Ω–∏—è, —á–µ–º –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è (–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–æ–≤–∞—Ä–æ–≤/—É—Å–ª—É–≥), –≥–µ–æ–≥—Ä–∞—Ñ–∏—è. –ï—Å–ª–∏ –µ—Å—Ç—å –æ–¥–Ω–æ–∏–º—ë–Ω–Ω—ã–µ —Ñ–∏—Ä–º—ã ‚Äî –∫—Ä–∞—Ç–∫–æ —É–∫–∞–∂–∏ –¥–∏–∑–∞–º–±–∏–≥—É–∞—Ü–∏—é –ø–æ –ø—Ä–æ—Ñ–∏–ª—é/—Å–∞–π—Ç—É.

### –ë–∏–∑–Ω–µ—Å-–º–æ–¥–µ–ª—å
–ö–∞–∫ –∫–æ–º–ø–∞–Ω–∏—è –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç: –∫–∞–Ω–∞–ª—ã (—Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–¥–∞–∂–∏/–¥–∏—Å—Ç—Ä–∏–±—É—Ü–∏—è/–æ–ø—Ç/—Ä–æ–∑–Ω–∏—Ü–∞/–º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å—ã/–¥–∏–ª–µ—Ä—ã), —Å–µ—Ä–≤–∏—Å–Ω—ã–µ –º–æ–¥–µ–ª–∏, –ø–æ–¥–ø–∏—Å–∫–∏/—Å–µ—Ä–≤–∏—Å–Ω—ã–µ –∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏. –ë–µ–∑ —Ü–∏—Ñ—Ä –∏ –æ—Ü–µ–Ω–æ–∫ ‚Äî —Ç–æ–ª—å–∫–æ —è–≤–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å).

### –ê–∫—Ç–∏–≤—ã –∏ –ø–ª–æ—â–∞–¥–∫–∏
–ê–¥—Ä–µ—Å–∞ –≤—Å–µ—Ö –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ (–æ—Ñ–∏—Å—ã, —Å–∫–ª–∞–¥—ã, –†–¶, –º–∞–≥–∞–∑–∏–Ω—ã/–ü–í–ó). –ü–ª–æ—â–∞–¥–∏ (–º¬≤) –∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (–∫–æ–ª-–≤–æ –†–¶/—Å–∫–ª–∞–¥–æ–≤, —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ/–∞—Ä–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ) ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä—è–º–æ —É–∫–∞–∑–∞–Ω–æ –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö.

### –ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–æ—â–Ω–æ—Å—Ç–∏ –∏ –∞–¥—Ä–µ—Å–∞
–£–∫–∞–∂–∏ –Ω–∞–ª–∏—á–∏–µ/–æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞. –ü—Ä–∏–≤–µ–¥–∏ –ê–î–†–ï–°–ê –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–ª–æ—â–∞–¥–æ–∫/—Ü–µ—Ö–æ–≤/–∫–æ–º–±–∏–Ω–∞—Ç–æ–≤ –∏ –ø–æ –∫–∞–∂–¥–æ–π ‚Äî —á—Ç–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è. –ú–æ—â–Ω–æ—Å—Ç–∏ (–µ–¥./–º¬≤/—Ç–æ–Ω–Ω/–º–µ—Å—è—Ü) –∏ —Å—Ç–µ–ø–µ–Ω—å –∑–∞–≥—Ä—É–∑–∫–∏ ‚Äî –µ—Å–ª–∏ —Ä–∞—Å–∫—Ä—ã—Ç—ã. –õ–æ–∫–∞—Ü–∏–∏ –ª–æ–≥–∏—Å—Ç–∏–∫–∏ –∏ —Å–∫–ª–∞–¥—Å–∫–æ–π —Å–µ—Ç–∏ ‚Äî –∫—Ä–∞—Ç–∫–æ.

### –ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–ª–∞–Ω—ã –∏ –ø—Ä–æ–µ–∫—Ç—ã
–ó–∞—è–≤–ª–µ–Ω–Ω—ã–µ/–æ–∂–∏–¥–∞–µ–º—ã–µ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ –≤ –Ω–æ–≤—ã–µ –ª–∏–Ω–∏–∏/—Å–∫–ª–∞–¥—ã/–†–¶/–ø–ª–æ—â–∞–¥–∫–∏; —Å—Ä–æ–∫–∏ –∏ —Å—Ç–∞—Ç—É—Å. –°—Å—ã–ª–∫–∏ –Ω–∞ –ø–µ—Ä–≤–æ–∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ —Å–∫–æ–±–∫–∞—Ö.

### –ö–ª–∏–µ–Ω—Ç—ã –∏ –∫–∞–Ω–∞–ª—ã —Å–±—ã—Ç–∞
B2B/B2C; —Å–µ–≥–º–µ–Ω—Ç—ã/–≤–µ—Ä—Ç–∏–∫–∞–ª–∏; –ø—Ä–æ–¥–∞–∂–∏ —á–µ—Ä–µ–∑ —Å–∞–π—Ç/–º–∞–≥–∞–∑–∏–Ω—ã/–º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å—ã/–¥–∏–ª–µ—Ä–æ–≤; –æ—Ä–∏–µ–Ω—Ç–∏—Ä—ã –ø–æ —á–∏—Å–ª—É –∫–ª–∏–µ–Ω—Ç–æ–≤ ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—É–±–ª–∏—á–Ω–æ –∏ —Å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º.

### –ß–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å –ø–µ—Ä—Å–æ–Ω–∞–ª–∞
–ß–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤/—à—Ç–∞—Ç–∞ (–µ—Å–ª–∏ –ø—É–±–ª–∏—á–Ω–æ), –¥–∞—Ç–∞/–ø–µ—Ä–∏–æ–¥ –∏ –∏—Å—Ç–æ—á–Ω–∏–∫ –≤ —Å–∫–æ–±–∫–∞—Ö. –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ—Ü–µ–Ω–∫–∏ ‚Äî –ø–µ—Ä–µ–¥–∞–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞.

### –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã (–†–æ—Å—Å–∏—è)
–ü–µ—Ä–µ—á–∏—Å–ª–∏ 5‚Äì12 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –ø–æ –ø—Ä–æ—Ñ–∏–ª—é –±–∏–∑–Ω–µ—Å–∞; —Ñ–æ—Ä–º–∞—Ç: –ù–∞–∑–≤–∞–Ω–∏–µ (–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç) –≤ –æ–¥–Ω–æ–º –∞–±–∑–∞—Ü–µ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é. –¢–æ–ª—å–∫–æ –∫–æ–º–ø–∞–Ω–∏–∏, —Ä–µ–∞–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞—é—â–∏–µ –≤ –†–§ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –ª–µ—Ç.

### –ù–æ–≤–æ—Å—Ç–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –ª–µ—Ç)
–î–∞–π 5‚Äì12 –∑–Ω–∞—á–∏–º—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –æ –∫–æ–º–ø–∞–Ω–∏–∏: ¬´–ó–∞–≥–æ–ª–æ–≤–æ–∫¬ª ‚Äî URL (–¥–∞—Ç–∞). –í –æ–¥–Ω–æ–º –∞–±–∑–∞—Ü–µ; –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π —Å—Å—ã–ª–∫–∏.

### –ò–Ω—Ç–µ—Ä–≤—å—é (–≤–ª–∞–¥–µ–ª—å—Ü—ã/—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ)
–î–∞–π 3‚Äì8 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤—å—é/–ø—É–±–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤: ¬´–°–ø–∏–∫–µ—Ä ‚Äî –ó–∞–≥–æ–ª–æ–≤–æ–∫/—Ç–µ–º–∞¬ª ‚Äî URL (–¥–∞—Ç–∞). –í –æ–¥–Ω–æ–º –∞–±–∑–∞—Ü–µ. –ù–µ —Ä–∞—Å–∫—Ä—ã–≤–∞–π –¥–æ–ª–∏/—Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤–ª–∞–¥–µ–Ω–∏—è, —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç –∏–Ω—Ç–µ—Ä–≤—å—é.

### –¶–∏—Ñ—Ä–æ–≤—ã–µ –∫–∞–Ω–∞–ª—ã –∏ –∫–æ–Ω—Ç–∞–∫—Ç—ã
–°–∞–π—Ç (–µ—Å–ª–∏ –µ—Å—Ç—å), e-mail/—Ç–µ–ª–µ—Ñ–æ–Ω—ã (–µ—Å–ª–∏ –ø—É–±–ª–∏—á–Ω–æ), –í–°–ï –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–æ—Ü—Å–µ—Ç–∏ (VK, Telegram, YouTube, RuTube, OK, Instagram*, Facebook*, LinkedIn), –∫–∞—Ç–∞–ª–æ–≥–∏/–∫–∞—Ä—Ç—ã (2–ì–ò–°, –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç—ã, Google Maps) ‚Äî –ø—Ä–∏–≤–æ–¥–∏ –ü–†–Ø–ú–´–ï URL.

### –ò—Å—Ç–æ—á–Ω–∏–∫–∏
–ü–µ—Ä–µ—á–∏—Å–ª–∏ –≤—Å–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä—è–º—ã–µ URL —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é, –±–µ–∑ –Ω—É–º–µ—Ä–∞—Ü–∏–∏.

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
‚Äî –ù–µ —É–∫–∞–∑—ã–≤–∞–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–≤—ã—Ä—É—á–∫–∞/–ø—Ä–∏–±—ã–ª—å/EBITDA –∏ —Ç.–ø.), –ò–ù–ù/–û–ì–†–ù/—É—Å—Ç–∞–≤–Ω—ã–π –∫–∞–ø–∏—Ç–∞–ª –∏ –≤—ã–≤–æ–¥—ã –Ω–∞ –∏—Ö –æ—Å–Ω–æ–≤–µ.
‚Äî –°—Ç—Ä–æ–≥–æ –∏–∑–±–µ–≥–∞–π –ø–æ–≤—Ç–æ—Ä–æ–≤ –º–µ–∂–¥—É —Ä–∞–∑–¥–µ–ª–∞–º–∏ –∏ –ø–æ–≤—Ç–æ—Ä–æ–≤ —Å—Å—ã–ª–æ–∫.
‚Äî –¢–æ–ª—å–∫–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ–º—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤; –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî '–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö'.
""".strip()

def invest_snapshot(company: str, site_hint: Optional[str] = None,
                    model: str = "sonar", recency: Optional[str] = None,
                    max_tokens: int = 1500) -> str:
    prompt = build_invest_prompt(company, site_hint=site_hint)
    raw = _pplx_call_invest(prompt, model=model, recency=recency, max_tokens=max_tokens)
    return sanitize_invest(raw)

@st.cache_data(ttl=86_400, show_spinner="üìù –°–æ–±–∏—Ä–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏‚Ä¶")
def get_invest_snapshot(company: str,
                        site_hint: Optional[str] = None,
                        model: str = "sonar",
                        recency: Optional[str] = None,
                        max_tokens: int = 1500) -> dict:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict: {'md': markdown_text, 'raw': raw_text_for_debug}
    """
    try:
        md = invest_snapshot(company, site_hint=site_hint, model=model, recency=recency, max_tokens=max_tokens)
        return {"md": md, "raw": md}
    except PPLXError as e:
        return {"md": f"_–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å INVEST SNAPSHOT: {e}_", "raw": ""}
# ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ





# ‚ï≠‚îÄüë• –ò–Ω—Ç–µ—Ä–≤—å—é: –æ–±–æ–≥–∞—â–∞–µ–º INVEST SNAPSHOT –ª–∏—Ü–∞–º–∏ –∏–∑ Checko (Sonar-only) ‚îÄ‚ïÆ
#   ‚Ä¢ –≠—Ç–∞–ø 1 ‚Äî –∏–Ω—Ç–µ—Ä–≤—å—é –ø–æ –§–ò–û –∏–∑ Checko
#   ‚Ä¢ –≠—Ç–∞–ø 2 ‚Äî –µ—Å–ª–∏ –Ω–µ—Ç/–º–∞–ª–æ –§–ò–û ‚Üí discovery –§–ò–û —á–µ—Ä–µ–∑ Sonar
#   ‚Ä¢ –ü–æ–¥–º–µ–Ω–∞ —Å–µ–∫—Ü–∏–∏ "### –ò–Ω—Ç–µ—Ä–≤—å—é (–≤–ª–∞–¥–µ–ª—å—Ü—ã/—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ)" –≤ –≥–æ—Ç–æ–≤–æ–º Markdown
# –¢—Ä–µ–±—É–µ—Ç: _pplx_call_invest, sanitize_invest –∏–∑ –±–ª–æ–∫–∞ INVEST SNAPSHOT
# ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ
import re, html
from typing import Optional

def _clean_person(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s*\(.*?\)\s*$", "", s)            # —É–±–∏—Ä–∞–µ–º —Ö–≤–æ—Å—Ç –≤ —Å–∫–æ–±–∫–∞—Ö: (–ò–ù–ù‚Ä¶, –¥–æ–ª—è‚Ä¶)
    s = re.sub(r"\s{2,}", " ", s)
    return s

def _names_from_checko(company_info: dict | None) -> list[str]:
    if not isinstance(company_info, dict):
        return []
    raw = []
    for key in ("leaders_raw", "founders_raw"):
        v = company_info.get(key) or []
        if isinstance(v, list):
            raw.extend([str(x) for x in v if x])
        elif isinstance(v, str):
            raw.append(v)
    out, seen = [], set()
    for p in raw:
        fio = _clean_person(p)
        k = fio.lower()
        if fio and k not in seen:
            seen.add(k); out.append(fio)
    return out

def _domain_from_site(site_hint: str | None) -> str:
    if not site_hint:
        return ""
    m = re.search(r"^(?:https?://)?([^/]+)", site_hint.strip(), re.I)
    return (m.group(1) if m else "").lower()

def _extract_urls(text: str) -> list[str]:
    return list(dict.fromkeys(re.findall(r'https?://[^\s<>)"\'\]]+', text or "")))

def _build_people_discovery_prompt(company: str,
                                   site_hint: str | None,
                                   market: str | None) -> str:
    dom = _domain_from_site(site_hint)
    mkt = f"(—Ä—ã–Ω–æ–∫: {market}). " if market else ""
    site_line = f"–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç (–µ—Å–ª–∏ –≤–µ—Ä–Ω–æ): {site_hint}. " if site_hint else ""
    return f"""
–ù–∞–π–¥–∏ –¥–µ–π—Å—Ç–≤—É—é—â–∏—Ö —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏/–∏–ª–∏ –æ—Å–Ω–æ–≤–∞—Ç–µ–ª–µ–π –∫–æ–º–ø–∞–Ω–∏–∏ ¬´{company}¬ª. {mkt}{site_line}
–û—Ö–≤–∞—Ç 5 –ª–µ—Ç. –¢–æ–ª—å–∫–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã —Å –ü–†–Ø–ú–´–ú–ò URL.

–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ ‚Äî —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏:
PERSON: <–§–ò–û> ‚Äî <–¥–æ–ª–∂–Ω–æ—Å—Ç—å/—Ä–æ–ª—å> ‚Äî <–ø—Ä—è–º–æ–π URL –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫>

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
‚Äî –ù–µ —É–∫–∞–∑—ã–≤–∞—Ç—å –ò–ù–ù/–û–ì–†–ù, –¥–æ–ª–∏, —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤–ª–∞–¥–µ–Ω–∏—è –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏.
‚Äî –ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç{(' ('+dom+')' if dom else '')}, –°–ú–ò, –ø—Ä–æ—Ñ–∏–ª—å–Ω—ã–µ –º–µ–¥–∏–∞, –≤–∏–¥–µ–æ/–ø–æ–¥–∫–∞—Å—Ç—ã, —Å–æ—Ü—Å–µ—Ç–∏ –∫–æ–º–ø–∞–Ω–∏–∏.
‚Äî –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî –≤—ã–≤–µ–¥–∏ ¬´PERSON: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö¬ª.
""".strip()

def _parse_people_lines(text: str) -> list[str]:
    if not text:
        return []
    ppl = []
    for ln in text.splitlines():
        m = re.match(r"\s*PERSON:\s*(.+?)\s+‚Äî\s+.+?\s+‚Äî\s+https?://", ln.strip(), re.I)
        if m:
            fio = _clean_person(m.group(1))
            if fio:
                ppl.append(fio)
    return list(dict.fromkeys(ppl))

def _build_interviews_prompt(company: str,
                             names: list[str],
                             site_hint: str | None,
                             market: str | None) -> str:
    names_block = "; ".join(names[:10]) or "‚Äî"
    site_line = f"–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç: {site_hint}. " if site_hint else ""
    mkt = f"(—Ä—ã–Ω–æ–∫: {market})" if market else ""
    return f"""
–¢—ã ‚Äî –º–µ–¥–∏–∞-–∞–Ω–∞–ª–∏—Ç–∏–∫. –ù–∞–π–¥–∏ –∏–Ω—Ç–µ—Ä–≤—å—é/–ø—É–±–ª–∏—á–Ω—ã–µ —Ä–∞–∑–≥–æ–≤–æ—Ä—ã –ø–æ –ª—é–¥—è–º [{names_block}] –∏–∑ –∫–æ–º–ø–∞–Ω–∏–∏ ¬´{company}¬ª {mkt}.
{site_line}–û—Ö–≤–∞—Ç 5 –ª–µ—Ç. –¢–æ–ª—å–∫–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ–º—ã–µ —Ñ–∞–∫—Ç—ã –∏ –ü–†–Ø–ú–´–ï URL. –ù–∏–∫–∞–∫–∏—Ö –ò–ù–ù/–û–ì–†–ù/—Ñ–∏–Ω–∞–Ω—Å–æ–≤.

–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ ‚Äî –û–î–ò–ù –∞–±–∑–∞—Ü (–±–µ–∑ —Å–ø–∏—Å–∫–æ–≤):
¬´–§–ò–û ‚Äî –ø–ª–æ—â–∞–¥–∫–∞/–∏–∑–¥–∞–Ω–∏–µ ‚Äî –∫—Ä–∞—Ç–∫–∞—è —Å—É—Ç—å (1 —Ñ—Ä–∞–∑–∞) ‚Äî URL (YYYY-MM-DD)¬ª;
–∑–∞–ø–∏—Å–∏ —Ä–∞–∑–¥–µ–ª—è–π —Ç–æ—á–∫–æ–π —Å –∑–∞–ø—è—Ç–æ–π ¬´;¬ª, –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π —Å—Å—ã–ª–∫–∏. –í—Å–µ–≥–æ 3‚Äì8 –∑–∞–ø–∏—Å–µ–π.
–í –∫–æ–Ω—Ü–µ –∞–±–∑–∞—Ü–∞ —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª –¥–æ–±–∞–≤—å: ¬´–ò—Å—Ç–æ—á–Ω–∏–∫–∏: <URL1>, <URL2>, ...¬ª (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ).
""".strip()

_SEC_INTERV_RE = re.compile(r"(^|\n)###\s*–ò–Ω—Ç–µ—Ä–≤—å—é[^\n]*\n.*?(?=\n###\s|\Z)",
                            flags=re.S | re.I)

def _replace_interviews_section(md: str, new_paragraph: str) -> str:
    block = f"\n### –ò–Ω—Ç–µ—Ä–≤—å—é (–≤–ª–∞–¥–µ–ª—å—Ü—ã/—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ)\n{new_paragraph.strip()}\n"
    if _SEC_INTERV_RE.search(md or ""):
        return _SEC_INTERV_RE.sub(block, md, count=1)
    # –µ—Å–ª–∏ —Ä–∞–∑–¥–µ–ª–∞ –Ω–µ—Ç ‚Äî –≤—Å—Ç–∞–≤–∏–º –ø–µ—Ä–µ–¥ ¬´–¶–∏—Ñ—Ä–æ–≤—ã–µ –∫–∞–Ω–∞–ª—ã¬ª –∏–ª–∏ –≤ –∫–æ–Ω–µ—Ü
    m = re.search(r"(^|\n)###\s*–¶–∏—Ñ—Ä–æ–≤—ã–µ\s+–∫–∞–Ω–∞–ª—ã[^\n]*", md or "", flags=re.I)
    if m:
        return md[:m.start()] + block + md[m.start():]
    return (md or "").rstrip() + block

def interviews_from_checko_sonar(company: str,
                                 company_info: dict | None = None,
                                 site_hint: str | None = None,
                                 market: str | None = None,
                                 max_people_discovery: int = 6) -> tuple[list[str], str]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (names, paragraph_markdown).
    names ‚Äî —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –§–ò–û, paragraph_markdown ‚Äî –æ–¥–∏–Ω –∞–±–∑–∞—Ü —Å –∏–Ω—Ç–µ—Ä–≤—å—é.
    """
    # 1) –∏–º–µ–Ω–∞ –∏–∑ Checko
    names = _names_from_checko(company_info)

    # 2) –µ—Å–ª–∏ –∏–º—ë–Ω –Ω–µ—Ç/–º–∞–ª–æ ‚Äî discovery —á–µ—Ä–µ–∑ Sonar
    if len(names) < 2:
        try:
            p_disc = _build_people_discovery_prompt(company, site_hint, market)
            raw = _pplx_call_invest(p_disc, model="sonar", recency=None, max_tokens=900)
            discovered = _parse_people_lines(raw)
        except Exception:
            discovered = []
        for fio in discovered:
            if fio.lower() not in {n.lower() for n in names}:
                names.append(fio)
        names = names[:max_people_discovery] or ["–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"]

    # 3) –∏–Ω—Ç–µ—Ä–≤—å—é –ø–æ –∏—Ç–æ–≥–æ–≤–æ–º—É —Å–ø–∏—Å–∫—É
    try:
        p_int = _build_interviews_prompt(company, names, site_hint, market)
        digest = _pplx_call_invest(p_int, model="sonar", recency=None, max_tokens=1400)
    except Exception as e:
        digest = f"–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö (–æ—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∏–Ω—Ç–µ—Ä–≤—å—é: {e})"

    # –¥–µ–¥—É–ø URL –≤–Ω—É—Ç—Ä–∏ –∞–±–∑–∞—Ü–∞
    def _urls_in(t: str) -> list[str]: return _extract_urls(t)
    seen = set(); parts = []
    for part in re.split(r"\s*;\s*", (digest or "").strip()):
        u = next(iter(_urls_in(part)), None)
        if not u or u not in seen:
            parts.append(part.strip())
            if u: seen.add(u)
    paragraph = "; ".join(parts)
    paragraph = sanitize_invest(paragraph)   # —Ñ–∏–ª—å—Ç—Ä —Ñ–∏–Ω–∞–Ω—Å–æ–≤/–ò–ù–ù/–û–ì–†–ù

    return names, paragraph

def invest_snapshot_enriched(
    company: str,
    site_hint: Optional[str] = None,
    company_info: dict | None = None,
    market: str | None = None,
    model: str = "sonar",
    recency: Optional[str] = None,
    max_tokens: int = 1500,
) -> str:
    """
    1) –î–µ–ª–∞–µ—Ç –æ–±—ã—á–Ω—ã–π INVEST SNAPSHOT (sonar).
    2) –°–æ–±–∏—Ä–∞–µ—Ç –∏–Ω—Ç–µ—Ä–≤—å—é –≤ –¥–≤–∞ —à–∞–≥–∞:
       ‚Äî —Å–Ω–∞—á–∞–ª–∞ –ø–æ –§–ò–û –∏–∑ Checko,
       ‚Äî –∑–∞—Ç–µ–º –∏—â–µ—Ç –§–ò–û –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –∏ –¥–æ–ø–æ–ª–Ω—è–µ—Ç.
    3) –ü–æ–¥–º–µ–Ω—è–µ—Ç —Å–µ–∫—Ü–∏—é ¬´### –ò–Ω—Ç–µ—Ä–≤—å—é (–≤–ª–∞–¥–µ–ª—å—Ü—ã/—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ)¬ª
       –±–ª–æ–∫–æ–º —Å –§–ò–û –∏ –¥–≤—É–º—è –¥–∞–π–¥–∂–µ—Å—Ç–∞–º–∏.
    """
    # 1) –±–∞–∑–æ–≤—ã–π –æ—Ç—á—ë—Ç
    base_md = invest_snapshot(
        company, site_hint=site_hint, model=model, recency=recency, max_tokens=max_tokens
    )

    # 2) ¬´–¥–≤–æ–π–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤—å—é¬ª: Checko ‚Üí –∏–Ω—Ç–µ—Ä–Ω–µ—Ç
    dual = build_dual_interviews_from_v2(
        company,
        company_info=company_info,
        site_hint=site_hint,
        market=market,
        max_people_inet=8,
    )
    names_checko = ", ".join(dual.get("names_checko") or []) or "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
    names_inet   = ", ".join(dual.get("names_inet")   or []) or "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"

    # 3) —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ–π —Å–µ–∫—Ü–∏–∏ (Markdown, –±–µ–∑ HTML)
    parts: list[str] = []
    parts.append(f"**–§–ò–û (Checko):** {names_checko}")
    parts.append(f"**–§–ò–û (–∏–Ω—Ç–µ—Ä–Ω–µ—Ç):** {names_inet}")

    digest_checko = (dual.get("digest_checko") or "").strip()
    digest_inet   = (dual.get("digest_inet")   or "").strip()

    if digest_checko and digest_checko.lower() != "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö":
        parts.append("")
        parts.append("**–î–∞–π–¥–∂–µ—Å—Ç –∏–Ω—Ç–µ—Ä–≤—å—é ‚Äî Checko**")
        parts.append(digest_checko)

    if digest_inet and digest_inet.lower() != "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö":
        parts.append("")
        parts.append("**–î–∞–π–¥–∂–µ—Å—Ç –∏–Ω—Ç–µ—Ä–≤—å—é ‚Äî –∏–Ω—Ç–µ—Ä–Ω–µ—Ç**")
        parts.append(digest_inet)

    # —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ñ–∏–Ω–ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏/–ò–ù–ù/–û–ì–†–ù –Ω–∞ –≤—Å—è–∫–∏–π
    new_block = "\n".join(parts).strip()
    
    # 4) –ø–æ–¥–º–µ–Ω—è–µ–º/–≤—Å—Ç–∞–≤–ª—è–µ–º —Å–µ–∫—Ü–∏—é ¬´–ò–Ω—Ç–µ—Ä–≤—å—é¬ª
    return _replace_interviews_section(base_md, new_block)


@st.cache_data(ttl=86_400, show_spinner="üìù –°–æ–±–∏—Ä–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ (enriched)‚Ä¶")
def get_invest_snapshot_enriched(
    company: str,
    site_hint: Optional[str] = None,
    company_info: dict | None = None,
    market: str | None = None,
    model: str = "sonar",
    recency: Optional[str] = None,
    max_tokens: int = 1500,
) -> dict:
    try:
        md = invest_snapshot_enriched(
            company,
            site_hint=site_hint,
            company_info=company_info,
            market=market,
            model=model,
            recency=recency,
            max_tokens=max_tokens,
        )
        return {"md": md, "raw": md}
    except PPLXError as e:
        return {"md": f"_–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å INVEST SNAPSHOT (enriched): {e}_", "raw": ""}







class RAG:
    """
    summary    ‚Äì —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç (Google-—Å–Ω–∏–ø–ø–µ—Ç—ã + –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞)
    queries    ‚Äì –∑–∞–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ LLM —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª–∞ –¥–ª—è Google
    snippets   ‚Äì —Å–ø–∏—Å–æ–∫ (url, text) –∏–∑ Google
    news_snippets ‚Äì —Å–Ω–∏–ø–ø–µ—Ç—ã —Å –∫—Ä—É–ø–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å–∞–π—Ç–æ–≤
    site_ctx   ‚Äì –∫–æ—Ä–æ—Ç–∫–∏–π —Å–Ω–∏–ø–ø–µ—Ç ¬´site:<–¥–æ–º–µ–Ω> ‚Ä¶¬ª
    site_pass  ‚Äì –ø–æ–¥—Ä–æ–±–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ (–≥–æ—Ç–æ–≤—ã–π summary –æ—Ç SiteRAG)
    """
    def __init__(self, company: str, *, website: str = "", market: str = "",
                 years=(2022, 2023, 2024), country: str = "–†–æ—Å—Å–∏—è",
                 steps: int = 3, snips: int = 4,
                 llm_model: str = "gpt-4o-mini",company_info: dict | None = None,):
        self.company   = company.strip()
        self.website   = website.strip()
        self.market    = market.strip()
        self.country   = country
        self.years     = years
        self.steps     = steps
        self.snips     = snips
        self.llm_model = llm_model
        self.company_info = company_info or {}

    # ---------- site-snippet –∏–∑ Google ---------------------------------
    async def _site_ctx(self) -> str:
        dom = tldextract.extract(self.website).registered_domain if self.website else ""
        snip = await _site_snippet(dom)
        if snip:
            return f"{snip}\n—Ä—ã–Ω–æ–∫ –∫–æ–º–ø–∞–Ω–∏–∏ ‚Äì {self.market}" if self.market else snip
        return f"—Ä—ã–Ω–æ–∫ –∫–æ–º–ø–∞–Ω–∏–∏ ‚Äì {self.market}" if self.market else ""

    # ---------- GPT ‚Üí –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã --------------------------------
    async def _queries(self, hist="") -> list[str]:
        dom  = tldextract.extract(self.website).registered_domain if self.website else ""
        base = f'"{self.company}"' + (f' OR site:{dom}' if dom else "")
        sys  = (
            "–¢–´ ‚Äî –û–ü–´–¢–ù–´–ô –ò–°–°–õ–ï–î–û–í–ê–¢–ï–õ–¨ –†–´–ù–ö–û–í –ò –î–ê–ù–ù–´–•. –°–§–û–†–ú–£–õ–ò–†–£–ô –ù–ï –ú–ï–ù–ï–ï 30 –ü–†–û–°–¢–´–• –†–ê–ó–ù–û–û–ë–†–ê–ó–ù–´–• GOOGLE-–ó–ê–ü–†–û–°–û–í –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï, "
            f"–ü–û–ó–í–û–õ–Ø–Æ–©–ò–• –°–û–ë–†–ê–¢–¨ –ò–ù–§–û–†–ú–ê–¶–ò–Æ –û –ö–û–ú–ü–ê–ù–ò–ò ¬´{self.company}¬ª –ù–ê –†–´–ù–ö–ï ¬´{self.market}¬ª "
            f"({self.country}, {', '.join(map(str, self.years))}).\n"
            "–ö–ê–ñ–î–´–ô –ó–ê–ü–†–û–° –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –î–û–õ–ñ–ï–ù –°–û–î–ï–†–ñ–ê–¢–¨ –ù–ê–ó–í–ê–ù–ò–ï –ö–û–ú–ü–ê–ù–ò–ò.\n"
            "### –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –ë–õ–û–ö–ò\n"
            "1. –û–ü–ò–°–ê–ù–ò–ï –ö–û–ú–ü–ê–ù–ò–ò –ò –ë–†–ï–ù–î–´.\n"
            "2. –ß–ò–°–õ–ï–ù–ù–û–°–¢–¨ –°–û–¢–†–£–î–ù–ò–ö–û–í.\n"
            "3. –ü–†–û–ò–ó–í–û–î–°–¢–í–ï–ù–ù–´–ï –ú–û–©–ù–û–°–¢–ò.\n"
            "4. –ò–ù–í–ï–°–¢–ò–¶–ò–ò –ò –†–ê–°–®–ò–†–ï–ù–ò–Ø.\n"
            "5. –ê–î–†–ï–°–ê –®–¢–ê–ë-–ö–í–ê–†–¢–ò–†–´ –ò –ü–†–û–ò–ó–í–û–î–°–¢–í.\n"
            "6. –°–û–¶–ò–ê–õ–¨–ù–´–ï –°–ï–¢–ò.\n"
            "7. –ò–°–¢–û–†–ò–Ø.\n"
            "8. –ü–†–ò–ë–´–õ–¨ –ò –û–ë–™–Å–ú–´ –ü–†–û–î–£–ö–¶–ò–ò.\n"
            "9. –ö–û–ù–ö–£–†–ï–ù–¢–´ (–ù–ê–ó–í–ê–ù–ò–ï –ò –°–ê–ô–¢).\n"
            "10. –£–ü–û–ú–ò–ù–ê–ù–ò–Ø –ù–ê –§–û–†–£–ú–ê–• –ò –í –†–ï–ô–¢–ò–ù–ì–ê–•.\n"
            "–ü–û –ö–ê–ñ–î–û–ú–£ –ë–õ–û–ö–£ –°–î–ï–õ–ê–ô –ù–ï–°–ö–û–õ–¨–ö–û –†–ê–ó–ù–´–• –ó–ê–ü–†–û–°–û–í.\n"
            "### –°–û–í–ï–¢–´ –ü–û –ö–û–ù–°–¢–†–£–ö–¶–ò–ò –ó–ê–ü–†–û–°–û–í\n"
            "- –ò–°–ü–û–õ–¨–ó–£–ô –û–ü–ï–†–ê–¢–û–†–´: `site:`, `intitle:`, `inurl:`, `filetype:pdf`, `OR`.\n"
            "- –î–û–ë–ê–í–õ–Ø–ô –ì–û–î–´ –ò –ù–ê–ó–í–ê–ù–ò–Ø –ü–†–û–î–£–ö–¢–û–í –ò –ë–†–ï–ù–î–û–í, –ï–°–õ–ò –ù–£–ñ–ù–û.\n"
            f"- –î–õ–Ø –û–§–ò–¶–ò–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–• –ü–†–ò–ú–ï–ù–Ø–ô `site:{dom}` –ò–õ–ò –°–ê–ô–¢–´ –†–ï–ì–£–õ–Ø–¢–û–†–û–í.\n"
            "### –ü–†–ê–í–ò–õ–ê\n"
            "- –ù–ï –î–£–ë–õ–ò–†–£–ô –ó–ê–ü–†–û–°–´.\n"
            "- –ù–ï –î–û–ë–ê–í–õ–Ø–ô –ö–û–ú–ú–ï–ù–¢–ê–†–ò–ò, –ù–£–ú–ï–†–ê–¶–ò–Æ –ò –≠–ú–û–î–ó–ò.\n"
            "- –í–´–í–û–î–ò –¢–û–õ–¨–ö–û –°–¢–†–û–ö–ò –í –í–ò–î–ï `QUERY: ...`.\n"
            "### CHAIN OF THOUGHTS (–í–ù–£–¢–†–ï–ù–ù–ï, –ù–ï –í–´–í–û–î–ò–¢–¨)\n"
            "1. –ü–û–ù–Ø–¢–¨ –∑–∞–¥–∞—á—É.\n"
            "2. –°–§–û–†–ú–ò–†–û–í–ê–¢–¨ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã.\n"
            "3. –°–ì–ï–ù–ï–†–ò–†–û–í–ê–¢–¨ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã.\n"
            "4. –°–ö–û–ú–ë–ò–ù–ò–†–û–í–ê–¢–¨ –∏—Ö —Å –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞–º–∏.\n"
            "5. –í–´–í–ï–°–¢–ò —Å—Ç—Ä–æ–∫–∏ `QUERY:`.\n"
        )
        raw = await _gpt(
            [{"role": "system", "content": sys},
             {"role": "user",   "content": f'base={base}{hist}'}],
            model=self.llm_model, T=0.1)
        ql = re.findall(r"QUERY:\s*(.+)", raw, flags=re.I)

        if not hist:
            templates = [
                f'"{self.company}" –æ–ø–∏—Å–∞–Ω–∏–µ',
                f'"{self.company}" –±—Ä–µ–Ω–¥—ã',
                f'"{self.company}" —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∏',
                f'"{self.company}" —á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å',
                f'"{self.company}" –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–æ—â–Ω–æ—Å—Ç–∏',
                f'"{self.company}" –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏',
                f'"{self.company}" —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ',
                f'"{self.company}" –∞–¥—Ä–µ—Å',
                f'"{self.company}" –∏—Å—Ç–æ—Ä–∏—è',
                f'"{self.company}" –ø—Ä–∏–±—ã–ª—å',
                f'"{self.company}" –æ–±—ä—ë–º –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞',
                f'"{self.company}" –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã',
                f'"{self.company}" —Ä–µ–π—Ç–∏–Ω–≥',
                f'—Ñ–æ—Ä—É–º "{self.company}"',
                f'site:news.* "{self.company}"',
            ]
            ql = templates + [q for q in ql if q not in templates]

        # ‚îÄ‚îÄ‚îÄ —Ü–µ–ª–µ–≤—ã–µ —Å–æ—Ü—Å–µ—Ç–∏ –∏ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        social_sites = ["vk.com", "facebook.com", "linkedin.com",
                        "youtube.com", "ok.ru"]
        extras = [f'"{self.company}" site:{s}' for s in social_sites]
        if dom:
            extras.append(f'"{self.company}" site:{dom}')

        # dedup —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        ql.extend(extras)
        ql = list(dict.fromkeys(ql))
        return ql

    # ---------- —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç ----------------------------------------
    async def _summary(self, ctx: str) -> str:
        sys = (
            "–¢–´ ‚Äî –í–´–°–û–ö–û–ö–í–ê–õ–ò–§–ò–¶–ò–†–û–í–ê–ù–ù–´–ô –ê–ù–ê–õ–ò–¢–ò–ö –†–´–ù–ö–û–í. –°–û–°–¢–ê–í–¨ –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–´–ô "
            "–ê–ù–ê–õ–ò–¢–ò–ß–ï–°–ö–ò–ô –û–¢–ß–Å–¢ –û –ö–û–ú–ü–ê–ù–ò–ò –ò–ó –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–´–• –ê–ë–ó–ê–¶–ï–í –í –°–õ–ï–î–£–Æ–©–ï–ú "
            "–§–ò–ö–°–ò–†–û–í–ê–ù–ù–û–ú –ü–û–†–Ø–î–ö–ï: "
            "1) –û–ü–ò–°–ê–ù–ò–ï; "
            "2) –ë–†–ï–ù–î–´ (–ø–µ—Ä–µ—á–µ–Ω—å –∏ –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ); "
            "3) –ß–ò–°–õ–ï–ù–ù–û–°–¢–¨ –°–û–¢–†–£–î–ù–ò–ö–û–í; "
            "4) –ü–†–û–ò–ó–í–û–î–°–¢–í–ï–ù–ù–´–ï –ú–û–©–ù–û–°–¢–ò (–ø–ª–æ—â–∞–¥—å, –æ–±—ä—ë–º—ã –ø–æ –≥–æ–¥–∞–º/–¥–Ω—è–º); "
            "5) –ò–ù–í–ï–°–¢–ò–¶–ò–ò –ò –ü–†–û–ï–ö–¢–´ –†–ê–°–®–ò–†–ï–ù–ò–Ø (—Å—É–º–º–∞, –ø–ª–∞–Ω—ã, —Ä—ã–Ω–∫–∏); "
            "6) –ê–î–†–ï–° HQ –ò –ü–†–û–ò–ó–í–û–î–°–¢–í–ï–ù–ù–´–• –ü–õ–û–©–ê–î–û–ö; "
            "7) –°–û–¶–ò–ê–õ–¨–ù–´–ï –°–ï–¢–ò (–í–ö, Facebook, LinkedIn, YouTube, –û–¥–Ω–æ–∫–ª–∞—Å—Å–Ω–∏–∫–∏, —Å–∞–π—Ç); "
            "8) –ò–°–¢–û–†–ò–Ø –ò –ö–õ–Æ–ß–ï–í–´–ï –°–û–ë–´–¢–ò–Ø; "
            "9) –ü–†–ò–ë–´–õ–¨/–û–ë–™–Å–ú–´ –ü–†–û–î–£–ö–¶–ò–ò; "
            "10) –ö–û–ù–ö–£–†–ï–ù–¢–´ (–Ω–∞–∑–≤–∞–Ω–∏—è, —Å–∞–π—Ç—ã, –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ); "
            "11) –£–ß–ê–°–¢–ò–ï –í –§–û–†–£–ú–ê–•/–ù–û–í–û–°–¢–Ø–•/–†–ï–ô–¢–ò–ù–ì–ê–•. "
            "–ü–û–°–õ–ï –ö–ê–ñ–î–û–ì–û –§–ê–ö–¢–ê –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –£–ö–ê–ó–´–í–ê–ô –°–°–´–õ–ö–£-–ò–°–¢–û–ß–ù–ò–ö –í –ö–†–£–ì–õ–´–• –°–ö–û–ë–ö–ê–• (–ü–û–õ–ù–´–ô URL). "
            "–ï–°–õ–ò –î–ê–ù–ù–´–• –ù–ï–¢ ‚Äî –ü–ò–®–ò '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'. "
            "–ù–ï –î–£–ë–õ–ò–†–£–ô –ò–ù–§–û–†–ú–ê–¶–ò–Æ –ò –ù–ï –í–´–î–£–ú–´–í–ê–ô –§–ê–ö–¢–û–í. "
            "–ù–ï –ò–°–ü–û–õ–¨–ó–£–ô MARKDOWN, –ù–ï –£–ö–ê–ó–´–í–ê–ô –í–´–†–£–ß–ö–£ (REVENUE) –ù–ò –í –ö–ê–ö–û–ú –í–ò–î–ï, –ù–û –ú–û–ñ–ù–û –£–ö–ê–ó–´–í–ê–¢–¨ –ü–†–ò–ë–´–õ–¨ –ü–û –ü–†–û–î–£–ö–¢–ê–ú.\n"
        )
        
        return await _gpt(
            [{"role": "system", "content": sys},
             {"role": "user",   "content": ctx[:20_000]}],
            model=self.llm_model, T=0.25)

    def _normalize_sections(self, summary: str) -> str:
        sections = summary.split("\n\n")
        norm = []
        for sec in sections:
            lines = [l.strip() for l in sec.splitlines() if l.strip()]
            seen, uniq = set(), []
            for line in lines:
                if line not in seen:
                    seen.add(line)
                    uniq.append(line)
            norm.append("\n".join(uniq) if uniq else "–Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        return "\n\n".join(norm)

    # ---------- orchestrator -------------------------------------------
    async def _run_async(self):
        # paralell: —Å–Ω–∏–ø–ø–µ—Ç + –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞
        site_ctx_task = asyncio.create_task(self._site_ctx())
        site_pass_task = (
            asyncio.create_task(asyncio.to_thread(_site_passport_sync, self.website))
            if self.website else None
        )
        

        queries, snippets, hist = [], [], ""
        news_snippets: list[tuple[str, str]] = []
        async with aiohttp.ClientSession() as s:
            for _ in range(self.steps):
                ql = await self._queries(hist)
                ql = [f"{q} {self.market}" if self.market and
                      self.market.lower() not in q.lower() else q for q in ql]
                queries += ql
                res = await asyncio.gather(*[_google(s, q, self.snips) for q in ql])
                snippets += sum(res, [])
                hist = f"\n–°–Ω–∏–ø–ø–µ—Ç–æ–≤: {len(snippets)}"

            news_domains = [
                "rbc.ru",
                "kommersant.ru",
                "vedomosti.ru",
                "tass.ru",
                "forbes.ru",
            ]
            news_queries = [f'site:{d} "{self.company}"' for d in news_domains]
            queries += news_queries
            res = await asyncio.gather(*[_google(s, q, self.snips) for q in news_queries])
            news_snippets = sum(res, [])
            snippets += news_snippets

        site_ctx  = await site_ctx_task
        site_pass = await site_pass_task if site_pass_task else ""

        # –≤—ã–¥–µ–ª—è–µ–º —Å–Ω–∏–ø–ø–µ—Ç—ã —Å —Å–æ—Ü—Å–µ—Ç—è–º–∏
        dom = tldextract.extract(self.website).registered_domain if self.website else ""
        social_domains = ["vk.com", "facebook.com", "linkedin.com",
                          "youtube.com", "ok.ru"]
        if dom:
            social_domains.append(dom)
        social_snips = [
            (u, t) for u, t in snippets
            if any(sd in u.lower() or sd in t.lower() for sd in social_domains)
        ]

        # ---------- —Å–æ–±–∏—Ä–∞–µ–º –µ–¥–∏–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è GPT -----------------
        ctx_parts: list[str] = []

        # 1) –∫–æ—Ä–æ—Ç–∫–∏–π —Å–Ω–∏–ø–ø–µ—Ç –∏–∑ Google (¬´site:‚Ä¶¬ª)
        if site_ctx:
            ctx_parts.append(f"SITE_SNIPPET:\n{site_ctx}")

        # 2) –ø–æ–ª–Ω—ã–π –ø–∞—Å–ø–æ—Ä—Ç, —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π SiteRAG
        if site_pass:
            ctx_parts.append(f"SITE_PASSPORT:\n{site_pass}")

        # 3) —Ñ–∞–∫—Ç—ã –∏–∑ checko / fin-API
        company_doc_txt = ""
        if self.company_info:                       # ‚Üê –±—ã–ª –ø–µ—Ä–µ–¥–∞–Ω –≤ __init__
            def _pair(k, v):
                if v in (None, "", []): return ""
                if isinstance(v, list):
                    v = "; ".join(map(str, v[:10]))
                return f"* **{k}:** {v}"
            company_doc_txt = "\n".join(
                p for p in (_pair(k, v) for k, v in self.company_info.items()) if p
            )
            if company_doc_txt:
                ctx_parts.append(f"COMPANY_DOC:\n{company_doc_txt}")

        # 4) —Å–æ—Ü—Å–µ—Ç–∏
        if social_snips:
            ctx_parts.append(
                "SOCIAL_SNIPPETS:\n" +
                "\n".join(f"URL:{u}\nTXT:{t}" for u, t in social_snips)
            )

        # 5) Google-—Å–Ω–∏–ø–ø–µ—Ç—ã
        ctx_parts.append(
            "\n".join(f"URL:{u}\nTXT:{t}" for u, t in snippets)
        )

        # ---------- —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç ----------------------------------
        summary = await self._summary("\n\n".join(ctx_parts))
        summary = self._normalize_sections(summary)

        return {
            "summary":     summary,
            "queries":     queries,
            "snippets":    snippets,
            "news_snippets": news_snippets,
            "site_ctx":    site_ctx,
            "site_pass":   site_pass,
            "company_doc": company_doc_txt   # ‚Üê –Ω–æ–≤—ã–π –∫–ª—é—á (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω –≤–æ —Ñ—Ä–æ–Ω—Ç–µ)
        }


    # ---------- –ø—É–±–ª–∏—á–Ω—ã–π —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å -----------------
    def run(self) -> dict:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å–æ –≤—Å–µ–º–∏ –ø–æ–ª—è–º–∏.
        –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –∫–æ–≥–¥–∞ event-loop —É–∂–µ –∑–∞–ø—É—â–µ–Ω
        (–Ω–∞–ø—Ä–∏–º–µ—Ä, –≤ Jupyter, –≤–Ω—É—Ç—Ä–∏ Streamlit-callback –∏ —Ç.–ø.).
        """
        try:
            loop = asyncio.get_event_loop()
            if loop and loop.is_running():
                # nest_asyncio.patch() —É–∂–µ –≤—ã–∑–≤–∞–Ω –≤—ã—à–µ, –ø–æ—ç—Ç–æ–º—É –º–æ–∂–Ω–æ:
                return loop.run_until_complete(self._run_async())
        except RuntimeError:
            # get_event_loop() –º–æ–∂–µ—Ç –±—Ä–æ—Å–∏—Ç—å, –µ—Å–ª–∏ —Ü–∏–∫–ª–∞ –Ω–µ—Ç ‚Äî —Ç–æ–≥–¥–∞ –ø—Ä–æ—Å—Ç–æ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π
            pass

        return asyncio.run(self._run_async())












# ‚ï≠‚îÄüåê  Market RAG helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
async def google_snippets(query: str, num: int = 4):
    q = re.sub(r'[\"\'‚Äú‚Äù]', '', query)[:80]
    params = {
        "key": KEYS["GOOGLE_API_KEY"],
        "cx":  KEYS["GOOGLE_CX"],
        "q":   q, "num": num, "hl": "ru", "gl": "ru"
    }
    async with aiohttp.ClientSession() as sess:
        async with sess.get("https://www.googleapis.com/customsearch/v1",
                            params=params, headers=HEADERS, timeout=8) as r:
            if r.status != 200:
                logging.warning(f"[Google] {r.status}")
                return []
            js = await r.json()
            return [(it["link"], it.get("snippet", ""))
                    for it in js.get("items", []) if not _bad(it["link"])]

async def gpt_async(messages, T=0.2, model="gpt-4o-mini"):
    chat = await openai.ChatCompletion.acreate(
        model=model, temperature=T, messages=messages)
    return chat.choices[0].message.content.strip()

class FastMarketRAG:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict(summary, queries, snippets) –∑–∞ ~10 —Å."""
    def __init__(self, market, country="–†–æ—Å—Å–∏—è",
                 years=(2021, 2022, 2023, 2024),
                 steps=1, snips=6):
        self.market, self.country = market, country
        self.years, self.steps, self.snips = years, steps, snips



    
    async def _queries(self, hist=""):
        sys = (
            "–¢–´ ‚Äî –û–ü–´–¢–ù–´–ô –ò–°–°–õ–ï–î–û–í–ê–¢–ï–õ–¨ –†–´–ù–ö–û–í –ò –î–ê–ù–ù–´–•. –°–§–û–†–ú–£–õ–ò–†–£–ô 10‚Äì12 –¢–û–ß–ù–´–• –ò –≠–§–§–ï–ö–¢–ò–í–ù–´–• GOOGLE-–ó–ê–ü–†–û–°–û–í, "
            f"–ù–ê–ü–†–ê–í–õ–ï–ù–ù–´–• –ù–ê –°–ë–û–† –°–¢–†–£–ö–¢–£–†–ò–†–û–í–ê–ù–ù–û–ô –ò–ù–§–û–†–ú–ê–¶–ò–ò –û –†–´–ù–ö–ï ¬´{self.market}¬ª –í –°–¢–†–ê–ù–ï {self.country.upper()} –ó–ê –ü–ï–†–ò–û–î {', '.join(map(str, self.years))}. "
            "–ü–û–ò–°–ö–û–í–´–ï –ó–ê–ü–†–û–°–´ –î–û–õ–ñ–ù–´ –û–•–í–ê–¢–´–í–ê–¢–¨ –°–õ–ï–î–£–Æ–©–ò–ï –ê–°–ü–ï–ö–¢–´ –†–´–ù–ö–ê: "
            "1) –û–ë–™–Å–ú –ò –î–ò–ù–ê–ú–ò–ö–ê –†–´–ù–ö–ê, "
            "2) –°–¢–†–£–ö–¢–£–†–ê –ò –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø, "
            "3) –û–°–ù–û–í–ù–´–ï –ò–ì–†–û–ö–ò –ò –ò–• –î–û–õ–ò, "
            "4) –¶–ï–ù–´ –ò –¶–ï–ù–û–í–´–ï –¢–ï–ù–î–ï–ù–¶–ò–ò, "
            "5) –ö–õ–Æ–ß–ï–í–´–ï –¢–†–ï–ù–î–´ –ò –ò–ù–û–í–ê–¶–ò–ò, "
            "6) –†–ï–ì–ò–û–ù–ê–õ–¨–ù–´–ô –†–ê–ó–†–ï–ó, "
            "7) –§–ê–ö–¢–û–†–´ –†–û–°–¢–ê –ò –ë–ê–†–¨–ï–†–´ –í–•–û–î–ê, "
            "8) –°–î–ï–õ–ö–ò, IPO, –°–õ–ò–Ø–ù–ò–Ø, "
            "9) –ê–ù–ê–õ–ò–¢–ò–ß–ï–°–ö–ò–ï –û–¢–ß–Å–¢–´ –ò –î–û–ö–õ–ê–î–´ "
            "–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê: QUERY: <–°–¢–†–û–ö–ê –î–õ–Ø –ü–û–ò–°–ö–ê –í GOOGLE>. "
            "–ù–ï –ü–û–í–¢–û–†–Ø–ô –ó–ê–ü–†–û–°–´. –ù–ï –î–û–ë–ê–í–õ–Ø–ô –õ–ò–®–ù–ò–• –ü–†–ï–î–ò–°–õ–û–í–ò–ô ‚Äî –¢–û–õ–¨–ö–û –°–ü–ò–°–ö–û–ú."
)
        raw = await gpt_async([
            {"role": "system", "content": sys},
            {"role": "user",   "content": hist}
        ], T=0.12)
        return re.findall(r"QUERY:\s*(.+)", raw, flags=re.I)

    async def _run_async(self):
        snippets, hist = [], ""
        for _ in range(self.steps):
            ql = await self._queries(hist)
            tasks = [google_snippets(q, self.snips) for q in ql]
            for res in await asyncio.gather(*tasks):
                snippets.extend(res)
            hist = f"—Å–Ω–∏–ø–ø–µ—Ç–æ–≤={len(snippets)}"

        context = "\n".join(f"URL:{u}\nTXT:{t}" for u, t in snippets)[:18000]
        sys = (
            f"–¢–´ ‚Äî –í–´–°–û–ö–û–ö–õ–ê–°–°–ù–´–ô –ê–ù–ê–õ–ò–¢–ò–ö –†–´–ù–ö–ê ¬´{self.market}¬ª –í –°–¢–†–ê–ù–ï {self.country.upper()}. "
            "–°–§–û–†–ú–ò–†–£–ô –ü–û–ì–û–î–û–í–û–ô –û–ë–ó–û–† –†–´–ù–ö–ê, –ì–î–ï –ö–ê–ñ–î–´–ô –ì–û–î –ü–†–ï–î–°–¢–ê–í–õ–ï–ù –û–¢–î–ï–õ–¨–ù–´–ú –ù–ê–ü–û–õ–ù–ï–ù–ù–´–ú –ê–ë–ó–ê–¶–ï–ú, –í–ö–õ–Æ–ß–ê–Æ–©–ò–ú –°–õ–ï–î–£–Æ–©–ò–ï –≠–õ–ï–ú–ï–ù–¢–´: "
            "1) –û–ë–™–Å–ú –†–´–ù–ö–ê (–µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∞), "
            "2) –¢–ï–ú–ü –†–û–°–¢–ê (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö –∏–ª–∏ –∞–±—Å–æ–ª—é—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö), "
            "3) –°–¢–†–£–ö–¢–£–†–ê –ò –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø (–ø–æ —Ç–∏–ø—É –ø—Ä–æ–¥—É–∫—Ç–∞, –∫–ª–∏–µ–Ω—Ç—É, –∫–∞–Ω–∞–ª—É –∏ –¥—Ä.), "
            "4) –†–ï–ì–ò–û–ù–ê–õ–¨–ù–´–ï –†–ê–ó–†–ï–ó–´ (–∫–ª—é—á–µ–≤—ã–µ —Ä–µ–≥–∏–æ–Ω—ã –≤–Ω—É—Ç—Ä–∏ —Å—Ç—Ä–∞–Ω—ã), "
            "5) –û–°–ù–û–í–ù–´–ï –ò–ì–†–û–ö–ò –ò –ò–• –î–û–õ–ò (—Å –¥–æ–ª—è–º–∏ –≤ %, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã), "
            "6) –ö–†–£–ü–ù–´–ï –°–î–ï–õ–ö–ò –ò –°–û–ë–´–¢–ò–Ø (M&A, IPO, –ø–∞—Ä—Ç–Ω–µ—Ä—Å—Ç–≤–∞), "
            "7) –¶–ï–ù–û–í–û–ô –ê–ù–ê–õ–ò–ó (—É—Ä–æ–≤–Ω–∏ —Ü–µ–Ω, –¥–∏–Ω–∞–º–∏–∫–∞, —Ñ–∞–∫—Ç–æ—Ä—ã –≤–ª–∏—è–Ω–∏—è), "
            "8) –ö–õ–Æ–ß–ï–í–´–ï –¢–†–ï–ù–î–´ (—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, —Å–ø—Ä–æ—Å, —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –¥—Ä.), "
            "9) –ë–ê–†–¨–ï–†–´ –ò –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø (–≤—Ö–æ–¥, –ª–æ–≥–∏—Å—Ç–∏–∫–∞, –Ω–æ—Ä–º–∞—Ç–∏–≤–∫–∞), "
            "10) –í–´–í–û–î–´ –ü–û –ì–û–î–£ (–∫–ª—é—á–µ–≤—ã–µ –∏—Ç–æ–≥–∏ –∏ —Å–¥–≤–∏–≥–∏). "
            "11) –∏—Ç–æ–≥–æ–≤—ã–º –∞–±–∑–∞—Ü–µ–º –≤—ã–≤–µ–¥–∏ –æ–±—ä–µ–º—ã —Ä—ã–Ω–∫–∞ –ø–æ –≥–æ–¥–∞–º –∫–æ—Ç–æ—Ä—ã–µ —Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–ª–∏ –≤ –ø—Ä–æ—à–ª—ã—Ö –∞–±–∑–∞—Ü–∞—Ö"
            "–í–°–ï –§–ê–ö–¢–´ –î–û–õ–ñ–ù–´ –ë–´–¢–¨ –£–ù–ò–ö–ê–õ–¨–ù–´–ú–ò, –ù–ï –ü–û–í–¢–û–†–Ø–¢–¨–°–Ø –ò –ü–û–î–¢–í–ï–†–ñ–î–Å–ù–ù–´ –†–ï–ê–õ–¨–ù–´–ú–ò –°–°–´–õ–ö–ê–ú–ò –ù–ê –ò–°–¢–û–ß–ù–ò–ö–ò –í –ö–†–£–ì–õ–´–• –°–ö–û–ë–ö–ê–• (–§–û–†–ú–ê–¢: –ü–û–õ–ù–´–ô URL). "
            "–ù–ï –ò–°–ü–û–õ–¨–ó–£–ô MARKDOWN, –ù–ï –ü–†–ò–î–£–ú–´–í–ê–ô –§–ê–ö–¢–´ ‚Äî –¢–û–õ–¨–ö–û –î–û–ö–£–ú–ï–ù–¢–ò–†–û–í–ê–ù–ù–´–ï –î–ê–ù–ù–´–ï. "
            "–≤—Å–µ–≥–¥–∞ –æ—Å—Ç–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏"

)
        summary = await gpt_async([
            {"role": "system", "content": sys},
            {"role": "user",   "content": context}
        ], T=0.19)
        return {"summary": summary, "queries": ql, "snippets": snippets}

    def run(self):
        return asyncio.run(self._run_async())

# –∫–µ—à–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö –±—ã–ª–æ –º–≥–Ω–æ–≤–µ–Ω–Ω–æ
@st.cache_data(ttl=86_400, show_spinner="üîé –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç‚Ä¶")
def get_market_rag(market):
    return FastMarketRAG(market).run()


# ‚ï≠‚îÄüßæ  MARKET EVIDENCE (Perplexity) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
#   ‚Ä¢ –ê–±–∑–∞—Ü—ã-–∏—Å—Ç–æ—á–Ω–∏–∫–∏ + —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤–æ–¥–∫–∞ + 2 –±–ª–æ–∫–∞ "–°–¢–†–£–ö–¢–£–†–ê"
#   ‚Ä¢ –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π requests-–≤—ã–∑–æ–≤ Perplexity, –∞–∫–∫—É—Ä–∞—Ç–Ω—ã–π –≤—ã–≤–æ–¥ –≤ Streamlit
# ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ
import os
import re
import json
import requests
import streamlit as st
from typing import Optional, Tuple, Dict

API_URL_PPLX = "https://api.perplexity.ai/chat/completions"


class PPLXError(Exception):
    pass


# –°–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –≤—ã–≤–æ–¥–∏–º (—é—Ä-—Å—É—â–Ω–æ—Å—Ç–∏)
_FORBIDDEN = re.compile(r"(–∞–∫—Ü–∏–æ–Ω–µ—Ä|–≤–ª–∞–¥–µ–ª—å—Ü|–±–µ–Ω–µ—Ñ–∏—Ü–∏–∞—Ä|–∏–Ω–Ω|–æ–≥—Ä–Ω)", re.IGNORECASE)


def _sanitize_evidence(text: str) -> str:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ —á–∏—Å—Ç–∏—Ç –ª–∏—à–Ω–∏–µ –ø—É—Å—Ç—ã–µ."""
    lines, out, blank = [], [], False
    for ln in (text or "").splitlines():
        if _FORBIDDEN.search(ln):
            continue
        lines.append(ln)
    for ln in lines:
        if ln.strip() == "":
            if not blank:
                out.append("")
            blank = True
        else:
            out.append(ln.rstrip())
            blank = False
    return "\n".join(out).strip()


def _get_pplx_key() -> str:
    key = (os.getenv("SONAR_API_KEY") or os.getenv("PPLX_API_KEY") or os.getenv("PERPLEXITY_API_KEY") or "").strip()
    if (not key.startswith("pplx-")) or (len(key) < 40) or key.endswith("..."):
        raise PPLXError("–ü–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä—å Perplexity API key: –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å 'pplx-' –∏ –±—ã—Ç—å –¥–ª–∏–Ω–Ω—ã–º (–æ–±—ã—á–Ω–æ >40 —Å–∏–º–≤–æ–ª–æ–≤).")
    return key


def _call_pplx(
    prompt: str,
    *,
    model: str = "sonar",
    recency: Optional[str] = None,
    temperature: float = 0.0,
    max_tokens: int = 1800,
    timeout: int = 60,
) -> str:
    headers = {
        "Authorization": f"Bearer {_get_pplx_key()}",
        "Content-Type": "application/json",
        "Accept": "application/json",
        "User-Agent": "adv-market-evidence/1.1",
    }
    payload = {
        "model": model,
        "messages": [
            {
                "role": "system",
                "content": (
                    "–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫ —Ä—ã–Ω–∫–æ–≤. –û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–Ω–æ, —Å –ü–†–Ø–ú–´–ú–ò URL. "
                    "–ù–ï —É–ø–æ–º–∏–Ω–∞–π –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤/–∞–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤/–±–µ–Ω–µ—Ñ–∏—Ü–∏–∞—Ä–æ–≤, –ò–ù–ù/–û–ì–†–ù."
                ),
            },
            {"role": "user", "content": prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    if recency in {"hour", "day", "week", "month", "year"}:
        payload["search_recency_filter"] = recency

    r = requests.post(API_URL_PPLX, headers=headers, json=payload, timeout=timeout)
    if r.status_code != 200:
        try:
            err = r.json()
        except Exception:
            err = {"error": r.text[:800]}
        raise PPLXError(f"HTTP {r.status_code}: {json.dumps(err, ensure_ascii=False)[:800]}")
    data = r.json()
    return data["choices"][0]["message"]["content"]


def build_market_evidence_prompt(
    market: str,
    country: str = "–†–æ—Å—Å–∏—è",
    years_force: tuple = (2021, 2022, 2023, 2024),
    min_sources: int = 6,
) -> str:
    years_txt = ", ".join(str(y) for y in years_force)
    # –î–í–ê –Ø–í–ù–´–• –ó–ê–í–ï–†–®–ê–Æ–©–ò–• FENCED-–ë–õ–û–ö–ê: ```text ... ```
    return f"""
–°–æ–±–µ—Ä–∏ "evidence" –ø–æ —Ä—ã–Ω–∫—É ¬´{market}¬ª (—Å—Ç—Ä–∞–Ω–∞: {country}) –∏–∑ —Ä–∞–∑–Ω—ã—Ö –û–¢–ö–†–´–¢–´–• –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê ‚Äî –°–¢–†–û–ì–û –¢–ï–ö–°–¢ –° –ê–ë–ó–ê–¶–ê–ú–ò (–ë–ï–ó –°–ü–ò–°–ö–û–í/–¢–ê–ë–õ–ò–¶/CSV):
‚Äî –ö–∞–∂–¥—ã–π –ù–û–í–´–ô –ê–ë–ó–ê–¶ –ø–æ—Å–≤—è—â—ë–Ω –û–î–ù–û–ú–£ —Ä–µ—Å—É—Ä—Å—É (–∏—Å—Ç–æ—á–Ω–∏–∫—É): –Ω–∞—á–∏–Ω–∞–π —Ç–∞–∫ ‚Äî ¬´–ò—Å—Ç–æ—á–Ω–∏–∫: <–∏–∑–¥–∞—Ç–µ–ª—å/–Ω–∞–∑–≤–∞–Ω–∏–µ>, <–≥–æ–¥/–¥–∞—Ç–∞>, URL: <–ø—Ä—è–º–æ–π_–ª–∏–Ω–∫>.¬ª
‚Äî –í–Ω—É—Ç—Ä–∏ –∞–±–∑–∞—Ü–∞ –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–µ—Ä—ã. –ü–∏—à–∏ –∫–æ–º–ø–∞–∫—Ç–Ω–æ, –Ω–æ –≤–∫–ª—é—á–∞–π –í–°–ï –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ß–ò–°–õ–ê –ø–æ —Ä—ã–Ω–∫—É —Å –µ–¥–∏–Ω–∏—Ü–∞–º–∏:
   ‚Ä¢ –≥–æ–¥–æ–≤—ã–µ –æ–±—ä—ë–º—ã —Ä—ã–Ω–∫–∞ –≤ –¥–µ–Ω—å–≥–∞—Ö (–ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ ‚ÇΩ; –µ—Å–ª–∏ —Ç–æ–ª—å–∫–æ $, –≤—Å—ë —Ä–∞–≤–Ω–æ –≤–∫–ª—é—á–∞–π –∏ –ø–æ–º–µ—á–∞–π –∫–∞–∫ $);
   ‚Ä¢ –≥–æ–¥–æ–≤—ã–µ NATURAL-–æ–±—ä—ë–º—ã (—à—Ç., –º¬≤, —Ç, –ø–æ—Å–µ—â–µ–Ω–∏—è –∏ —Ç.–ø.), –µ—Å–ª–∏ –µ—Å—Ç—å;
   ‚Ä¢ –µ—Å–ª–∏ –µ—Å—Ç—å —Å–µ–≥–º–µ–Ω—Ç—ã/—Ä–µ–≥–∏–æ–Ω—ã ‚Äî –∫—Ä–∞—Ç–∫–æ –¥–æ–±–∞–≤—å –∫–ª—é—á–µ–≤—ã–µ —Ü–∏—Ñ—Ä—ã.
‚Äî –ü–æ –ì–û–î–ê–ú {years_txt} —Å—Ç–∞—Ä–∞–π—Å—è –¥–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è, –µ—Å–ª–∏ —É –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –æ–Ω–∏ –µ—Å—Ç—å; –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî —è–≤–Ω–æ –Ω–∞–ø–∏—à–∏ ¬´–∑–∞ {years_txt} —É –∏—Å—Ç–æ—á–Ω–∏–∫–∞: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö¬ª.
‚Äî –£ –ö–ê–ñ–î–û–ì–û —Ñ–∞–∫—Ç–∞ ‚Äî –ü–†–Ø–ú–û–ô URL –≤ —ç—Ç–æ–º –∂–µ –∞–±–∑–∞—Ü–µ.
‚Äî –ú–∏–Ω–∏–º—É–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {min_sources}. –†–∞–∑–Ω—ã–µ –¥–æ–º–µ–Ω—ã/–∏–∑–¥–∞—Ç–µ–ª–∏ (–Ω–æ–≤–æ—Å—Ç–∏/–∞–Ω–∞–ª–∏—Ç–∏–∫–∞/–æ—Ç—á—ë—Ç—ã/–≥–æ—Å—Å—Ç–∞—Ç/–ø—Ä–æ—Ñ–∏–ª—å–Ω—ã–µ –º–µ–¥–∏–∞).

–ó–∞–∫–ª—é—á–∏—Ç–µ–ª—å–Ω—ã–π –∞–±–∑–∞—Ü (–ø–æ—Å–ª–µ–¥–Ω–∏–π):
‚Äî –°—Ñ–æ—Ä–º–∏—Ä—É–π –æ–±—â—É—é –∫–∞—Ä—Ç–∏–Ω—É –ø–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –†–Ø–î–ê–ú –ò –ü–†–û–ì–ù–û–ó–ê–ú: –ø–µ—Ä–µ—á–∏—Å–ª–∏, –∫–∞–∫–∏–µ —Å–µ—Ä–∏–∏ –≥–æ–¥–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –º—ã –ø–æ–ª—É—á–∏–ª–∏ (–∫—Ç–æ –∏–∑–¥–∞—Ç–µ–ª—å/–≤–∞–ª—é—Ç–∞/–ø–µ—Ä–∏–º–µ—Ç—Ä), –≤—ã—Å—Ç—Ä–æ–π –•–†–û–ù–û–õ–û–ì–ò–Æ 2021‚Üí2024 –∏ –ø—Ä–æ–≥–Ω–æ–∑—ã (—Å –µ–¥–∏–Ω–∏—Ü–∞–º–∏ –∏ —Å—Å—ã–ª–∫–∞–º–∏), –æ—Ç–º–µ—Ç—å —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è (baseline vs –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã) –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è.

–°–¢–†–£–ö–¢–£–†–ê (—Å–≤–æ–¥–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞; –î–û–ë–ê–í–¨ –ü–û–°–õ–ï –∑–∞–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–±–∑–∞—Ü–∞) ‚Äî –î–í–ê fenced-–±–ª–æ–∫–∞:
1) –î–µ–Ω—å–≥–∏:
```text
–ò—Å—Ç–æ—á–Ω–∏–∫ | –ü–µ—Ä–∏–æ–¥ 1 | –ü–µ—Ä–∏–æ–¥ 2 | –ü–µ—Ä–∏–æ–¥ 3 | ...
<–∫—Ä–∞—Ç–∫–æ–µ_–Ω–∞–∑–≤–∞–Ω–∏–µ_–∏—Å—Ç–æ—á–Ω–∏–∫–∞> | <YYYY>: <—á–∏—Å–ª–æ> <–≤–∞–ª—é—Ç–∞/–º–∞—Å—à—Ç–∞–±> | <YYYY>: <—á–∏—Å–ª–æ> <–≤–∞–ª—é—Ç–∞/–º–∞—Å—à—Ç–∞–±> | ...
2) –ù–∞—Ç—É—Ä–∞–ª—å–Ω—ã–µ:
–ò—Å—Ç–æ—á–Ω–∏–∫ | –ü–µ—Ä–∏–æ–¥ 1 | –ü–µ—Ä–∏–æ–¥ 2 | –ü–µ—Ä–∏–æ–¥ 3 | ...
<–∫—Ä–∞—Ç–∫–æ–µ_–Ω–∞–∑–≤–∞–Ω–∏–µ_–∏—Å—Ç–æ—á–Ω–∏–∫–∞> | <YYYY>: <—á–∏—Å–ª–æ> <–µ–¥.–∏–∑–º.> | <YYYY>: <—á–∏—Å–ª–æ> <–µ–¥.–∏–∑–º.> | ...
[–µ—Å–ª–∏ —É –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –Ω–µ—Ç –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫ ‚Äî —É–∫–∞–∂–∏ ¬´–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö¬ª –æ–¥–Ω–æ–π —è—á–µ–π–∫–æ–π]
–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Å—Ç–∏–ª—é:
‚Äî –¢–æ–ª—å–∫–æ –∞–±–∑–∞—Ü—ã –∏ –¥–≤–∞ –∑–∞–≤–µ—Ä—à–∞—é—â–∏—Ö fenced-–±–ª–æ–∫–∞ ```text, –±–µ–∑ —Å–ø–∏—Å–∫–æ–≤/–Ω—É–º–µ—Ä–∞—Ü–∏–∏/—Ç–∞–±–ª–∏—Ü/CSV.
‚Äî –ï–¥–∏–Ω–∏—Ü—ã –∏ –≤–∞–ª—é—Ç–∞ –≤—Å–µ–≥–¥–∞ —Ä—è–¥–æ–º —Å —á–∏—Å–ª–æ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, ¬´–º–ª—Ä–¥ ‚ÇΩ¬ª, ¬´$ –º–ª–Ω¬ª, ¬´—Ç—ã—Å. –ø–æ—Å–µ—â–µ–Ω–∏–π¬ª, ¬´–º¬≤¬ª).
‚Äî –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π —á–∏—Å–ª–∞ ‚Äî —Ç–æ–ª—å–∫–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ–º—ã–µ —Ñ–∞–∫—Ç—ã —Å –ü–†–Ø–ú–´–ú–ò URL.
‚Äî –ë–µ–∑ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤/–∞–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤/–ò–ù–ù/–û–ì–†–ù.
""".strip()

def market_evidence_report(
    market: str,
    country: str = "–†–æ—Å—Å–∏—è",
    min_sources: int = 6,
    model: str = "sonar",
    recency: Optional[str] = None,
    max_tokens: int = 1800,
) -> str:
    assert isinstance(min_sources, int) and 3 <= min_sources <= 15, "min_sources ‚àà [3, 15]"
    prompt = build_market_evidence_prompt(market, country=country, min_sources=min_sources)
    raw = _call_pplx(prompt, model=model, recency=recency, max_tokens=max_tokens)
    return _sanitize_evidence(raw)


def _split_evidence_blocks(raw_text: str) -> Tuple[str, str, str]:
    if not raw_text:
        return "", "", ""
    blocks = [m.group(1) for m in re.finditer(r"```text\s*(.*?)\s*```", raw_text, flags=re.S|re.I)]
    money_block   = blocks[0].strip() if len(blocks) > 0 else ""
    natural_block = blocks[1].strip() if len(blocks) > 1 else ""
    plain = re.sub(r"```text\s*.*?\s*```", "", raw_text, flags=re.S|re.I).strip()
    return plain, money_block, natural_block


def _linkify(text: str) -> str:
    """
    –ó–∞–º–µ–Ω—è–µ—Ç http/https —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–µ <a>.
    –ù–µ —Ç—Ä–æ–≥–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω–æ–π HTML.
    """
    url_re = re.compile(r"(https?://[^\s<>)\"']+)")
    return url_re.sub(r'<a href="\1" target="_blank" rel="noopener noreferrer">\1</a>', text)


@st.cache_data(ttl=86_400, show_spinner="üîé –°–æ–±–∏—Ä–∞–µ–º —Ä—ã–Ω–æ—á–Ω—ã–µ EVIDENCE‚Ä¶")
def get_market_evidence(
    market: str,
    country: str = "–†–æ—Å—Å–∏—è",
    min_sources: int = 6,
    model: str = "sonar",
    recency: Optional[str] = None,
    max_tokens: int = 1800,
) -> Dict[str, str]:
    """
    Streamlit-–∫—ç—à: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å:
    ‚Ä¢ text_html ‚Äî –≤–µ—Å—å —Ç–µ–∫—Å—Ç (–∞–±–∑–∞—Ü—ã-–∏—Å—Ç–æ—á–Ω–∏–∫–∏ + —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –∞–±–∑–∞—Ü), —Å—Å—ã–ª–∫–∏ –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã
    ‚Ä¢ money_block / natural_block ‚Äî —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–≤—É—Ö –º–∞—Ç—Ä–∏—Ü –¥–ª—è st.code(..., language="text")
    ‚Ä¢ raw_text ‚Äî –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
    """
    try:
        raw = market_evidence_report(
            market=market,
            country=country,
            min_sources=min_sources,
            model=model,
            recency=recency,
            max_tokens=max_tokens,
        )
    except PPLXError as e:
        return {
            "text_html": f"<i>–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å MARKET EVIDENCE: {str(e)}</i>",
            "money_block": "",
            "natural_block": "",
            "raw_text": "",
        }

    plain, money, natural = _split_evidence_blocks(raw)
    text_html = linkify_keep_url(plain).replace("\n", "<br>")

    return {"text_html": text_html, "money_block": money, "natural_block": natural, "raw_text": raw}






# === Leaders & Interviews (2-pass + union, Sonar-only, no cache) ==============
# –ò—Å–ø–æ–ª—å–∑—É–µ—Ç: re, html, typing, _pplx_call_invest
import re, html
from typing import Optional, List, Dict, Tuple

_URL_RE = re.compile(r'https?://[^\s<>)"\'\]]+')

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –£—Ç–∏–ª–∏—Ç—ã
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _norm(s: Optional[str]) -> str:
    import re
    return re.sub(r"\s{2,}", " ", (s or "").strip())

def _extract_urls(text: str) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ URL –≤ –ø–æ—Ä—è–¥–∫–µ –ø–æ—è–≤–ª–µ–Ω–∏—è."""
    return list(dict.fromkeys(_URL_RE.findall(text or "")))

def _dedup_urls_in_paragraph(paragraph: str) -> str:
    """
    –†–µ–∂–µ–º –ø–æ ¬´;¬ª, –æ—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—É—é –∑–∞–ø–∏—Å—å —Å –∫–∞–∂–¥—ã–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–º URL.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∫–ª–µ–µ–Ω–Ω—ã–π –∞–±–∑–∞—Ü —Ç–µ–º –∂–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º.
    """
    seen, out = set(), []
    for part in re.split(r"\s*;\s*", (paragraph or "").strip()):
        if not part:
            continue
        u = next(iter(_extract_urls(part)), None)
        if (not u) or (u not in seen):
            out.append(part.strip())
            if u:
                seen.add(u)
    return "; ".join(out)

def _clean_person(s: str) -> str:
    """–£–±–∏—Ä–∞–µ—Ç —Ö–≤–æ—Å—Ç—ã –≤ —Å–∫–æ–±–∫–∞—Ö –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã: '–ò–≤–∞–Ω–æ–≤ (–ò–ù–ù..., –¥–æ–ª—è...)' ‚Üí '–ò–≤–∞–Ω–æ–≤'."""
    s = (s or "").strip()
    s = re.sub(r"\s*\(.*?\)\s*$", "", s)
    s = re.sub(r"\s{2,}", " ", s)
    return s

def _domain_from_site(site_hint: Optional[str]) -> str:
    if not site_hint:
        return ""
    m = re.search(r"^(?:https?://)?([^/]+)", site_hint.strip(), re.I)
    return (m.group(1) if m else "").lower()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –î–æ–ª–∏ / –ø—Ä–æ—Ü–µ–Ω—Ç—ã (—Å—Ç—Ä–æ–≥–∞—è —Ç—Ä–∞–∫—Ç–æ–≤–∫–∞ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _to_float_safe(x) -> Optional[float]:
    """
    –û—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (–ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –¥–æ–ª–µ–π).
    –ü—Ä–∏–≤–æ–¥–∏—Ç —Å—Ç—Ä–æ–∫—É/—á–∏—Å–ª–æ –∫ float –±–µ–∑ –ª–æ–≥–∏–∫–∏ *100.
    """
    try:
        if x is None:
            return None
        return float(str(x).replace(",", "."))
    except Exception:
        return None

def _to_percent_strict(x) -> Optional[float]:
    """
    '25', '25%', '12,5' -> 25.0
    0.25 -> 0.25 (—ç—Ç–æ 0.25%, –ù–ò–ß–ï–ì–û –Ω–µ –¥–æ–º–Ω–æ–∂–∞–µ–º).
    """
    try:
        if x is None:
            return None
        s = str(x).strip()
        if s.endswith("%"):
            s = s[:-1]
        s = s.replace(",", ".")
        return float(s)
    except Exception:
        return None

def _share_from_checko_dict(item: dict) -> Optional[float]:
    """
    –°—Ç—Ä–æ–≥–æ –±–µ—Ä—ë–º item['–î–æ–ª—è']['–ü—Ä–æ—Ü–µ–Ω—Ç'] –∏–ª–∏ –ø–ª–æ—Å–∫—É—é '–î–æ–ª—è' –∫–∞–∫ –ø—Ä–æ—Ü–µ–Ω—Ç—ã.
    –ù–∏–∫–∞–∫–∏—Ö —ç–≤—Ä–∏—Å—Ç–∏–∫.
    """
    d = item.get("–î–æ–ª—è")
    if isinstance(d, dict) and ("–ü—Ä–æ—Ü–µ–Ω—Ç" in d):
        return _to_percent_strict(d.get("–ü—Ä–æ—Ü–µ–Ω—Ç"))
    if "–î–æ–ª—è" in item and not isinstance(d, dict):
        return _to_percent_strict(item.get("–î–æ–ª—è"))
    return None

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –†–∞–∑–±–æ—Ä Checko-—è—á–µ–π–∫–∏ –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏ –ø–µ—Ä—Å–æ–Ω
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _parse_checko_cell(cell, role_hint: Optional[str] = None) -> List[Dict]:
    """
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç leaders_raw / founders_raw –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π:
      {'fio','inn','share_pct','role'}
    –õ–û–ì–ò–ö–ê –î–û–õ–ï–ô ‚Äî —Å—Ç—Ä–æ–≥–æ –∫–∞–∫ –≤ Checko: '–î–æ–ª—è.–ü—Ä–æ—Ü–µ–Ω—Ç' (–∏–ª–∏ –ø–ª–æ—Å–∫–∞—è '–î–æ–ª—è').
    """
    import ast
    out: List[Dict] = []

    def _emit(fio=None, inn=None, share=None, role=None):
        item = {
            "fio": _norm(fio),
            "inn": _norm(str(inn)) if inn else None,
            "share_pct": _to_percent_strict(share),
            "role": _norm(role or role_hint),
        }
        if item["fio"] or item["inn"]:
            out.append(item)

    if cell is None:
        return out

    # —Å—Ç—Ä–æ–∫–∞ ‚Üí –ø—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ –ª–∏—Ç–µ—Ä–∞–ª ([{‚Ä¶}] / {...}), –∏–Ω–∞—á–µ –≤—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º —Ö–≤–æ—Å—Ç—ã
    if isinstance(cell, str):
        s = cell.strip()
        if not s:
            return out
        if s.startswith("[") or s.startswith("{"):
            try:
                parsed = ast.literal_eval(s)
                return _parse_checko_cell(parsed, role_hint=role_hint)
            except Exception:
                pass
        m_inn = re.search(r"(?:–ò–ù–ù|inn)\s*[:‚Ññ]?\s*([0-9]{8,12})", s, re.I)
        inn = m_inn.group(1) if m_inn else None
        m_share = re.search(r"(?:–¥–æ–ª—è|share)[^0-9]*([0-9]+[.,]?[0-9]*)\s*%?", s, re.I)
        share = m_share.group(1) if m_share else None
        fio = re.sub(r"\s*\(.*?\)\s*$", "", s).strip()
        _emit(fio=fio, inn=inn, share=share)
        return out

    # dict ‚Üí —á–∏—Ç–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä –∫–ª—é—á–µ–π
    if isinstance(cell, dict):
        fio   = cell.get("–§–ò–û") or cell.get("fio") or cell.get("name")
        inn   = cell.get("–ò–ù–ù") or cell.get("inn") or cell.get("tax_id")
        share = _share_from_checko_dict(cell)
        role  = cell.get("–î–æ–ª–∂–Ω–æ—Å—Ç—å") or cell.get("role") or role_hint
        _emit(fio=fio, inn=inn, share=share, role=role)
        return out

    # list ‚Üí —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ
    if isinstance(cell, list):
        for it in cell:
            out.extend(_parse_checko_cell(it, role_hint=role_hint))
        return out

    # fallback
    _emit(fio=str(cell))
    return out

def _pick_ceo(leaders: List[Dict], names_fallback: Optional[List[str]] = None) -> Optional[Dict]:
    """
    –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –≥–µ–Ω–¥–∏—Ä–µ–∫—Ç–æ—Ä–∞:
      1) –ø–æ '–≥–µ–Ω–µ—Ä–∞–ª—å';
      2) –ø–æ '–¥–∏—Ä–µ–∫—Ç–æ—Ä'/'—Ä—É–∫–æ–≤–æ–¥'/'ceo';
      3) –∏–Ω–∞—á–µ –ø–µ—Ä–≤—ã–π –∏–∑ leaders;
      4) –∏–Ω–∞—á–µ —Ñ–æ–ª–±—ç–∫ –ø–æ –ø–µ—Ä–≤–æ–º—É –∏–º–µ–Ω–∏.
    """
    for p in leaders:
        r = (p.get("role") or "").lower()
        if "–≥–µ–Ω–µ—Ä–∞–ª—å" in r or "–≥–µ–Ω. –¥–∏—Ä" in r or "–≥–µ–Ω–¥–∏—Ä" in r or "general director" in r:
            return p
    for p in leaders:
        r = (p.get("role") or "").lower()
        if any(k in r for k in ("–¥–∏—Ä–µ–∫—Ç–æ—Ä", "—Ä—É–∫–æ–≤–æ–¥", "ceo")):
            return p
    if leaders:
        return leaders[0]
    if names_fallback:
        return {"fio": names_fallback[0], "inn": None, "share_pct": None, "role": "—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å"}
    return None

def _shareholders_from_founders(founders: List[Dict]) -> List[Dict]:
    """
    –ì–æ—Ç–æ–≤–∏–º —Ç–∞–±–ª–∏—Ü—É –∞–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤:
      ‚Äî –ø—Ä–∏–≤–æ–¥–∏–º –¥–æ–ª–∏ —Å—Ç—Ä–æ–≥–æ (–±–µ–∑ *100),
      ‚Äî auto-scale: –µ—Å–ª–∏ –≤—Å–µ ‚â§1 –∏ —Å—É–º–º–∞ ‚â§1.5 ‚Äî —Å—á–∏—Ç–∞–µ–º –¥–æ–ª–∏ –æ—Ç 1 ‚Üí *100,
      ‚Äî —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é.
    """
    rows_raw = []
    for p in founders:
        fio = (p.get("fio") or "").strip()
        inn = p.get("inn")
        share_raw = _to_percent_strict(p.get("share_pct"))
        rows_raw.append({"fio": fio, "inn": inn, "share_pct": share_raw})

    vals = [r["share_pct"] for r in rows_raw if r["share_pct"] is not None]
    scale = 1.0
    if vals:
        max_v = max(vals)
        ssum = sum(vals)
        if 0 < max_v <= 1.0 and 0 < ssum <= 1.5:
            scale = 100.0

    rows = []
    for r in rows_raw:
        v = r["share_pct"]
        rows.append({
            "fio": r["fio"],
            "inn": r["inn"],
            "share_pct": (v * scale) if v is not None else None
        })

    with_share = [r for r in rows if r["share_pct"] is not None]
    no_share   = [r for r in rows if r["share_pct"] is None]
    with_share.sort(key=lambda x: x["share_pct"], reverse=True)
    return with_share + no_share

def _markdown_shareholders_table(rows: List[Dict]) -> str:
    if not rows:
        return "_–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö_"
    lines = ["| –§–ò–û | –ò–ù–ù | –î–æ–ª—è, % |", "|---|---|---|"]
    for r in rows:
        fio  = r.get("fio") or ""
        inn  = r.get("inn") or ""
        val  = r.get("share_pct")
        share = "" if val is None else f"{float(val):.2f}"
        lines.append(f"| {fio} | {inn} | {share} |")
    return "\n".join(lines)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ò–º–µ–Ω–∞ –∏–∑ Checko (–±–æ–≥–∞—Ç–∞—è –≤–µ—Ä—Å–∏—è ‚Üí leaders/founders + dedup –§–ò–û)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _names_from_checko_rich(company_info: Optional[Dict]) -> Tuple[List[Dict], List[Dict], List[str]]:
    leaders, founders = [], []
    if isinstance(company_info, dict):
        leaders = _parse_checko_cell(company_info.get("leaders_raw"), role_hint="—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å")
        founders = _parse_checko_cell(company_info.get("founders_raw"), role_hint="–∞–∫—Ü–∏–æ–Ω–µ—Ä/—É—á—Ä–µ–¥–∏—Ç–µ–ª—å")
    names, seen = [], set()
    for p in leaders + founders:
        fio = (p.get("fio") or "").strip()
        if fio:
            k = fio.lower()
            if k not in seen:
                seen.add(k); names.append(fio)
    return leaders, founders, names

def _names_from_checko(company_info: Optional[Dict]) -> List[str]:
    """
    –õ—ë–≥–∫–∏–π —Ä–µ–∂–∏–º: –¥–æ—Å—Ç–∞—ë–º —Å—Ç—Ä–æ–∫–∏ –∏–∑ leaders_raw/founders_raw –∏ —á–∏—Å—Ç–∏–º ¬´(–ò–ù–ù‚Ä¶, –¥–æ–ª—è ‚Ä¶)¬ª.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–∞–∫ –±—ç–∫–∞–ø.
    """
    if not isinstance(company_info, dict):
        return []
    raw: List[str] = []
    for k in ("leaders_raw", "founders_raw"):
        v = company_info.get(k) or []
        if isinstance(v, list):
            raw.extend([_clean_person(str(x)) for x in v if x])
        elif isinstance(v, str):
            raw.append(_clean_person(v))
    out, seen = [], set()
    for fio in raw:
        key = fio.lower()
        if fio and key not in seen:
            seen.add(key); out.append(fio)
    return out

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ü—Ä–æ–º–ø—Ç—ã/–ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è –¥–∏—Å–∫–∞–≤–µ—Ä–∏ –ø–µ—Ä—Å–æ–Ω –∏ –∏–Ω—Ç–µ—Ä–≤—å—é
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _build_people_discovery_prompt(company: str,
                                   site_hint: Optional[str],
                                   market: Optional[str]) -> str:
    dom = _domain_from_site(site_hint)
    mkt = f"(—Ä—ã–Ω–æ–∫: {market}). " if market else ""
    site_line = f"–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç (–µ—Å–ª–∏ –≤–µ—Ä–Ω–æ): {site_hint}. " if site_hint else ""
    pref = f"‚Äî –ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç{(' ('+dom+')' if dom else '')}, –°–ú–ò, –ø—Ä–æ—Ñ–∏–ª—å–Ω—ã–µ –º–µ–¥–∏–∞, –≤–∏–¥–µ–æ/–ø–æ–¥–∫–∞—Å—Ç—ã, —Å–æ—Ü—Å–µ—Ç–∏ –∫–æ–º–ø–∞–Ω–∏–∏."
    return f"""
–ù–∞–π–¥–∏ –¥–µ–π—Å—Ç–≤—É—é—â–∏—Ö —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π –∏/–∏–ª–∏ –æ—Å–Ω–æ–≤–∞—Ç–µ–ª–µ–π –∫–æ–º–ø–∞–Ω–∏–∏ ¬´{company}¬ª. {mkt}{site_line}
–û—Ö–≤–∞—Ç 5 –ª–µ—Ç. –¢–æ–ª—å–∫–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ —Ñ–∞–∫—Ç—ã —Å –ü–†–Ø–ú–´–ú–ò URL.

–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ ‚Äî —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏:
PERSON: <–§–ò–û> ‚Äî <–¥–æ–ª–∂–Ω–æ—Å—Ç—å/—Ä–æ–ª—å> ‚Äî <–ø—Ä—è–º–æ–π URL –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫>

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
‚Äî –ù–µ —É–∫–∞–∑—ã–≤–∞—Ç—å –ò–ù–ù/–û–ì–†–ù, –¥–æ–ª–∏, —Å—Ç—Ä—É–∫—Ç—É—Ä—É –≤–ª–∞–¥–µ–Ω–∏—è –∏ —Ñ–∏–Ω–ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏.
{pref}
‚Äî –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî –≤—ã–≤–µ–¥–∏ ¬´PERSON: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö¬ª.
""".strip()

# –¢–µ—Ä–ø–∏–º—ã–π –∫ —Ä–∞–∑–Ω—ã–º —Ç–∏—Ä–µ/–¥–µ—Ñ–∏—Å–∞–º –ø–∞—Ä—Å–µ—Ä
_PERSON_LINE_RE = re.compile(r"\s*PERSON:\s*(.+?)\s+[‚Äî‚Äì-]\s+.+?\s+[‚Äî‚Äì-]\s+https?://", re.I)

def _parse_people_lines(text: str) -> List[str]:
    if not text:
        return []
    ppl: List[str] = []
    for ln in text.splitlines():
        if "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö" in ln.lower():
            continue
        m = _PERSON_LINE_RE.match(ln.strip())
        if m:
            fio = _clean_person(m.group(1))
            if fio:
                ppl.append(fio)
    return list(dict.fromkeys(ppl))

def _build_interviews_prompt(company: str,
                             names: List[str],
                             site_hint: Optional[str],
                             market: Optional[str]) -> str:
    names_block = "; ".join(names[:10]) or "‚Äî"
    site_line = f"–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç: {site_hint}. " if site_hint else ""
    mkt = f"(—Ä—ã–Ω–æ–∫: {market})" if market else ""
    return f"""
–¢—ã ‚Äî –º–µ–¥–∏–∞-–∞–Ω–∞–ª–∏—Ç–∏–∫. –ù–∞–π–¥–∏ –∏–Ω—Ç–µ—Ä–≤—å—é/–ø—É–±–ª–∏—á–Ω—ã–µ —Ä–∞–∑–≥–æ–≤–æ—Ä—ã –ø–æ –ª—é–¥—è–º [{names_block}] –∏–∑ –∫–æ–º–ø–∞–Ω–∏–∏ ¬´{company}¬ª {mkt}.
{site_line}–û—Ö–≤–∞—Ç 5 –ª–µ—Ç. –¢–æ–ª—å–∫–æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ–º—ã–µ —Ñ–∞–∫—Ç—ã –∏ –ü–†–Ø–ú–´–ï URL. –ù–∏–∫–∞–∫–∏—Ö –ò–ù–ù/–û–ì–†–ù/—Ñ–∏–Ω–∞–Ω—Å–æ–≤.

–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ ‚Äî –û–î–ò–ù –∞–±–∑–∞—Ü (–±–µ–∑ —Å–ø–∏—Å–∫–æ–≤):
¬´–§–ò–û ‚Äî –ø–ª–æ—â–∞–¥–∫–∞/–∏–∑–¥–∞–Ω–∏–µ ‚Äî –∫—Ä–∞—Ç–∫–∞—è —Å—É—Ç—å (1 —Ñ—Ä–∞–∑–∞) ‚Äî URL (YYYY-MM-DD)¬ª;
–∑–∞–ø–∏—Å–∏ —Ä–∞–∑–¥–µ–ª—è–π —Ç–æ—á–∫–æ–π —Å –∑–∞–ø—è—Ç–æ–π ¬´;¬ª, –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π —Å—Å—ã–ª–∫–∏. –í—Å–µ–≥–æ 3‚Äì8 –∑–∞–ø–∏—Å–µ–π.
–í –∫–æ–Ω—Ü–µ –∞–±–∑–∞—Ü–∞ —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª –¥–æ–±–∞–≤—å: ¬´–ò—Å—Ç–æ—á–Ω–∏–∫–∏: <URL1>, <URL2>, ...¬ª (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ).
""".strip()

# –ú—è–≥–∫–∞—è —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ—Ä–≤—å—é: –≤—ã—Ä–µ–∑–∞–µ–º —Ç–æ–ª—å–∫–æ –ò–ù–ù/–û–ì–†–ù (—Ç–∞–±–ª–∏—Ü–∞ –ø—Ä–∏ —ç—Ç–æ–º —É–∂–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –≤—ã—à–µ)
_FORBID_ID_RE = re.compile(r"\b(–ò–ù–ù|–û–ì–†–ù)\b", re.I)
def sanitize_interviews(text: str) -> str:
    parts = []
    for part in re.split(r"\s*;\s*", (text or "").strip()):
        if part and not _FORBID_ID_RE.search(part):
            parts.append(part.strip())
    return "; ".join(parts)

def _interviews_by_names(company: str,
                         names: List[str],
                         site_hint: Optional[str],
                         market: Optional[str]) -> str:
    """
    –ò—â–µ–º –∏–Ω—Ç–µ—Ä–≤—å—é –ø–æ –∑–∞–¥–∞–Ω–Ω—ã–º –§–ò–û –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–¥–∏–Ω –∞–±–∑–∞—Ü:
      LLM ‚Üí de-dup –ø–æ URL ‚Üí –º—è–≥–∫–∞—è —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è (–±–µ–∑ —Ñ–∏–Ω.—Ñ–∏–ª—å—Ç—Ä–æ–≤).
    """
    if not names:
        return "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
    prompt = _build_interviews_prompt(company, names, site_hint, market)
    try:
        raw = _pplx_call_invest(prompt, model="sonar", recency=None, max_tokens=1400)
    except Exception as e:
        return f"–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö (–æ—à–∏–±–∫–∞: {e})"
    para = _dedup_urls_in_paragraph(raw)   # 1) –¥—É–±–ª–∏–∫–∞—Ç—ã
    para = sanitize_interviews(para)       # 2) —É–±–∏—Ä–∞–µ–º –ò–ù–ù/–û–ì–†–ù
    return para or "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"

def _discover_people(company: str,
                     site_hint: Optional[str],
                     market: Optional[str],
                     top_n: int = 10) -> List[str]:
    """–î–∏—Å–∫–∞–≤–µ—Ä–∏ –§–ò–û —á–µ—Ä–µ–∑ Sonar (—É—Å—Ç–æ–π—á–∏–≤—ã–π –ø—Ä–æ–º–ø—Ç, —Ç–µ—Ä–ø–∏–º—ã–π –ø–∞—Ä—Å–µ—Ä)."""
    prompt = _build_people_discovery_prompt(company, site_hint, market)
    try:
        raw = _pplx_call_invest(prompt, model="sonar", recency=None, max_tokens=900)
    except Exception:
        return []
    names = _parse_people_lines(raw)
    return names[:top_n]

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –±–ª–æ–∫–∞
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def build_dual_interviews(
    company: str,
    company_info: Optional[Dict] = None,
    site_hint: Optional[str] = None,
    market: Optional[str] = None,
    max_people_inet: int = 10
) -> Dict[str, object]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É:
      {
        "names_checko": List[str],
        "digest_checko": str,
        "names_inet": List[str],
        "digest_inet": str,
        "names_union": List[str],
        "digest_union": str,
        "ceo": Dict|None,
        "shareholders": List[Dict],
        "md_block": str,  # –ì–û–¢–û–í–´–ô markdown (–≥–µ–Ω–¥–∏—Ä, –∞–∫—Ü–∏–æ–Ω–µ—Ä—ã, –∏–Ω—Ç–µ—Ä–≤—å—é x3)
      }

    –í–∞–∂–Ω–æ:
      ‚Äî –ò–ù–ù/–¥–æ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ —Ç–∞–±–ª–∏—Ü–µ –∏ –õ–ò–®–¨ –ü–û–¢–û–ú —á–∏—Å—Ç–∏–º –∏–Ω—Ç–µ—Ä–≤—å—é.
      ‚Äî sanitize_invest –Ω–∞ md_block –ù–ï –ø—Ä–∏–º–µ–Ω—è–µ–º.
    """
    # 1) Checko ‚Üí leaders/founders/names
    leaders, founders, names_checko = _names_from_checko_rich(company_info)

    # CEO + –∞–∫—Ü–∏–æ–Ω–µ—Ä—ã
    ceo = _pick_ceo(leaders, names_fallback=names_checko)
    shareholders = _shareholders_from_founders(founders)

    # 2) –ò–Ω—Ç–µ—Ä–≤—å—é –ø–æ Checko-–∏–º–µ–Ω–∞–º
    digest_checko = _interviews_by_names(company, names_checko, site_hint, market) if names_checko else "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"

    # 3) –ò–Ω—Ç–µ—Ä–Ω–µ—Ç-–¥–∏—Å–∫–∞–≤–µ—Ä–∏ ‚Üí –∏–º–µ–Ω–∞ ‚Üí –∏–Ω—Ç–µ—Ä–≤—å—é
    names_inet = _discover_people(company, site_hint, market, top_n=max_people_inet)
    digest_inet = _interviews_by_names(company, names_inet, site_hint, market) if names_inet else "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"

    # 3.5) –û–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ (–¥–∞—ë—Ç –±–æ–ª—å—à–µ —Ö–∏—Ç–æ–≤)
    names_union = list(dict.fromkeys((names_checko or []) + (names_inet or [])))[:12]
    digest_union = _interviews_by_names(company, names_union, site_hint, market) if names_union else "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"

    # 4) Markdown-–±–ª–æ–∫ (–±–µ–∑ –æ–±—â–µ–π —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏–∏, —á—Ç–æ–±—ã –Ω–µ —É–±–∏—Ç—å —Ç–∞–±–ª–∏—Ü—É)
    ceo_line = "_–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö_"
    if ceo:
        inn_txt = f"(–ò–ù–ù {ceo['inn']})" if ceo.get("inn") else ""
        ceo_line = f"**–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä:** {ceo.get('fio','').strip()} {inn_txt}".strip()

    sh_tbl = _markdown_shareholders_table(shareholders)

    md_parts = [
        ceo_line,
        "",
        "**–ê–∫—Ü–∏–æ–Ω–µ—Ä—ã**",
        sh_tbl,
        "",
        "**–ò–Ω—Ç–µ—Ä–≤—å—é (–ø–æ –¥–∞–Ω–Ω—ã–º Checko):**",
        (digest_checko or "_–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö_").strip(),
        "",
        "**–ò–Ω—Ç–µ—Ä–≤—å—é (–∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–¥–∏—Å–∫–∞–≤–µ—Ä–∏):**",
        (digest_inet or "_–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö_").strip(),
        "",
        "**–ò–Ω—Ç–µ—Ä–≤—å—é (–æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –ø–æ–∏—Å–∫):**",
        (digest_union or "_–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö_").strip(),
    ]
    md_block = "\n".join(md_parts).strip()

    return {
        "names_checko": names_checko,
        "digest_checko": digest_checko,
        "names_inet": names_inet,
        "digest_inet": digest_inet,
        "names_union": names_union,
        "digest_union": digest_union,
        "ceo": ceo,
        "shareholders": shareholders,
        "md_block": md_block,
    }

# Backward-compat alias
build_dual_interviews_from_v2 = build_dual_interviews




# ---------- 1. –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–ª–∏–µ–Ω—Ç Checko ----------
@st.cache_data(ttl=3_600)
def ck_call(endpoint: str, inn: str):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ –∫ Checko API.

    endpoint : 'company', 'finances', 'analytics', ‚Ä¶
    inn      : —Å—Ç—Ä–æ–∫–∞ –ò–ù–ù
    """
    url = f"https://api.checko.ru/v2/{endpoint}"
    r = requests.get(
        url,
        params={"key": KEYS["CHECKO_API_KEY"], "inn": inn},
        timeout=10,
    )
    r.raise_for_status()
    return r.json()["data"]

# ---------- 2. –¢–æ–Ω–∫–∏–µ –æ–±—ë—Ä—Ç–∫–∏ (–ø–æ –∂–µ–ª–∞–Ω–∏—é) ----------
ck_company = functools.partial(ck_call, "company")
ck_fin     = functools.partial(ck_call, "finances")
# –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å ck_analytics = functools.partial(ck_call, "analytics")



# ---------- 4. –ü–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –ª–∏–¥–µ—Ä–æ–≤ / —É—á—Ä–µ–¥–∏—Ç–µ–ª–µ–π ----------
def extract_people(cell) -> list[str]:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —è—á–µ–π–∫—É ¬´–†—É–∫–æ–≤–æ–¥¬ª / ¬´–£—á—Ä–µ–¥_–§–õ¬ª –∏
    –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ ¬´–§–ò–û (–ò–ù–ù‚Ä¶, –¥–æ–ª—è ‚Ä¶%)¬ª.
    """
    # 0) —Å—Ä–∞–∑—É –æ—Ç—Å–µ–∫–∞–µ–º None / NaN
    if cell is None or (isinstance(cell, float) and pd.isna(cell)):
        return []

    # 1) –µ—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ ‚Üí –ø—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ Python-–ª–∏—Ç–µ—Ä–∞–ª
    if isinstance(cell, str):
        cell = cell.strip()
        if not cell:
            return []
        try:
            cell = ast.literal_eval(cell)  # '[{‚Ä¶}]' ‚Üí list | dict | str
        except (ValueError, SyntaxError):
            # –ø—Ä–æ—Å—Ç–æ —Å—Ç—Ä–æ–∫–∞ —Å –æ–¥–Ω–∏–º –§–ò–û
            return [cell]

    # 2) –æ–¥–∏–Ω–æ—á–Ω—ã–π dict ‚Üí –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ list
    if isinstance(cell, dict):
        cell = [cell]

    # 3) –µ—Å–ª–∏ —ç—Ç–æ —É–∂–µ list ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç
    if isinstance(cell, list):
        people = []
        for item in cell:
            if isinstance(item, str):
                people.append(item.strip())
            elif isinstance(item, dict):
                fio  = item.get("–§–ò–û") or item.get("fio") or ""
                inn  = item.get("–ò–ù–ù") or item.get("inn")
                share = item.get("–î–æ–ª—è", {}).get("–ü—Ä–æ—Ü–µ–Ω—Ç")
                line = fio
                if inn:
                    line += f" (–ò–ù–ù {inn}"
                    if share is not None:
                        line += f", –¥–æ–ª—è {share}%)"
                    else:
                        line += ")"
                people.append(line)
        return [p for p in people if p]      # –±–µ–∑ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
    # 4) –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø ‚Üí –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ —Å—Ç—Ä–æ–∫—É
    return [str(cell)]



def _safe_div(a: float | None, b: float | None) -> float | None:
    if a is None or b in (None, 0):
        return None
    try:
        return a / b
    except ZeroDivisionError:
        return None




def _to_float_safe(x) -> Optional[float]:
    try:
        if x is None:
            return None
        s = str(x).strip()
        if s.endswith("%"):
            s = s[:-1]
        s = s.replace(",", ".")
        val = float(s)
        # –µ—Å–ª–∏ –¥–∞–ª–∏ –¥–æ–ª—é –≤ –¥–æ–ª—è—Ö (0.25) ‚Äî –ø–µ—Ä–µ–≤–µ–¥—ë–º –≤ –ø—Ä–æ—Ü–µ–Ω—Ç—ã
        if 0 < val <= 1.0:
            val = val * 100.0
        return val
    except Exception:
        return None

def _normalize_share_any(share_obj) -> Optional[float]:
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç:
    - —á–∏—Å–ª–æ/—Å—Ç—Ä–æ–∫—É: '25', '25%', '12,5', 0.25
    - dict –≤–∏–¥–∞ {'–ü—Ä–æ—Ü–µ–Ω—Ç': 25} –∏–ª–∏ {'–î—Ä–æ–±—å': {'–ß–∏—Å–ª–∏—Ç–µ–ª—å':1,'–ó–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å':2}}
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç float –∏–ª–∏ None.
    """
    if share_obj is None:
        return None

    if isinstance(share_obj, dict):
        # 1) —è–≤–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç
        if "–ü—Ä–æ—Ü–µ–Ω—Ç" in share_obj:
            return _to_float_safe(share_obj.get("–ü—Ä–æ—Ü–µ–Ω—Ç"))
        if "percent" in share_obj:
            return _to_float_safe(share_obj.get("percent"))
        # 2) –¥—Ä–æ–±—å
        frac = share_obj.get("–î—Ä–æ–±—å") or share_obj.get("fraction")
        if isinstance(frac, dict):
            try:
                num = float(str(frac.get("–ß–∏—Å–ª–∏—Ç–µ–ª—å") or frac.get("num") or 0).replace(",", "."))
                den = float(str(frac.get("–ó–Ω–∞–º–µ–Ω–∞—Ç–µ–ª—å") or frac.get("den") or 0).replace(",", "."))
                if den:
                    return (num / den) * 100.0
            except Exception:
                pass
        # 3) –æ–±–æ–π–¥—ë–º –≤–ª–æ–∂–µ–Ω–Ω—ã–µ –∫–ª—é—á–∏
        for v in share_obj.values():
            got = _normalize_share_any(v)
            if got is not None:
                return got
        return None

    # —á–∏—Å–ª–æ/—Å—Ç—Ä–æ–∫–∞
    return _to_float_safe(share_obj)

def _parse_checko_cell(cell, role_hint: Optional[str] = None) -> List[Dict]:
    """
    –î–µ–ª–∞–µ—Ç –∏–∑ leaders_raw / founders_raw —Å–ø–∏—Å–æ–∫:
      {'fio','inn','share_pct','role'}
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç str | dict | list, —Ä–∞–∑–Ω—ã–µ —Å—Ö–µ–º—ã –¥–æ–ª–µ–π.
    """
    import re, ast
    out: List[Dict] = []

    def _emit(fio=None, inn=None, share=None, role=None):
        fio = _norm(fio)
        inn = _norm(str(inn)) if inn else None
        share_pct = _normalize_share_any(share)
        item = {"fio": fio, "inn": inn, "share_pct": share_pct, "role": _norm(role or role_hint)}
        if item["fio"] or item["inn"]:
            out.append(item)

    if cell is None:
        return out

    # –µ—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON-–ø–æ–¥–æ–±–Ω–æ–µ
    if isinstance(cell, str):
        s = cell.strip()
        if s.startswith("[") or s.startswith("{"):
            try:
                parsed = ast.literal_eval(s)
                return _parse_checko_cell(parsed, role_hint=role_hint)
            except Exception:
                pass
        # –≤—ã–¥–µ—Ä–Ω–µ–º –ò–ù–ù/–¥–æ–ª—é –∏–∑ —Å—Ç—Ä–æ–∫–∏
        m_inn = re.search(r"(?:–ò–ù–ù|inn)\s*[:‚Ññ]?\s*([0-9]{8,12})", s, re.I)
        inn = m_inn.group(1) if m_inn else None
        m_share = re.search(r"(?:–¥–æ–ª—è|share)[^0-9]*([0-9]+[.,]?[0-9]*)\s*%?", s, re.I)
        share = m_share.group(1) if m_share else None
        fio = re.sub(r"\s*\(.*?\)\s*$", "", s).strip()
        _emit(fio=fio, inn=inn, share=share)
        return out

    if isinstance(cell, dict):
        fio   = cell.get("–§–ò–û") or cell.get("fio") or cell.get("name")
        inn   = cell.get("–ò–ù–ù") or cell.get("inn") or cell.get("tax_id")
        # 1) –∫–ª–∞—Å—Å–∏–∫–∞ Checko: –¥–æ–ª—è –º–æ–∂–µ—Ç –±—ã—Ç—å dict –∏–ª–∏ ¬´–ø–ª–æ—Å–∫–∞—è¬ª
        share = None
        if isinstance(cell.get("–î–æ–ª—è"), dict):
            share = cell["–î–æ–ª—è"]
        elif "share" in cell:
            share = cell["share"]
        elif "–î–æ–ª—è" in cell and not isinstance(cell.get("–î–æ–ª—è"), dict):
            share = cell.get("–î–æ–ª—è")
        role  = cell.get("–î–æ–ª–∂–Ω–æ—Å—Ç—å") or cell.get("role") or role_hint
        _emit(fio=fio, inn=inn, share=share, role=role)
        return out

    if isinstance(cell, list):
        for it in cell:
            out.extend(_parse_checko_cell(it, role_hint=role_hint))
        return out

    # fallback
    _emit(fio=str(cell))
    return out

def _pick_ceo(leaders: List[Dict], names_fallback: Optional[List[str]] = None) -> Optional[Dict]:
    """
    –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –≥–µ–Ω–¥–∏—Ä–µ–∫—Ç–æ—Ä–∞:
    1) –ø–æ —Ä–æ–ª–∏ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º '–≥–µ–Ω–µ—Ä–∞–ª—å';
    2) –ø–æ '–¥–∏—Ä–µ–∫—Ç–æ—Ä'/'—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å'/'CEO';
    3) –∏–Ω–∞—á–µ –ø–µ—Ä–≤—ã–π –∏–∑ leaders;
    4) –∏–Ω–∞—á–µ —Ñ–æ–ª–±—ç–∫ –ø–æ –ø–µ—Ä–≤–æ–º—É –∏–º–µ–Ω–∏.
    """
    if not leaders and names_fallback:
        return {"fio": names_fallback[0], "inn": None, "share_pct": None, "role": "—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å"}

    for p in leaders:
        r = (p.get("role") or "").lower()
        if "–≥–µ–Ω–µ—Ä–∞–ª—å" in r:
            return p
    for p in leaders:
        r = (p.get("role") or "").lower()
        if any(k in r for k in ("–¥–∏—Ä–µ–∫—Ç–æ—Ä", "—Ä—É–∫–æ–≤–æ–¥", "ceo")):
            return p
    if leaders:
        return leaders[0]
    if names_fallback:
        return {"fio": names_fallback[0], "inn": None, "share_pct": None, "role": "—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å"}
    return None

def _shareholders_from_founders(founders: List[Dict]) -> List[Dict]:
    """
    –ß–∏—Å—Ç–∏–º/—Å–æ—Ä—Ç–∏—Ä—É–µ–º –∞–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤. –ï—Å–ª–∏ –¥–æ–ª—è –¥–∞–Ω–∞ –≤ –¥–æ–ª—è—Ö (<=1),
    –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –ø—Ä–æ—Ü–µ–Ω—Ç—ã. –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é.
    """
    rows = []
    for p in founders:
        fio = (p.get("fio") or "").strip()
        inn = p.get("inn")
        share = _to_float_safe(p.get("share_pct"))  # –Ω–∞ —Å–ª—É—á–∞–π —Å—Ç–∞—Ä—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        rows.append({"fio": fio, "inn": inn, "share_pct": share})

    with_share = [r for r in rows if r["share_pct"] is not None]
    no_share   = [r for r in rows if r["share_pct"] is None]
    with_share.sort(key=lambda x: x["share_pct"], reverse=True)
    return with_share + no_share



# –∫–µ—à–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∫–ª–∏–∫–∞—Ö –Ω–µ –¥–µ—Ä–≥–∞—Ç—å LLM –∏ —Å–∞–π—Ç –∑–∞–Ω–æ–≤–æ
@st.cache_data(ttl=86_400, show_spinner=False)










def run_ai_insight_tab() -> None:

        

    # ‚ï≠‚îÄüéõ  UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
    st.title("üìä AI Company Insight")
    st.markdown("–í–≤–µ–¥–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ (–∫–∞–∂–¥–∞—è –∫–æ–º–ø–∞–Ω–∏—è ‚Äî –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ).")
    
    c1, c2, c3, c4 = st.columns(4)
    with c1: inns_raw  = st.text_area("–ò–ù–ù")          # ‚úÖ –±–µ–∑ key=* ‚Äî –Ω–∞–º –Ω–µ –Ω—É–∂–Ω—ã –¥–≤–µ –∫–æ–ø–∏–∏
    with c2: names_raw = st.text_area("–ù–∞–∑–≤–∞–Ω–∏–µ")
    with c3: mkts_raw  = st.text_area("–†—ã–Ω–æ–∫")
    with c4: sites_raw = st.text_area("–°–∞–π—Ç")
    
    aggregate_mode = st.checkbox("üßÆ –°—É–º–º–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏–Ω–∞–Ω—Å—ã –ø–æ –≤—Å–µ–º –ò–ù–ù")
    
    if st.button("üîç –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –æ—Ç—á—ë—Ç", key="ai_build"):
        with st.spinner("–°—á–∏—Ç–∞–µ–º –æ—Ç—á—ë—Ç‚Ä¶"):

    
            # ---------- –ø–∞—Ä—Å–∏–Ω–≥ ----------
            split = lambda s: [i.strip() for i in s.splitlines() if i.strip()]
            inns   = split(inns_raw)
            names  = split(names_raw)
            mkts   = split(mkts_raw)
            sites  = split(sites_raw)
            
            # ---------- –≤–∞–ª–∏–¥–∞—Ü–∏—è ----------
            if not inns:
                st.error("–£–∫–∞–∂–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ò–ù–ù."); st.stop()
            
            if aggregate_mode:            # Œ£-—Ä–µ–∂–∏–º
                # –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–∞—Å—Ç—è–≥–∏–≤–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                if len(names) == 1 and len(inns) > 1:  names *= len(inns)
                if len(mkts)  == 1 and len(inns) > 1:  mkts  *= len(inns)
                if len(sites) == 1 and len(inns) > 1:  sites *= len(inns)
            
                # —Ç–µ–ø–µ—Ä—å –≤—Å—ë –ª–∏–±–æ –ø—É—Å—Ç–æ–µ, –ª–∏–±–æ —Å–æ–≤–ø–∞–¥–∞–µ—Ç –ø–æ –¥–ª–∏–Ω–µ
                for lst, lbl in [(names, "–ù–∞–∑–≤–∞–Ω–∏–µ"), (mkts, "–†—ã–Ω–æ–∫")]:
                    if lst and len(lst) != len(inns):
                        st.error(f"–ß–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ ¬´{lbl}¬ª –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 1 –∏–ª–∏ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —á–∏—Å–ª–æ–º –ò–ù–ù."); st.stop()
            
            else:                         # –æ–¥–∏–Ω–æ—á–Ω—ã–π —Ä–µ–∂–∏–º
                if not (names and mkts):
                    st.error("–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –ø–æ–ª—è ‚Äî –ò–ù–ù, –ù–∞–∑–≤–∞–Ω–∏–µ –∏ –†—ã–Ω–æ–∫."); st.stop()
                if len({len(inns), len(names), len(mkts)}) != 1:
                    st.error("–ß–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ –≤–æ –≤—Å–µ—Ö —Ç—Ä—ë—Ö –ø–æ–ª—è—Ö –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å."); st.stop()
                if sites and len(sites) != len(inns):
                    st.error("–ß–∏—Å–ª–æ —Å—Ç—Ä–æ–∫ ¬´–°–∞–π—Ç¬ª –¥–æ–ª–∂–Ω–æ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —á–∏—Å–ª–æ–º –ò–ù–ù."); st.stop()
            
            # ---------- –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—ã —Å–ø–∏—Å–∫–æ–≤ ----------
            pad = lambda lst: lst if lst else [""] * len(inns)
            names_full = pad(names)
            mkts_full  = pad(mkts)
            sites_full = pad(sites)
            YEARS = ["2022", "2023", "2024"]
            df_companies = pd.DataFrame([ck_company(i) for i in inns])

            
            def parse_people_cell(cell) -> list[str]:
                """
                –ü—Ä–∏–Ω–∏–º–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —è—á–µ–π–∫–∏ ¬´–†—É–∫–æ–≤–æ–¥¬ª –∏–ª–∏ ¬´–£—á—Ä–µ–¥_–§–õ¬ª
                –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ ¬´–§–ò–û (–ò–ù–ù xxxx, –¥–æ–ª—è yy%)¬ª.
                –†–∞–±–æ—Ç–∞–µ—Ç –∏ –µ—Å–ª–∏ cell = NaN, '', —Å–ø–∏—Å–æ–∫, dict, —Å—Ç—Ä–æ–∫–∞-JSON.
                """
                # –ø—É—Å—Ç–æ / NaN
                if cell is None or (isinstance(cell, float) and pd.isna(cell)):
                    return []
            
                # –µ—Å–ª–∏ –ø—Ä–∏—à–ª–∞ —Å—Ç—Ä–æ–∫–∞ ‚Äî –ø—Ä–æ–±—É–µ–º –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –≤ –æ–±—ä–µ–∫—Ç
                if isinstance(cell, str):
                    cell = cell.strip()
                    if not cell:
                        return []
                    try:
                        cell = ast.literal_eval(cell)      # '[{‚Ä¶}]' -> python
                    except (ValueError, SyntaxError):
                        # –ø—Ä–æ—Å—Ç–æ —Å—Ç—Ä–æ–∫–∞ —Å –æ–¥–Ω–∏–º –§–ò–û
                        return [cell]
            
                # –æ–¥–∏–Ω–æ—á–Ω—ã–π dict
                if isinstance(cell, dict):
                    cell = [cell]
            
                # list
                if isinstance(cell, list):
                    out = []
                    for item in cell:
                        if isinstance(item, str):          # —É–∂–µ –≥–æ—Ç–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞
                            out.append(item.strip())
                        elif isinstance(item, dict):       # –Ω–∞—à –æ—Å–Ω–æ–≤–Ω–æ–π —Å–ª—É—á–∞–π
                            fio   = item.get("–§–ò–û") or ""
                            inn   = item.get("–ò–ù–ù") or ""
                            share = item.get("–î–æ–ª—è", {}).get("–ü—Ä–æ—Ü–µ–Ω—Ç")
                            line  = fio
                            if inn:
                                line += f" (–ò–ù–ù {inn}"
                                line += f", –¥–æ–ª—è {int(share)}%)" if share is not None else ")"
                            out.append(line)
                    return [s for s in out if s]
                # fallback
                return [str(cell)]
            
            def row_people_json(row: pd.Series) -> dict:
                """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç {'leaders_raw': [...], 'founders_raw': [...]}."""
                # ‚îÄ‚îÄ 1. —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                leaders = parse_people_cell(row.get("–†—É–∫–æ–≤–æ–¥"))
            
                # ‚îÄ‚îÄ 2. —É—á—Ä–µ–¥–∏—Ç–µ–ª–∏: –∫–æ–ª–æ–Ω–∫–∞ '–£—á—Ä–µ–¥' ‚Üí dict ‚Üí –∫–ª—é—á '–§–õ' ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                founders_cell = None
                uc = row.get("–£—á—Ä–µ–¥")
                if isinstance(uc, dict):
                    founders_cell = uc.get("–§–õ")          # —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
                else:
                    founders_cell = uc                    # fallback (–µ—Å–ª–∏ —Ñ–æ—Ä–º–∞—Ç –¥—Ä—É–≥–æ–π)
            
                founders = parse_people_cell(founders_cell)
            
                return {"leaders_raw": leaders, "founders_raw": founders}
            
            people_cols = df_companies.apply(row_people_json, axis=1, result_type="expand")
            df_companies = pd.concat([df_companies, people_cols], axis=1)

            

            
            PNL_CODES = [                       # –≤—Å—ë, —á—Ç–æ —Ö–æ—Ç–∏–º –≤–∏–¥–µ—Ç—å –≤ –¥–ª–∏–Ω–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ
                ("–í—ã—Ä—É—á–∫–∞ (‚ÇΩ –º–ª–Ω)",                "2110"),
                ("–°–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å –ø—Ä–æ–¥–∞–∂ (‚ÇΩ –º–ª–Ω)",   "2120"),
                ("–í–∞–ª–æ–≤–∞—è –ø—Ä–∏–±—ã–ª—å (‚ÇΩ –º–ª–Ω)",        "2200"),
                ("–ö–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ —Ä–∞—Å—Ö–æ–¥—ã (‚ÇΩ –º–ª–Ω)",   "2210"),
                ("–£–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–∏–µ —Ä–∞—Å—Ö–æ–¥—ã (‚ÇΩ –º–ª–Ω)", "2220"),
                ("–ü—Ä–∏–±—ã–ª—å –æ—Ç –ø—Ä–æ–¥–∞–∂ (‚ÇΩ –º–ª–Ω)",      "2300"),
                ("–î–æ—Ö–æ–¥—ã –æ—Ç —É—á–∞—Å—Ç–∏—è (‚ÇΩ –º–ª–Ω)",      "2310"),
                ("–ü—Ä–æ—Ü–µ–Ω—Ç—ã –∫ –ø–æ–ª—É—á–µ–Ω–∏—é (‚ÇΩ –º–ª–Ω)",   "2320"),
                ("–ü—Ä–æ—Ü–µ–Ω—Ç—ã –∫ —É–ø–ª–∞—Ç–µ (‚ÇΩ –º–ª–Ω)",      "2330"),
                ("–ü—Ä–æ—á–∏–µ –¥–æ—Ö–æ–¥—ã (‚ÇΩ –º–ª–Ω)",          "2340"),
                ("–ü—Ä–æ—á–∏–µ —Ä–∞—Å—Ö–æ–¥—ã (‚ÇΩ –º–ª–Ω)",         "2350"),
                ("–ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å (‚ÇΩ –º–ª–Ω)",         "2400"),
                ("–°–æ–≤–æ–∫—É–ø–Ω—ã–π –¥–æ–ª–≥ (‚ÇΩ –º–ª–Ω)",        "_total_debt"),
                ("–î–µ–Ω–µ–∂–Ω—ã–µ —Å—Ä–µ–¥—Å—Ç–≤–∞ (‚ÇΩ –º–ª–Ω)",      "_cash"),
                ("–ö—Ä–µ–¥–∏—Ç–æ—Ä—Å–∫–∞—è –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç—å (‚ÇΩ –º–ª–Ω)", "1520"),
                ("–ß–∏—Å—Ç—ã–π –¥–æ–ª–≥ (‚ÇΩ –º–ª–Ω)",            "_net_debt"),
                ("EBIT margin (%)",                "_ebit_margin"),
                ("Net Debt / EBIT",                "_netdebt_ebit"),
            ]
            
            # ---------- ‚ë† —Å–≤–æ–¥–Ω–∞—è –≤–∫–ª–∞–¥–∫–∞, –µ—Å–ª–∏ –Ω—É–∂–Ω–∞ ----------
            def build_agg_finances() -> dict[str, dict[str, float | None]]:
                """–°—É–º–º–∏—Ä—É–µ—Ç –≤—Å–µ –ò–ù–ù –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å agg[year][code]."""
                NUMERIC = {c for _, c in PNL_CODES if c.isdigit()} | {"1250", "1400", "1500"}
                raw = {y: defaultdict(float) for y in YEARS}
            
                for inn in inns:
                    fin = ck_fin(inn)
                    for y in YEARS:
                        for code in NUMERIC:
                            v = fin.get(y, {}).get(code)
                            if isinstance(v, (int, float)):
                                raw[y][code] += v / 1e6        # ‚Üí –º–ª–Ω
            
                agg = {}
                for y in YEARS:
                    rev   = raw[y]["2110"]
                    ebit  = raw[y]["2200"]
                    cash  = raw[y]["1250"]
                    debt  = raw[y]["1400"] + raw[y]["1500"]
                    pay   = raw[y]["1520"]
            
                    net_debt    = debt - cash - pay
                    ebit_margin = _safe_div(ebit, rev)
                    ebit_margin = ebit_margin * 100 if ebit_margin is not None else None
                    nd_ebit     = _safe_div(net_debt, ebit)
            
            
                    agg[y] = {**raw[y],
                              "_total_debt":  debt,
                              "_cash":        cash,
                              "_net_debt":    net_debt,
                              "_ebit_margin": ebit_margin,
                              "_netdebt_ebit":nd_ebit}
                return agg
            
            # —Å–æ–∑–¥–∞—ë–º –≤–∫–ª–∞–¥–∫–∏ –∑–∞—Ä–∞–Ω–µ–µ (—á—Ç–æ–±—ã –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è tabs –°–£–©–ï–°–¢–í–û–í–ê–õ–ê –≤—Å–µ–≥–¥–∞)
            if aggregate_mode:
                tabs = st.tabs(["Œ£ –°–≤–æ–¥–Ω–æ"] + ([] if len(inns) == 1 else
                                               [f"{n} ({inn})" for inn, n in zip(inns, names_full)]))
                # –±–ª–æ–∫ Œ£ –°–≤–æ–¥–Ω–æ ‚Äî –≤—Å–µ–≥–¥–∞ –ø–µ—Ä–≤—ã–π
                with tabs[0]:
                    agg = build_agg_finances()
            
                    st.header("Œ£ –°–≤–æ–¥–Ω–∞—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞")
                    tbl = pd.DataFrame({"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å": [n for n, _ in PNL_CODES]})
                    for y in YEARS:
                        tbl[y] = [agg[y].get(code) for _, code in PNL_CODES]
            
                    def _fmt(v, pct=False, d=1):
                        if v is None or (isinstance(v, float) and np.isnan(v)): return "‚Äî"
                        return f"{v:.{d}f}{'%' if pct else ''}".replace(".", ",")
                    for i, (nm, _) in enumerate(PNL_CODES):
                        pct  = nm.endswith("%")
                        digs = 2 if ("Net" in nm or pct) else 1
                        tbl.iloc[i, 1:] = [_fmt(v, pct, digs) for v in tbl.iloc[i, 1:]]
            
                    st.dataframe(tbl.set_index("–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å"),
                                 use_container_width=True,
                                 height=min(880, 40 * len(PNL_CODES)))
            
            
                    # –≥—Ä–∞—Ñ–∏–∫
                    # --- –≥—Ä–∞—Ñ–∏–∫: –≤—ã—Ä—É—á–∫–∞ / EBIT / —á–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å + EBIT-margin -----------
                    fig, ax1 = plt.subplots(figsize=(7, 3.5))
                    x = np.arange(len(YEARS)); w = 0.25
                    
                    bars_rev  = ax1.bar(x - w, [agg[y]["2110"] or 0 for y in YEARS],
                                        w, label="–í—ã—Ä—É—á–∫–∞")
                    bars_ebit = ax1.bar(x,     [agg[y]["2200"] or 0 for y in YEARS],
                                        w, label="EBIT")
                    bars_prof = ax1.bar(x + w, [agg[y]["2400"] or 0 for y in YEARS],
                                        w, label="–ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å")
                    
                    # –ø–æ–¥–ø–∏—Å–∏ –Ω–∞ —Å—Ç–æ–ª–±—Ü–∞—Ö
                    for b in (*bars_rev, *bars_ebit, *bars_prof):
                        h = b.get_height()
                        if h and not np.isnan(h):
                            ax1.annotate(f"{h:.1f}", xy=(b.get_x() + b.get_width()/2, h),
                                         xytext=(0, 3), textcoords="offset points",
                                         ha="center", fontsize=8)
                    
                    # –ª–∏–Ω–∏—è EBIT-margin (%)
                    ax2 = ax1.twinx()
                    margins = [agg[y]["_ebit_margin"] if agg[y]["_ebit_margin"] else np.nan for y in YEARS]
                    ax2.plot(x, margins, linestyle="--", marker="o", label="EBIT margin, %")
                    
                    # –ø–æ–¥–ø–∏—Å–∏ ¬´—Ö %¬ª –Ω–∞–¥ —Ç–æ—á–∫–∞–º–∏ –ª–∏–Ω–∏–∏
                    for xx, yy in zip(x, margins):
                        if not np.isnan(yy):
                            ax2.annotate(f"{yy:.1f}%", xy=(xx, yy),
                                         xytext=(0, 5), textcoords="offset points",
                                         ha="center", fontsize=8)
                    
                    # –æ–±—ä–µ–¥–∏–Ω—è–µ–º –ª–µ–≥–µ–Ω–¥—É
                    h1, l1 = ax1.get_legend_handles_labels()
                    h2, l2 = ax2.get_legend_handles_labels()
                    ax1.legend(h1 + h2, l1 + l2, loc="upper left", fontsize=9)
                    # ‚üµ  –ø—Ä—è—á–µ–º —à–∫–∞–ª—ã Y
                    ax1.set_yticks([]); ax2.set_yticks([])
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                    
                    for ax in (ax1, ax2):
                        ax.spines[:].set_visible(False)
                    
                    # –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ
                    ax1.set_xticks(x); ax1.set_xticklabels(YEARS, fontsize=10)
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                    for ax in (ax1, ax2):
                        ax.spines[:].set_visible(False)
                    
                    fig.tight_layout(pad=1.0)
                    st.pyplot(fig)
            
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ü—Ä–æ—Ñ–∏–ª—å / —Ä—ã–Ω–æ–∫ / —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ (+ –ø–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    first_name = names_full[0] or "–ö–æ–º–ø–∞–Ω–∏—è"
                    first_mkt  = mkts_full[0]
                    first_site = sites_full[0]
                    first_inn = inns[0] if inns else None
                    
                    # --- –µ–¥–∏–Ω—ã–π RAG-–ø–∞–π–ø–ª–∞–π–Ω (Google-—Å–Ω–∏–ø–ø–µ—Ç—ã + —Å–∞–π—Ç) ---------------------
                    st.subheader("üìù –û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏")
                    desc_legacy = st.toggle("Legacy (Google/SiteRAG) description", value=False, key="desc_first")
                    
                    if desc_legacy:
                        with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ (Legacy)‚Ä¶"):
                            doc = RAG(first_name, website=first_site, market=first_mkt).run()
                    
                        html_main = linkify_as_word(doc["summary"]).replace("\n", "<br>")
                        st.markdown(
                            f"<div style='background:#F7F9FA;border:1px solid #ccc;"
                            f"border-radius:8px;padding:18px;line-height:1.55'>{html_main}</div>",
                            unsafe_allow_html=True,
                        )
                    
                        with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                            for i, q in enumerate(doc["queries"], 1):
                                st.markdown(f"**{i}.** {q}")
                    
                        with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                            st.dataframe(
                                pd.DataFrame(doc["snippets"], columns=["URL", "Snippet"]).head(15),
                                use_container_width=True,
                            )
                    
                        if doc.get("site_pass"):
                            with st.expander("üåê –ü–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞"):
                                st.markdown(
                                    f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                                    f"border-radius:8px;padding:18px;line-height:1.55'>"
                                    f"{_linkify(doc['site_pass']).replace(chr(10), '<br>')}</div>",
                                    unsafe_allow_html=True,
                                )
                        else:
                            st.info("–ü–∞—Å–ø–æ—Ä—Ç —Å–∞–π—Ç–∞ –Ω–µ –ø–æ–ª—É—á–µ–Ω (–Ω–µ—Ç URL, –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–ª–∏ –∏—Å—Ç–µ–∫ —Ç–∞–π-–∞—É—Ç).")
                    
                    else:
                        with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º INVEST SNAPSHOT‚Ä¶"):
                            inv = get_invest_snapshot(
                                first_name,
                                site_hint=first_site,
                                model="sonar",
                                recency=None,
                                max_tokens=1500
                            )
                            # –≤—ã—Ä–µ–∑–∞–µ–º —Ä–∞–∑–¥–µ–ª ¬´–ò–Ω—Ç–µ—Ä–≤—å—é¬ª –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
                            inv_clean = strip_interviews_section(inv["md"])
                            # –¥–µ–ª–∞–µ–º URL –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–º–∏ (–≤–Ω—É—Ç—Ä–∏ div –º–æ–∂–Ω–æ HTML)
                            inv_html = linkify_keep_url(inv_clean)
                            st.markdown(
                                f"<div style='background:#F7F9FA;border:1px solid #ccc;border-radius:8px;padding:18px;line-height:1.55'>{inv_html}</div>",
                                unsafe_allow_html=True,
                            )
                            doc = {"summary": inv_clean, "mode": "invest_snapshot"}
                        with st.expander("üîß –û—Ç–ª–∞–¥–∫–∞ (—Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç)"):
                            st.text(inv.get("raw") or "‚Äî")
                    
                    # ----------- –†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç (MARKET EVIDENCE) ------------------------
                    if first_mkt:
                        st.subheader("üìà –†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç")
                        legacy = st.toggle("Legacy (Google/GPT) mode", value=False, key="legacy_first")
                    
                        if legacy:
                            with st.spinner("–°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ä—ã–Ω–∫—É (Legacy)‚Ä¶"):
                                mkt_res = get_market_rag(first_mkt)
                            mkt_html = _linkify(mkt_res["summary"]).replace("\n", "<br>")
                            st.markdown(
                                f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                                f"border-radius:8px;padding:18px;line-height:1.55'>{mkt_html}</div>",
                                unsafe_allow_html=True,
                            )
                            with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                                for i, q in enumerate(mkt_res["queries"], 1):
                                    st.markdown(f"**{i}.** {q}")
                            with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                                st.dataframe(
                                    pd.DataFrame(mkt_res["snippets"], columns=["URL", "Snippet"]).head(15),
                                    use_container_width=True,
                                )
                        else:
                            with st.spinner("–°–æ–±–∏—Ä–∞–µ–º MARKET EVIDENCE‚Ä¶"):
                                ev = get_market_evidence(first_mkt, country="–†–æ—Å—Å–∏—è", min_sources=8, recency=None)
                            st.markdown(
                                f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;"
                                f"border-radius:8px;padding:18px;line-height:1.55'>{ev['text_html']}</div>",
                                unsafe_allow_html=True,
                            )
                            st.caption("–°–¢–†–£–ö–¢–£–†–ê –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º ‚Äî –¥–µ–Ω—å–≥–∏:")
                            st.code(ev["money_block"] or "‚Äî", language="text")
                            st.caption("–°–¢–†–£–ö–¢–£–†–ê –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º ‚Äî –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—ã–µ –æ–±—ä—ë–º—ã:")
                            st.code(ev["natural_block"] or "‚Äî", language="text")
                    
                            # (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –æ—Ç–ª–∞–¥–∫–∞ —Å—ã—Ä–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                            with st.expander("üîß –û—Ç–ª–∞–¥–∫–∞ (—Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç)"):
                                st.text(ev["raw_text"] or "‚Äî")

                        if "mkt_res" not in locals():
                            mkt_res = {}
                    
                    # ----------- –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é -----------------------------------
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    st.subheader("üë• –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é")
                    
                    company_info_row = {
                        "leaders_raw":  (df_companies.loc[idx, "leaders_raw"]  if "leaders_raw"  in df_companies.columns else []) or [],
                        "founders_raw": (df_companies.loc[idx, "founders_raw"] if "founders_raw" in df_companies.columns else []) or [],
                    }
                    
                    with st.spinner("–ò—â–µ–º –∏–Ω—Ç–µ—Ä–≤—å—é (Checko ‚Üí –∏–Ω—Ç–µ—Ä–Ω–µ—Ç)‚Ä¶"):
                        dual = build_dual_interviews_from_v2(
                            cmp_name, company_info=company_info_row, site_hint=site, market=mkt
                        )
                    
                    fio_checko = ", ".join(dual.get("names_checko") or []) or "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
                    fio_inet   = ", ".join(dual.get("names_inet") or [])   or "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö"
                    
                    digest_checko = sanitize_invest(dual.get("digest_checko") or "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
                    digest_inet   = sanitize_invest(dual.get("digest_inet") or "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
                    
                    # –¥–µ–ª–∞–µ–º —Å—Å—ã–ª–∫–∏ –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–º–∏:
                    dig_checko_html = linkify_keep_url(digest_checko).replace("\n", "<br>")
                    dig_inet_html   = linkify_keep_url(digest_inet).replace("\n", "<br>")
                    
                    block_checko = (f"<h4 style='margin:6px 0'>–î–∞–π–¥–∂–µ—Å—Ç –∏–Ω—Ç–µ—Ä–≤—å—é ‚Äî Checko</h4><div>{dig_checko_html}</div>"
                                    if digest_checko.strip().lower() != "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö" else "")
                    block_inet   = (f"<h4 style='margin:14px 0 6px'>–î–∞–π–¥–∂–µ—Å—Ç –∏–Ω—Ç–µ—Ä–≤—å—é ‚Äî –∏–Ω—Ç–µ—Ä–Ω–µ—Ç</h4><div>{dig_inet_html}</div>"
                                    if digest_inet.strip().lower() != "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö" else "")
                    
                    st.markdown(
                        f"<div style='background:#F9FAFB;border:1px solid #ddd;border-radius:8px;padding:18px;line-height:1.6'>"
                        f"<p><b>–§–ò–û (Checko):</b> {html.escape(fio_checko)}</p>"
                        f"<p><b>–§–ò–û (–∏–Ω—Ç–µ—Ä–Ω–µ—Ç):</b> {html.escape(fio_inet)}</p>"
                        f"<hr style='border:none;border-top:1px solid #eee;margin:10px 0'>"
                        f"{block_checko}"
                        f"{block_inet}"
                        f"</div>",
                        unsafe_allow_html=True,
                    )
                    
                    # 2) ‚¨áÔ∏è –ù–û–í–û–ï: —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç—á—ë—Ç (2-—à–∞–≥–æ–≤—ã–π PPLX: –∫–∞–Ω–¥–∏–¥–∞—Ç—ã ‚Üí —Ä–∞—Å–∫—Ä—ã—Ç–∏–µ —é—Ä–ª–∏—Ü ‚Üí —Ñ–∞–∫—Ç—ã + –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ä–∞–∑–¥–µ–ª ¬´–ò–Ω—Ç–µ—Ä–≤—å—é¬ª)
                    adv_mode = st.toggle("–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç—á—ë—Ç (–±–µ–Ω–µ—Ñ–∏—Ü–∏–∞—Ä—ã + —Ñ–∞–∫—Ç—ã + –∏–Ω—Ç–µ—Ä–≤—å—é)", value=True, key=f"owners_{idx}")
                    if adv_mode:
                        with st.spinner("–°–æ–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç—á—ë—Ç –ø–æ –≤–ª–∞–¥–µ–ª—å—Ü–∞–º/–±–µ–Ω–µ—Ñ–∏—Ü–∏–∞—Ä–∞–º‚Ä¶"):
                            try:
                                deep_text = two_step_perplexity_owners_v2(
                                    cmp_name,
                                    site_hint=site,
                                    market=mkt,
                                    model="sonar",
                                    recency=None,
                                    owners_limit=8,
                                    expand_org_limit=6,
                                    per_person_tokens=1500,
                                )
                                # –¥–µ–ª–∞–µ–º —Å—Å—ã–ª–∫–∏ –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—ã–º–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–º–µ—Ç–∫—É —Å—Ç—Ä–æ–∫
                                deep_html = linkify_keep_url(deep_text).replace("\n", "<br>")
                            except Exception as e:
                                deep_html = f"<i>–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç—á—ë—Ç: {html.escape(str(e))}</i>"
                    
                        st.markdown(
                            f"<div style='background:#FFF;border:1px dashed #cbd5e1;border-radius:8px;padding:16px;line-height:1.6'>"
                            f"{deep_html}"
                            f"</div>",
                            unsafe_allow_html=True,
                        )

                    
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –∫–æ–Ω–µ—Ü –±–ª–æ–∫–∞, –¥–∞–ª—å—à–µ –≤–∞—à –∫–æ–¥ (–µ—Å–ª–∏ –±—ã–ª) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            
            # ---------- ‚ë° –≤–∫–ª–∞–¥–∫–∏ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –∫–æ–º–ø–∞–Ω–∏—è–º ----------
            if aggregate_mode and len(inns) > 1:
                tabs = st.tabs(["Œ£ –°–≤–æ–¥–Ω–æ"] + [f"{n} ({inn})"
                                               for inn, n in zip(inns, names_full)])
            else:                                   # –æ–¥–∏–Ω–æ—á–Ω—ã–π —Ä–µ–∂–∏–º
                tabs = st.tabs([f"{n} ({inn})" for inn, n
                                in zip(inns, names_full)])
            
            start_idx = 1 if (aggregate_mode and len(inns) > 1) else 0
            
            for idx, (tab, inn, cmp_name, mkt, site) in enumerate(
                zip(tabs[start_idx:], inns, names_full, mkts_full, sites_full)
            ):
                with tab:
                    st.header(f"{cmp_name} ‚Äî {inn}")
                    # –¥–∞–ª—å—à–µ –≤–µ–∑–¥–µ –∏—Å–ø–æ–ª—å–∑—É–π cmp_name –≤–º–µ—Å—Ç–æ name
            
                    # ---------- –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –ø—Ä–æ—Ñ–∏–ª—å ----------
                    fin = ck_fin(inn)
                    calc = {y: {} for y in YEARS}
            
                    for y in YEARS:
                        yr = fin.get(y, {})
                        # –ø—Ä—è–º—ã–µ —Å—Ç—Ä–æ–∫–∏ –æ—Ç—á—ë—Ç–∞
                        for _, code in PNL_CODES:
                            if code.isdigit():
                                v = yr.get(code)
                                calc[y][code] = (v / 1e6) if isinstance(v, (int, float)) else None
            
                        # —Ä–∞—Å—á—ë—Ç–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
                        cash  = yr.get("1250", 0) / 1e6
                        debt  = (yr.get("1400", 0) + yr.get("1500", 0)) / 1e6
                        pay   = yr.get("1520", 0) / 1e6
                        rev   = calc[y].get("2110")
                        ebit  = calc[y].get("2200")
                        net_debt = debt - cash - pay
                        ebit_m = _safe_div(ebit, rev)
                        ebit_m = ebit_m * 100 if ebit_m is not None else None
                        nd_eb     = _safe_div(net_debt, ebit)
            
                        calc[y].update({
                            "_total_debt": debt or None,
                            "_cash":       cash or None,
                            "_net_debt":   net_debt if rev is not None else None,
                            "_ebit_margin":ebit_m,
                            "_netdebt_ebit":nd_eb,
                        })
            
                    # --- –¥–ª–∏–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ ---
                    tbl = pd.DataFrame({"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å": [n for n, _ in PNL_CODES]})
                    for y in YEARS:
                        tbl[y] = [calc[y].get(code) for _, code in PNL_CODES]
            
                    def fmt(v, pct=False, d=1):
                        if v is None or (isinstance(v, float) and np.isnan(v)): return "‚Äî"
                        return f"{v:.{d}f}{'%' if pct else ''}".replace(".", ",")
            
                    for i, (nm, _) in enumerate(PNL_CODES):
                        pct  = nm.endswith("%")
                        digs = 2 if ("Net" in nm or pct) else 1
                        tbl.iloc[i, 1:] = [fmt(v, pct, digs) for v in tbl.iloc[i, 1:]]
            
                    st.dataframe(tbl.set_index("–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å"),
                                 use_container_width=True,
                                 height=min(880, 40 * len(tbl)))
            
                    # --- –≥—Ä–∞—Ñ–∏–∫: –≤—ã—Ä—É—á–∫–∞ / EBIT / —á–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å + EBIT-margin ---
                    fig, ax1 = plt.subplots(figsize=(7, 3.5))
                    x = np.arange(len(YEARS)); w = 0.25
                    bars_r  = ax1.bar(x - w, [calc[y]["2110"] or 0 for y in YEARS], w, label="–í—ã—Ä—É—á–∫–∞")
                    bars_e  = ax1.bar(x,     [calc[y]["2200"] or 0 for y in YEARS], w, label="EBIT")
                    bars_p  = ax1.bar(x + w, [calc[y]["2400"] or 0 for y in YEARS], w, label="–ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å")
            
                    for b in (*bars_r, *bars_e, *bars_p):
                        h = b.get_height()
                        if h and not np.isnan(h):
                            ax1.annotate(f"{h:.1f}", xy=(b.get_x()+b.get_width()/2, h),
                                         xytext=(0,3), textcoords="offset points",
                                         ha="center", fontsize=8)
            
                    ax2 = ax1.twinx()
                    m_vals = [calc[y]["_ebit_margin"] if calc[y]["_ebit_margin"] else np.nan for y in YEARS]
                    ax2.plot(x, m_vals, linestyle="--", marker="o", label="EBIT margin, %")
                    # ----- –µ–¥–∏–Ω–∞—è –ª–µ–≥–µ–Ω–¥–∞ -----
                    h1, l1 = ax1.get_legend_handles_labels()   # bars
                    h2, l2 = ax2.get_legend_handles_labels()   # –ª–∏–Ω–∏—è margin
                    ax1.legend(h1 + h2, l1 + l2, loc='upper left', fontsize=9)
            
            
                    
                    for xx, yy in zip(x, m_vals):
                        if yy and not np.isnan(yy):
                            ax2.annotate(f"{yy:.1f}%", xy=(xx, yy),
                                         xytext=(0,5), textcoords="offset points",
                                         ha="center", fontsize=8)
            
                    ax1.set_xticks(x); ax1.set_xticklabels(YEARS, fontsize=10)
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                     # ‚üµ  –ø—Ä—è—á–µ–º —à–∫–∞–ª—ã Y
                    ax1.set_yticks([]); ax2.set_yticks([])
                    ax1.set_ylim(bottom=0); ax2.set_ylim(bottom=0)
                    
                    for ax in (ax1, ax2):
                        ax.spines[:].set_visible(False)
                    for ax in (ax1, ax2): ax.spines[:].set_visible(False)
                    ax1.legend(loc="upper left", fontsize=9)
                    fig.tight_layout(pad=1.0)
                    st.pyplot(fig)
            
                    
                                      
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ (INVEST SNAPSHOT, –±–µ–∑ –∏–Ω—Ç–µ—Ä–≤—å—é –≤ –≤—ã–≤–æ–¥–µ) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    
                    # —Å—Ç—Ä–∞—Ö–æ–≤–∫–∞: –µ—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω/—Ñ—É–Ω–∫—Ü–∏—è –µ—â—ë –Ω–µ –æ–±—ä—è–≤–ª–µ–Ω—ã ‚Äî –æ–±—ä—è–≤–∏–º –∑–¥–µ—Å—å
                    try:
                        _SEC_INTERV_RE
                    except NameError:
                        import re
                        _SEC_INTERV_RE = re.compile(
                            r"(^|\n)###\s*–ò–Ω—Ç–µ—Ä–≤—å—é[^\n]*\n.*?(?=\n###\s|\Z)", flags=re.S | re.I
                        )
                    
                    def strip_interviews_section(md: str) -> str:
                        """–£–±–∏—Ä–∞–µ—Ç –∏–∑ Markdown –±–ª–æ–∫ '### –ò–Ω—Ç–µ—Ä–≤—å—é ‚Ä¶' —Ü–µ–ª–∏–∫–æ–º (–¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ ### –∏–ª–∏ –∫–æ–Ω—Ü–∞)."""
                        if not md:
                            return ""
                        cleaned = _SEC_INTERV_RE.sub("\n", md).strip()
                        # —Å–∂–∏–º–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                        return re.sub(r"\n{3,}", "\n\n", cleaned)
                    
                    # 1) –≥–æ—Ç–æ–≤–∏–º people –∏–∑ Checko –¥–ª—è —ç—Ç–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–ª—å—à–µ –≤ –¥–≤—É—Ö –º–µ—Å—Ç–∞—Ö)
                    company_info_row = {
                        "leaders_raw":  (df_companies.loc[idx, "leaders_raw"]  if "leaders_raw"  in df_companies.columns else []) or [],
                        "founders_raw": (df_companies.loc[idx, "founders_raw"] if "founders_raw" in df_companies.columns else []) or [],
                    }
                    
                    with st.spinner("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º INVEST SNAPSHOT‚Ä¶"):
                        inv = get_invest_snapshot(
                            cmp_name,
                            site_hint=site,
                            model="sonar", recency=None, max_tokens=1500
                        )
                        # –≤—ã—Ä–µ–∑–∞–µ–º —Ä–∞–∑–¥–µ–ª ¬´–ò–Ω—Ç–µ—Ä–≤—å—é ‚Ä¶¬ª –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è, —á—Ç–æ–±—ã –æ–Ω –Ω–µ –ø–æ–ø–∞–¥–∞–ª –≤ –±–ª–æ–∫ ¬´–û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏¬ª
                        inv_clean = strip_interviews_section(inv["md"])
                        inv_html  = linkify_keep_url(inv_clean)
                    
                        st.markdown(
                            f"<div style='background:#F7F9FA;border:1px solid #ccc;border-radius:8px;padding:18px;line-height:1.55'>{inv_html}</div>",
                            unsafe_allow_html=True,
                        )
                        doc = {"summary": inv_clean, "mode": "invest_snapshot"}
                    
                    with st.expander("üîß –û—Ç–ª–∞–¥–∫–∞ (—Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç)"):
                        st.text(inv.get("raw") or "‚Äî")
                    
                    
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç (MARKET EVIDENCE) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    if mkt:
                        st.subheader("üìà –†—ã–Ω–æ—á–Ω—ã–π –æ—Ç—á—ë—Ç")
                        legacy = st.toggle("Legacy (Google/GPT) mode", value=False, key=f"legacy_{idx}")
                    
                        if legacy:
                            with st.spinner("–°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ä—ã–Ω–∫—É (Legacy)‚Ä¶"):
                                mkt_res = get_market_rag(mkt)
                            mkt_html = _linkify(mkt_res["summary"]).replace("\n", "<br>")
                            st.markdown(
                                f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;border-radius:8px;padding:18px;line-height:1.55'>{mkt_html}</div>",
                                unsafe_allow_html=True,
                            )
                            with st.expander("‚öôÔ∏è –ó–∞–ø—Ä–æ—Å—ã –∫ Google"):
                                for i, q in enumerate(mkt_res["queries"], 1):
                                    st.markdown(f"**{i}.** {q}")
                            with st.expander("üîç –°–Ω–∏–ø–ø–µ—Ç—ã (top-15)"):
                                df_leg = pd.DataFrame(mkt_res["snippets"], columns=["URL", "Snippet"]).head(15)
                                st.dataframe(df_leg, use_container_width=True)
                        else:
                            with st.spinner("–°–æ–±–∏—Ä–∞–µ–º MARKET EVIDENCE‚Ä¶"):
                                ev = get_market_evidence(mkt, country="–†–æ—Å—Å–∏—è", min_sources=8, recency=None)
                            st.markdown(
                                f"<div style='background:#F1F5F8;border:1px solid #cfd9e2;border-radius:8px;padding:18px;line-height:1.55'>{ev['text_html']}</div>",
                                unsafe_allow_html=True,
                            )
                            st.caption("–°–¢–†–£–ö–¢–£–†–ê –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º ‚Äî –¥–µ–Ω—å–≥–∏:")
                            st.code(ev["money_block"] or "‚Äî", language="text")
                            st.caption("–°–¢–†–£–ö–¢–£–†–ê –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º ‚Äî –Ω–∞—Ç—É—Ä–∞–ª—å–Ω—ã–µ –æ–±—ä—ë–º—ã:")
                            st.code(ev["natural_block"] or "‚Äî", language="text")
                            with st.expander("üîß –û—Ç–ª–∞–¥–∫–∞ (—Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç)"):
                                st.text(ev["raw_text"] or "‚Äî")
                    
                    
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    st.subheader("üë• –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–∏ –∏ –∏–Ω—Ç–µ—Ä–≤—å—é")
                    
                    company_info_row = {
                        "leaders_raw":  (df_companies.loc[idx, "leaders_raw"]  if "leaders_raw"  in df_companies.columns else []) or [],
                        "founders_raw": (df_companies.loc[idx, "founders_raw"] if "founders_raw" in df_companies.columns else []) or [],
                    }
                    
                    with st.spinner("–ò—â–µ–º –∏–Ω—Ç–µ—Ä–≤—å—é (Checko ‚Üí –∏–Ω—Ç–µ—Ä–Ω–µ—Ç)‚Ä¶"):
                        dual = build_dual_interviews_from_v2(
                            cmp_name, company_info=company_info_row, site_hint=site, market=mkt
                        )
                    
                    # ===== 1) –ì–µ–Ω–¥–∏—Ä–µ–∫—Ç–æ—Ä + –ê–∫—Ü–∏–æ–Ω–µ—Ä—ã (—á–∏—Å—Ç—ã–π Markdown) =====
                    ceo = dual.get("ceo") or {}
                    fio_ceo = (ceo.get("fio") or "").strip()
                    if fio_ceo:
                        inn_txt = f"(–ò–ù–ù {ceo.get('inn')})" if ceo.get("inn") else ""
                        st.markdown(f"**–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä:** {fio_ceo} {inn_txt}".strip())
                    else:
                        st.markdown("_–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö_")
                    
                    st.markdown("")
                    st.markdown("**–ê–∫—Ü–∏–æ–Ω–µ—Ä—ã**")
                    
                    shareholders = dual.get("shareholders") or []
                    st.markdown(_markdown_shareholders_table(shareholders))
                    
                    # ===== 2) –ò–Ω—Ç–µ—Ä–≤—å—é (HTML —Å linkify_keep_url) =====
                    digest_checko = sanitize_invest(dual.get("digest_checko") or "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
                    digest_inet   = sanitize_invest(dual.get("digest_inet")   or "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö")
                    
                    blocks = []
                    if digest_checko.strip().lower() != "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö":
                        dig_checko_html = linkify_keep_url(digest_checko).replace("\n", "<br>")
                        blocks.append(
                            "<h4 style='margin:12px 0 6px'>–î–∞–π–¥–∂–µ—Å—Ç –∏–Ω—Ç–µ—Ä–≤—å—é ‚Äî Checko</h4>"
                            f"<div>{dig_checko_html}</div>"
                        )
                    
                    if digest_inet.strip().lower() != "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö":
                        dig_inet_html = linkify_keep_url(digest_inet).replace("\n", "<br>")
                        blocks.append(
                            "<h4 style='margin:14px 0 6px'>–î–∞–π–¥–∂–µ—Å—Ç –∏–Ω—Ç–µ—Ä–≤—å—é ‚Äî –∏–Ω—Ç–µ—Ä–Ω–µ—Ç</h4>"
                            f"<div>{dig_inet_html}</div>"
                        )
                    
                    if blocks:
                        st.markdown(
                            "<div style='background:#F9FAFB;border:1px solid #ddd;border-radius:8px;padding:18px;line-height:1.6'>"
                            + "".join(blocks) +
                            "</div>",
                            unsafe_allow_html=True,
                        )

                # === Q&A helpers (–≤—Å—Ç–∞–≤–∏—Ç—å –û–î–ò–ù –†–ê–ó –≤—ã—à–µ UI) ===
                def _kb_collect_sections_for_company(cmp_name: str,
                                                     leaders_md: str = "",
                                                     digest_checko: str = "",
                                                     digest_inet: str = "") -> list[tuple[str,str]]:
                    kb = []
                    if leaders_md.strip():
                        kb.append(("Leaders & Shareholders", leaders_md))
                    if digest_checko.strip():
                        kb.append(("Interviews (Checko)", digest_checko))
                    if digest_inet.strip():
                        kb.append(("Interviews (Internet)", digest_inet))
                    return kb
                
                def _kb_simple_rank(query: str, sections: list[tuple[str,str]], top_k: int = 3) -> list[tuple[str,str,float]]:
                    import re
                    q = [t for t in re.findall(r"\w+", (query or "").lower()) if len(t) > 2]
                    scored = []
                    for title, txt in sections:
                        tokens = re.findall(r"\w+", (txt or "").lower())
                        score = sum(tokens.count(t) for t in q)
                        if score > 0:
                            scored.append((title, txt, float(score)))
                    scored.sort(key=lambda x: x[2], reverse=True)
                    return scored[:top_k]
                
                _EMP_PATTERNS = [
                    r"—á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å\s+—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤[:\s\-~]*([0-9\s]+)\s*(?:—á–µ–ª|employees|—Å–æ—Ç—Ä|—Å–æ—Ç—Ä—É–¥)\b",
                    r"employees[:\s\-~]*([0-9\s]+)\b",
                ]
                
                def _extract_employee_count(text: str) -> tuple[int | None, str | None]:
                    import re
                    t = (text or "")
                    for pat in _EMP_PATTERNS:
                        m = re.search(pat, t, flags=re.I)
                        if m:
                            raw = re.sub(r"\D", "", m.group(1) or "")
                            if raw.isdigit():
                                return int(raw), "regex"
                    return None, None
                
                def _qa_prompt_for_web(company: str, user_q: str, site_hint: str | None = None) -> str:
                    site = f"–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç: {site_hint}. " if site_hint else ""
                    return f"""–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å. –ù–∞–π–¥–∏ —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ –∫–æ–º–ø–∞–Ω–∏—é ¬´{company}¬ª.
                {site}–î–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ñ–∞–∫—Ç –∏ –¥–∞—Ç—É/–ø–µ—Ä–∏–æ–¥, –µ—Å–ª–∏ –≤–∞–∂–Ω–æ. –í–°–ï–ì–î–ê –ø—Ä–∏–≤–æ–¥–∏ –ü–†–Ø–ú–´–ï URL (2‚Äì4).
                ANSWER: <–∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç>
                DETAILS: <1‚Äì3 —É—Ç–æ—á–Ω–µ–Ω–∏—è>
                SOURCES: <URL1>; <URL2>; <URL3>
                Q: {user_q}""".strip()
                
                def ask_guide(company: str, user_q: str, kb_sections: list[tuple[str,str]],
                              site_hint: str | None = None, allow_web: bool = True) -> dict:
                    # 1) –ª–æ–∫–∞–ª—å–Ω–æ
                    top_local = _kb_simple_rank(user_q, kb_sections, top_k=3)
                    merged = "\n\n".join(sec for _, sec, _ in top_local) if top_local else ""
                    emp_local, how = _extract_employee_count(merged)
                    if emp_local:
                        return {
                            "answer_md": f"**–û—Ç–≤–µ—Ç:** {emp_local:,} —á–µ–ª.".replace(",", " "),
                            "used": "local",
                            "sources": [],
                            "raw": f"LOCAL({how})",
                            "suggest_patch": {"section":"INVEST SNAPSHOT","md_line": f"**–ß–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å:** {emp_local:,} —á–µ–ª.".replace(",", " ")},
                        }
                    # 2) –≤–µ–±
                    if allow_web:
                        prompt = _qa_prompt_for_web(company, user_q, site_hint)
                        raw = _pplx_call_invest(prompt, model="sonar", recency=None, max_tokens=800)
                        cleaned = sanitize_invest(raw)
                        urls = _extract_urls(cleaned)
                        emp_web, _ = _extract_employee_count(cleaned)
                        md_ans = cleaned
                        suggest = None
                        if emp_web:
                            md_ans = f"**–û—Ç–≤–µ—Ç:** {emp_web:,} —á–µ–ª.\n\n{cleaned}".replace(",", " ")
                            suggest = {"section":"INVEST SNAPSHOT","md_line": f"**–ß–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å:** {emp_web:,} —á–µ–ª.  \n–ò—Å—Ç–æ—á–Ω–∏–∫–∏: " + "; ".join(urls[:3])}
                        return {"answer_md": md_ans, "used": "web", "sources": urls[:4], "raw": raw, "suggest_patch": suggest}
                    # 3) –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö
                    return {"answer_md":"_–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç_.","used":"none","sources":[],"raw":"","suggest_patch":None}
                
                def insert_or_append_line(md_text: str, section_title: str, new_line_md: str) -> str:
                    import re
                    if not new_line_md: return md_text or ""
                    text = md_text or ""
                    pat_h = re.compile(rf"(?m)^(#{1,6}\s*{re.escape(section_title)}\s*$)")
                    pat_b = re.compile(rf"(?m)^\*\*{re.escape(section_title)}\*\*\s*$")
                    m = pat_h.search(text) or pat_b.search(text)
                    if m:
                        pos = m.end()
                        return text[:pos] + "\n" + new_line_md.strip() + "\n\n" + text[pos:]
                    return (text + f"\n\n## {section_title}\n{new_line_md.strip()}\n").strip()

                    
                    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Q&A: –°–ø—Ä–æ—Å–∏—Ç—å —Å–ø—Ä–∞–≤–∫—É ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                    st.markdown("---")
                    st.subheader("üîé –°–ø—Ä–æ—Å–∏—Ç—å —Å–ø—Ä–∞–≤–∫—É")
                    
                    user_q = st.text_input(
                        "–í–∞—à –≤–æ–ø—Ä–æ—Å –ø–æ –∫–æ–º–ø–∞–Ω–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä: ¬´–Ω–∞–π–¥–∏ —á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤¬ª)",
                        key=f"qa_{idx}"
                    )
                    
                    # –°–æ–±–∏—Ä–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–æ—Ä–ø—É—Å —Ç–æ–ª—å–∫–æ –∏–∑ —Ç–æ–≥–æ, —á—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –µ—Å—Ç—å
                    kb_sections = []
                    # –ë–ª–æ–∫ –ª–∏–¥–µ—Ä–æ–≤/–∞–∫—Ü–∏–æ–Ω–µ—Ä–æ–≤: —Å–æ–±–µ—Ä—ë–º –ø—Ä–æ—Å—Ç–æ–π md
                    leaders_md = ""
                    if fio_ceo:
                        inn_txt = f"(–ò–ù–ù {ceo.get('inn')})" if ceo.get("inn") else ""
                        leaders_md += f"–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä: {fio_ceo} {inn_txt}\n"
                    if shareholders:
                        for r in shareholders:
                            fio = (r.get("fio") or "").strip()
                            inn = r.get("inn") or ""
                            share = r.get("share_pct")
                            try:
                                share_f = float(str(share).replace(",", ".")) if share is not None else None
                                if share_f is not None and 0 < share_f <= 1.0:
                                    share_f *= 100.0
                            except Exception:
                                share_f = None
                            share_txt = (f"{share_f:.4g}".rstrip('0').rstrip('.') + "%") if share_f is not None else ""
                            leaders_md += f"- {fio}{f' (–ò–ù–ù {inn})' if inn else ''}{f' ‚Äî {share_txt}' if share_txt else ''}\n"
                    if leaders_md:
                        kb_sections.append(("Leaders & Shareholders", leaders_md))
                    
                    # –∏–Ω—Ç–µ—Ä–≤—å—é
                    if isinstance(digest_checko, str) and digest_checko.strip().lower() != "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö":
                        kb_sections.append(("Interviews (Checko)", digest_checko))
                    if isinstance(digest_inet, str) and digest_inet.strip().lower() != "–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö":
                        kb_sections.append(("Interviews (Internet)", digest_inet))
                    
                    # –∫–Ω–æ–ø–∫–∏
                    col_qa1, col_qa2 = st.columns([1,1])
                    
                    if col_qa1.button("–ò—Å–∫–∞—Ç—å –æ—Ç–≤–µ—Ç", key=f"qa_go_{idx}") and (user_q or "").strip():
                        try:
                            qa = ask_guide(
                                company=cmp_name,
                                user_q=user_q.strip(),
                                kb_sections=kb_sections,
                                site_hint=site,
                                allow_web=True
                            )
                            ans_html = linkify_keep_url(qa.get("answer_md") or "").replace("\n", "<br>")
                            st.markdown(
                                f"<div style='background:#F6F8FA;border:1px solid #e2e8f0;border-radius:8px;padding:14px;line-height:1.6'>{ans_html}</div>",
                                unsafe_allow_html=True
                            )
                            if qa.get("sources"):
                                st.caption("–ò—Å—Ç–æ—á–Ω–∏–∫–∏: " + " ‚Ä¢ ".join(qa["sources"]))
                    
                            # –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –≤—Å—Ç—Ä–∞–∏–≤–∞–Ω–∏–µ –≤ —Å–ø—Ä–∞–≤–∫—É (session_state), –±–µ–∑ –ø—Ä–∏–≤—è–∑–∫–∏ –∫ inv
                            suggest = qa.get("suggest_patch")
                            if suggest and col_qa2.button(f"–í—Å—Ç–∞–≤–∏—Ç—å –≤ —Ä–∞–∑–¥–µ–ª: {suggest['section']}", key=f"qa_apply_{idx}"):
                                ss_key = f"guide_section_{suggest['section']}_{idx}"
                                cur_md = st.session_state.get(ss_key, "")
                                new_md = insert_or_append_line(cur_md, suggest["section"], suggest["md_line"])
                                st.session_state[ss_key] = new_md  # –º–æ–∂–Ω–æ –ø–æ—Ç–æ–º –æ—Ç—Ä–µ–Ω–¥–µ—Ä–∏—Ç—å –≥–¥–µ –Ω—É–∂–Ω–æ
                                st.success("–î–æ–±–∞–≤–ª–µ–Ω–æ –≤ –ª–æ–∫–∞–ª—å–Ω—É—é –∫–æ–ø–∏—é —Å–ø—Ä–∞–≤–∫–∏ (session_state).")
                        except Exception as e:
                            st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫: {e}")




# === BACKGROUND / PROGRESS =====================================================
import os, time, re, json, io
import requests
from datetime import date, timedelta
from typing import Optional, Tuple, List, Iterable
import streamlit as st

def long_job(total_sec: int = 180, key_prog: str = "ai_prog"):
    """–§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞, –∫–∞–∂–¥—ã–µ 1 —Å –æ–±–Ω–æ–≤–ª—è–µ—Ç progress –≤ session_state."""
    for i in range(total_sec + 1):
        time.sleep(1)
        st.session_state[key_prog] = i / total_sec     # 0 ‚Ä¶ 1
    st.session_state["ai_done"] = True                 # –æ—Ç—á—ë—Ç –≥–æ—Ç–æ–≤

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 2. UI-—Ñ—É–Ω–∫—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –≤–∫–ª–∞–¥–∫–∏ "Advance Eye" (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def run_advance_eye_tab() -> None:
    st.header("üëÅÔ∏è Advance Eye")

    user_query = st.text_input("–í–≤–µ–¥–∏—Ç–µ –ò–ù–ù –∏–ª–∏ –§–ò–û")
    if st.button("üîç –ù–∞–π—Ç–∏ –∫–æ–Ω—Ç–∞–∫—Ç—ã") and user_query:
        with st.spinner("–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º Dyxless‚Ä¶"):
            res = dyxless_query(user_query, token=DYXLESS_TOKEN, max_rows=20_000)

        if res.get("status"):
            st.success(f"–ü–æ–∫–∞–∑–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: **{res['counts']}**")
            st.json(res["data"] or {"note": "–ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"})
        else:
            st.warning(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {res.get('error', '–ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç')}")

# === Q&A over Guide: –ª–æ–∫–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ ‚Üí –≤–µ–±-—Ñ–æ–ª–±—ç–∫ —á–µ—Ä–µ–∑ Sonar ==================
# (–æ—Å—Ç–∞–≤–ª–µ–Ω–æ –∫–∞–∫ —É –≤–∞—Å; –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–µ—Ç–µ –ø—Ä–∞–≤–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ)
from typing import Tuple

def _kb_collect_sections_for_company(
    cmp_name: str,
    inv_md: str | None = None,
    owners_md: str | None = None,
    leaders_md: str | None = None,
    interviews_md: str | None = None,
    extra_sections: dict[str, str] | None = None,
) -> list[tuple[str, str]]:
    kb: list[tuple[str, str]] = []
    if inv_md:         kb.append(("INVEST SNAPSHOT", inv_md))
    if owners_md:      kb.append(("Owners/Beneficiaries", owners_md))
    if leaders_md:     kb.append(("Leaders & Shareholders", leaders_md))
    if interviews_md:  kb.append(("Interviews", interviews_md))
    if extra_sections:
        for k, v in extra_sections.items():
            if v: kb.append((k, v))
    return kb

def _kb_simple_rank(query: str, sections: list[tuple[str,str]], top_k: int = 3) -> list[tuple[str,str,float]]:
    import re
    q = [t for t in re.findall(r"\w+", (query or "").lower()) if len(t) > 2]
    scored = []
    for title, txt in sections:
        tokens = re.findall(r"\w+", (txt or "").lower())
        score = sum(tokens.count(t) for t in q)
        if score > 0:
            scored.append((title, txt, float(score)))
    scored.sort(key=lambda x: x[2], reverse=True)
    return scored[:top_k]

_EMP_PATTERNS = [
    r"—á–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å\s+—Å–æ—Ç—Ä—É–¥–Ω–∏–∫–æ–≤[:\s\-~]*([0-9\s]+)\s*(?:—á–µ–ª|—Åotr|—Å–æ—Ç—Ä—É–¥|employees)\b",
    r"employees[:\s\-~]*([0-9\s]+)\b",
    r"–ø–µ—Ä—Å–æ–Ω–∞–ª[:\s\-~]*([0-9\s]+)\s*(?:—á–µ–ª|—Å–æ—Ç—Ä—É–¥)\b",
]

def _extract_employee_count(text: str) -> tuple[int | None, str | None]:
    import re
    t = (text or "")
    for pat in _EMP_PATTERNS:
        m = re.search(pat, t, flags=re.I)
        if m:
            raw = re.sub(r"\D", "", m.group(1) or "")
            if raw.isdigit():
                return int(raw), "regex"
    return None, None

def _qa_prompt_for_web(company: str, user_q: str, site_hint: str | None = None) -> str:
    site = f"–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç: {site_hint}. " if site_hint else ""
    return f"""
–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç-–∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å. –ù–∞–π–¥–∏ —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ –∫–æ–º–ø–∞–Ω–∏—é ¬´{company}¬ª. 
{site}–î–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Ñ–∞–∫—Ç –∏ –¥–∞—Ç—É/–ø–µ—Ä–∏–æ–¥, –µ—Å–ª–∏ –≤–∞–∂–Ω–æ. –í–°–ï–ì–î–ê –ø—Ä–∏–≤–æ–¥–∏ –ü–†–Ø–ú–´–ï URL –Ω–∞ –ø–µ—Ä–≤–æ–∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–Ω–µ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã), 2‚Äì4 —Å—Å—ã–ª–∫–∏.
–§–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞ —Å—Ç—Ä–æ–≥–æ —Ç–∞–∫–æ–π:
ANSWER: <–∫—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –≤ –æ–¥–Ω—É-–¥–≤–µ —Å—Ç—Ä–æ–∫–∏>
DETAILS: <1‚Äì3 –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–æ—è—Å–Ω–µ–Ω–∏—è/—É—Ç–æ—á–Ω–µ–Ω–∏—è, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ>
SOURCES: <URL1>; <URL2>; <URL3>
Q: {user_q}
""".strip()

def ask_guide(
    company: str,
    user_q: str,
    kb_sections: list[tuple[str,str]],
    site_hint: str | None = None,
    allow_web: bool = True,
) -> dict:
    top_local = _kb_simple_rank(user_q, kb_sections, top_k=3)
    merged_local = "\n\n".join(sec for _, sec, _ in top_local) if top_local else ""
    emp_local, how = _extract_employee_count(merged_local)
    if emp_local:
        ans = f"**–û—Ç–≤–µ—Ç:** {emp_local:,} —á–µ–ª.".replace(",", " ")
        return {
            "answer_md": ans,
            "used": "local",
            "sources": [],
            "raw": f"LOCAL({how})",
            "suggest_patch": {
                "section": "INVEST SNAPSHOT",
                "md_line": f"**–ß–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å:** {emp_local:,} —á–µ–ª.",
            },
        }

    if allow_web:
        prompt = _qa_prompt_for_web(company, user_q, site_hint)
        raw = _pplx_call_invest(prompt, model="sonar", recency=None, max_tokens=800)
        cleaned = sanitize_invest(raw)
        urls = _extract_urls(cleaned)
        emp_web, _ = _extract_employee_count(cleaned)
        md_ans = cleaned
        suggest = None
        if emp_web:
            md_ans = f"**–û—Ç–≤–µ—Ç:** {emp_web:,} —á–µ–ª.\n\n{cleaned}".replace(",", " ")
            suggest = {"section":"INVEST SNAPSHOT", "md_line": f"**–ß–∏—Å–ª–µ–Ω–Ω–æ—Å—Ç—å:** {emp_web:,} —á–µ–ª.  \n–ò—Å—Ç–æ—á–Ω–∏–∫–∏: " + "; ".join(urls[:3])}
        return {
            "answer_md": md_ans,
            "used": "web",
            "sources": urls[:4],
            "raw": raw,
            "suggest_patch": suggest,
        }

    return {
        "answer_md": "_–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –Ω–∏ –ª–æ–∫–∞–ª—å–Ω–æ, –Ω–∏ –≤ –≤–µ–±–µ._",
        "used": "none",
        "sources": [],
        "raw": "",
        "suggest_patch": None,
    }

def insert_or_append_line(md_text: str, section_title: str, new_line_md: str) -> str:
    import re
    if not new_line_md:
        return md_text or ""
    text = md_text or ""
    pat_h1 = re.compile(rf"(?m)^(#{1,6}\s*{re.escape(section_title)}\s*$)")
    pat_b  = re.compile(rf"(?m)^\*\*{re.escape(section_title)}\*\*\s*$")

    def _add_after(pos: int) -> str:
        before = text[:pos]
        after  = text[pos:]
        return before + "\n" + new_line_md.strip() + "\n\n" + after

    m = pat_h1.search(text) or pat_b.search(text)
    if m:
        insert_pos = m.end()
        return _add_after(insert_pos)
    block = f"\n\n## {section_title}\n{new_line_md.strip()}\n"
    return (text + block).strip()

# ==============================================================================
# === NEWS RUN (last N days, multi-company, NO auto-matrix, NO ```text) ========
# ==============================================================================
import re, io
import pandas as pd
from datetime import date, timedelta
from typing import Optional, Tuple, List, Iterable    


API_URL_PPLX_NEWS = "https://api.perplexity.ai/chat/completions"

# ==============================================================================
# === NEWS RUN (—Ç–∞–±–ª–∏—Ü–∞, N=15 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é, multi-company) =====================
# ==============================================================================

import re, io
import pandas as pd
from datetime import date, timedelta
from typing import Optional, Tuple, List, Iterable

API_URL_PPLX_NEWS = "https://api.perplexity.ai/chat/completions"

class PPLXNewsError(Exception):
    pass

def _get_pplx_key_news() -> str:
    key = (os.getenv("SONAR_API_KEY") or os.getenv("PPLX_API_KEY") or
           os.getenv("PERPLEXITY_API_KEY") or st.secrets.get("SONAR_API_KEY", "")).strip()
    if (not key.startswith("pplx-")) or (len(key) < 40) or key.endswith("..."):
        raise PPLXNewsError("Perplexity API key –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ –∑–∞–≥–ª—É—à–∫–∞. –ó–∞–¥–∞–π pplx-<‚Ä¶> (–¥–ª–∏–Ω–Ω—ã–π).")
    return key

def call_pplx_news(
    prompt: str,
    model: str = "sonar",
    recency: Optional[str] = None,    # 'week' / 'month' / 'year'
    temperature: float = 0.0,
    max_tokens: int = 1600,
    timeout: int = 60,
) -> str:
    headers = {
        "Authorization": f"Bearer {_get_pplx_key_news()}",
        "Content-Type": "application/json",
        "Accept": "application/json",
        "User-Agent": "news-run-multi/1.1",
    }
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": (
                "–¢—ã ‚Äî —Ä–µ–¥–∞–∫—Ç–æ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π –∏ —Ñ–∞–∫—Ç-—á–µ–∫–µ—Ä. –í–æ–∑–≤—Ä–∞—â–∞–π —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º—ã–µ —Ñ–∞–∫—Ç—ã —Å –ü–†–Ø–ú–´–ú–ò URL. "
                "–ù–∏–∫–æ–≥–¥–∞ –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π —Å—Å—ã–ª–∫–∏ –∏ –Ω–æ–≤–æ—Å—Ç–∏. –§–æ—Ä–º–∞—Ç —Å—Ç—Ä–æ–≥–æ –ø–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –ë–µ–∑ –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤/–ò–ù–ù/–û–ì–†–ù."
            )},
            {"role": "user", "content": prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    if recency in {"hour", "day", "week", "month", "year"}:
        payload["search_recency_filter"] = recency

    r = requests.post(API_URL_PPLX_NEWS, headers=headers, json=payload, timeout=timeout)
    if r.status_code != 200:
        try:
            err = r.json()
        except Exception:
            err = {"error": r.text[:800]}
        raise PPLXNewsError(f"HTTP {r.status_code}: {json.dumps(err, ensure_ascii=False)[:800]}")
    return r.json()["choices"][0]["message"]["content"]

# ---- sanitize / split / dedup -------------------------------------------------
_FORBIDDEN_NEWS = re.compile(r"(–∞–∫—Ü–∏–æ–Ω–µ—Ä|–≤–ª–∞–¥–µ–ª—å—Ü|–±–µ–Ω–µ—Ñ–∏—Ü–∏–∞—Ä|–∏–Ω–Ω|–æ–≥—Ä–Ω)", re.IGNORECASE)

def sanitize_news(text: str) -> str:
    lines = []
    for ln in (text or "").splitlines():
        if _FORBIDDEN_NEWS.search(ln):
            continue
        lines.append(ln.rstrip())
    out, blank = [], False
    for ln in lines:
        if not ln.strip():
            if not blank:
                out.append("")
            blank = True
        else:
            out.append(ln)
            blank = False
    return "\n".join(out).strip()

_PAR_SPLIT_NEWS = re.compile(r"(?=^–ò—Å—Ç–æ—á–Ω–∏–∫:\s*)", flags=re.IGNORECASE | re.MULTILINE)

def _normalize_companies_news(company: Iterable[str] | str | None) -> List[str]:
    if company is None:
        return []
    if isinstance(company, str):
        parts = re.split(r"[;,|\n]", company)
        comps = [p.strip() for p in parts if p.strip()]
    else:
        comps = [str(x).strip() for x in company if str(x).strip()]
    seen, out = set(), []
    for c in comps:
        cl = c.lower()
        if cl not in seen:
            seen.add(cl)
            out.append(c)
    return out

# -------- –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç–∞: –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ —Å –∫–ª—é—á–∞–º–∏ —á–µ—Ä–µ–∑ " | " --------------
def build_news_prompt_window(
    company: str,
    country: str,
    since: date,
    until: date,
    min_items: int,
    keywords: list[str] | None = None,
) -> str:
    user_terms = [k.strip() for k in (keywords or []) if k and k.strip()]
    # –∫–æ–º–ø–∞–Ω–∏—è –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫–∞–∫ ¬´—è–∫–æ—Ä—å¬ª
    term_hint = ", ".join(dict.fromkeys([company] + user_terms))

    return f"""
–°–¥–µ–ª–∞–π –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –¥–∞–π–¥–∂–µ—Å—Ç –ø—Ä–æ ¬´{company}¬ª –≤ —Å—Ç—Ä–∞–Ω–µ {country}.
–í–∫–ª—é—á–∞–π –¢–û–õ–¨–ö–û –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [{since.isoformat()} ‚Ä¶ {until.isoformat()}] –≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ.
–ò—â–∏ –ø–æ —Ç–µ—Ä–º–∏–Ω–∞–º: {term_hint}

–°–¢–†–û–ì–û –û–î–ù–ê –°–¢–†–û–ö–ê –Ω–∞ –º–∞—Ç–µ—Ä–∏–∞–ª, –Ω–∞—á–∏–Ω–∞–π —Å '–ò—Å—Ç–æ—á–Ω–∏–∫:' –∏ –∏—Å–ø–æ–ª—å–∑—É–π –∫–ª—é—á–∏ –≤ —Ç–∞–∫–æ–º –ø–æ—Ä—è–¥–∫–µ (—á–µ—Ä–µ–∑ ' | '):
–ò—Å—Ç–æ—á–Ω–∏–∫: <–∏–∑–¥–∞—Ç–µ–ª—å> | –î–∞—Ç–∞: <YYYY-MM-DD> | –ó–∞–≥–æ–ª–æ–≤–æ–∫: <title> | –ö–ª—é—á–µ–≤–æ–µ: <1‚Äì2 —Ñ–∞–∫—Ç–∞/—Ü–∏—Ñ—Ä—ã> | URL: <–ø—Ä—è–º–æ–π_–ª–∏–Ω–∫>

‚Äî –î–∞–π –º–∏–Ω–∏–º—É–º {min_items} –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ (–µ—Å–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –º–µ–Ω—å—à–µ ‚Äî –≤–µ—Ä–Ω–∏ —Å–∫–æ–ª—å–∫–æ –µ—Å—Ç—å, –Ω–æ –Ω–µ –≤—ã—Ö–æ–¥–∏ –∑–∞ –ø—Ä–µ–¥–µ–ª—ã –¥–∞—Ç—ã –∏ –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π).
‚Äî –°—Å—ã–ª–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –ø—Ä—è–º—ã–º–∏ (https://...).
‚Äî –ù–µ —É–ø–æ–º–∏–Ω–∞–π –≤–ª–∞–¥–µ–ª—å—Ü–µ–≤/–ò–ù–ù/–û–ì–†–ù.
""".strip()

# -------- —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –¥–µ–¥—É–ø –ø–æ –æ–∫–Ω—É/URL --------------------------------------
_DATE_RE_NEWS = re.compile(r"\b–î–∞—Ç–∞:\s*(\d{4}-\d{2}-\d{2})\b")
_URL_RE_NEWS  = re.compile(r"\bURL:\s*([^\s|]+)")
_PUB_RE       = re.compile(r"^–ò—Å—Ç–æ—á–Ω–∏–∫:\s*(.*?)\s*\|", flags=re.I|re.S)
_TITLE_RE     = re.compile(r"\b–ó–∞–≥–æ–ª–æ–≤–æ–∫:\s*(.*?)\s*\|\s*–ö–ª—é—á–µ–≤–æ–µ:", flags=re.I|re.S)
_KEY_RE       = re.compile(r"\b–ö–ª—é—á–µ–≤–æ–µ:\s*(.*?)\s*\|\s*URL:", flags=re.I|re.S)

def _parse_date_safe_news(s: str) -> Optional[date]:
    try:
        y, m, d = map(int, s.split("-"))
        return date(y, m, d)
    except Exception:
        return None

def filter_output_by_window_news(text: str, since: date, until: date) -> str:
    kept = []
    for p in [p.strip() for p in _PAR_SPLIT_NEWS.split(text or "") if p.strip()]:
        m = _DATE_RE_NEWS.search(p)
        if not m:
            continue
        dt = _parse_date_safe_news(m.group(1))
        if dt and since <= dt <= until:
            kept.append(p)
    return "\n".join(kept)

def _dedup_by_url_news(paragraphs: List[str]) -> List[str]:
    seen, out = set(), []
    for p in paragraphs:
        m = _URL_RE_NEWS.search(p)
        url = m.group(1).strip() if m else ""
        if url and url not in seen:
            seen.add(url)
            out.append(p)
    return out

def _split_paragraphs_news(text: str) -> List[str]:
    return [p.strip() for p in _PAR_SPLIT_NEWS.split(text or "") if p.strip()]

# -------- –ø–∞—Ä—Å–∏–º –≤ —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã ---------------------------------------------
def parse_news_rows(text: str) -> list[dict]:
    rows = []
    for p in _split_paragraphs_news(text):
        try:
            dt = _DATE_RE_NEWS.search(p).group(1)
            title = _TITLE_RE.search(p).group(1).strip()
            key   = _KEY_RE.search(p).group(1).strip()
            url   = _URL_RE_NEWS.search(p).group(1).strip()
            pub_m = _PUB_RE.search(p)
            pub   = pub_m.group(1).strip() if pub_m else ""
            rows.append({"–î–∞—Ç–∞": dt, "–ó–∞–≥–æ–ª–æ–≤–æ–∫": title, "–ö–ª—é—á–µ–≤–æ–µ": key, "–°—Å—ã–ª–∫–∞": url, "–ò–∑–¥–∞—Ç–µ–ª—å": pub})
        except Exception:
            # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫—Ä–∏–≤–æ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–±–∑–∞—Ü
            continue
    return rows

# -------- –æ—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è: –≤—Å–µ–≥–¥–∞ —Ç–∞—Ä–≥–µ—Ç = 15 --------------------------------
def news_run_last_days(
    company: Iterable[str] | str | None = None,   # –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤–≤–æ–¥–∏—Ç
    country: str = "–†–æ—Å—Å–∏—è",
    last_days: int = 31,
    keywords: str | list[str] | None = None,
    model: str = "sonar",
) -> str:
    today = date.today()
    since = today - timedelta(days=last_days)
    companies = _normalize_companies_news(company)
    if not companies:
        raise ValueError("–°–ø–∏—Å–æ–∫ –∫–æ–º–ø–∞–Ω–∏–π –ø—É—Å—Ç. –£–∫–∞–∂–∏—Ç–µ –º–∏–Ω–∏–º—É–º –æ–¥–Ω—É –∫–æ–º–ø–∞–Ω–∏—é/—Å—É—â–Ω–æ—Å—Ç—å.")

    # —Ä–∞—Å–ø–∞—Ä—Å–∏–º keywords
    if isinstance(keywords, str):
        kw = [k.strip() for k in re.split(r"[;,|\n]", keywords) if k.strip()]
    else:
        kw = [str(k).strip() for k in (keywords or []) if str(k).strip()]

    TARGET = 15  # ‚Üê —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞ –≤–µ—Å—å –≤—ã–≤–æ–¥
    n = len(companies)
    base = max(1, TARGET // n)
    extra = max(0, TARGET - base * n)

    recency = "week" if last_days <= 7 else ("month" if last_days <= 31 else "year")

    all_pars: List[str] = []
    for idx, comp in enumerate(companies):
        need = base + (1 if idx < extra else 0)
        prompt = build_news_prompt_window(comp, country, since, today, need, keywords=kw)
        raw = call_pplx_news(prompt, model=model, recency=recency, max_tokens=1600)
        clean = sanitize_news(raw)
        filtered = filter_output_by_window_news(clean, since, today)
        if not filtered:
            continue
        all_pars.extend(_split_paragraphs_news(filtered))

    if not all_pars:
        return f"[–û–∫–Ω–æ –¥–∞—Ç: {since.isoformat()} ‚Äî {today.isoformat()}]\n\n–Ω–µ—Ç —Å–≤–µ–∂–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –∑–∞–¥–∞–Ω–Ω–æ–º –æ–∫–Ω–µ."

    # –¥–µ–¥—É–ø –ø–æ URL –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ TARGET
    all_pars = _dedup_by_url_news(all_pars)[:TARGET]

    header = f"[–û–∫–Ω–æ –¥–∞—Ç: {since.isoformat()} ‚Äî {today.isoformat()}] –ö–æ–º–ø–∞–Ω–∏–∏: {', '.join(companies)}"
    body = "\n".join(all_pars)
    return header + "\n\n" + body

# -------- UI –≤–∫–ª–∞–¥–∫–∞: —Ä–∏—Å—É–µ–º —Ç–∞–±–ª–∏—Ü—É —Å –∫–ª–∏–∫–∞–±–µ–ª—å–Ω–æ–π —Å—Å—ã–ª–∫–æ–π -------------------
def run_news_run_tab() -> None:
    st.header("üóû News Run")

    c1, c2, c3 = st.columns([2,1,1])
    with c1:
        companies_raw = st.text_area(
            "–ö–æ–º–ø–∞–Ω–∏–∏/—Å—É—â–Ω–æ—Å—Ç–∏ (–ø–æ –æ–¥–Ω–æ–π –≤ —Å—Ç—Ä–æ–∫–µ –∏–ª–∏ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)",
            placeholder="–ü—Ä–∏–º–µ—Ä—ã:\n–õ—É–∫–æ–π–ª\n–í–æ—Å—Ç–æ–∫ –û–π–ª\n–¢–ú–ö",
            height=140
        )
    with c2:
        last_days = st.number_input("–ó–∞ —Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π –∏—Å–∫–∞—Ç—å", min_value=1, max_value=365, value=31, step=1)
    with c3:
        country = st.text_input("–°—Ç—Ä–∞–Ω–∞", value="–†–æ—Å—Å–∏—è")

    keywords_raw = st.text_area(
        "–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–ø–æ –æ–¥–Ω–æ–º—É –≤ —Å—Ç—Ä–æ–∫–µ –∏–ª–∏ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)",
        placeholder="–∫–æ–Ω—Ç—Ä–∞–∫—Ç—ã\n—Ç–µ–Ω–¥–µ—Ä\n–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ\n—ç–∫—Å–ø–æ—Ä—Ç",
        height=120
    )

    col_run, col_dl_txt, col_dl_csv = st.columns([1,1,1])
    if col_run.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –¥–∞–π–¥–∂–µ—Å—Ç"):
        companies = [x.strip() for x in re.split(r"[;,|\n]", companies_raw or "") if x.strip()]
        if not companies:
            st.error("–£–∫–∞–∂–∏—Ç–µ –º–∏–Ω–∏–º—É–º –æ–¥–Ω—É –∫–æ–º–ø–∞–Ω–∏—é/—Å—É—â–Ω–æ—Å—Ç—å.")
            st.stop()
        keywords = [k.strip() for k in re.split(r"[;,|\n]", keywords_raw or "") if k.strip()]

        try:
            with st.spinner("–ò—â–µ–º –∏ –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º –Ω–æ–≤–æ—Å—Ç–∏‚Ä¶"):
                text = news_run_last_days(
                    company=companies,
                    country=country,
                    last_days=int(last_days),
                    keywords=keywords,
                    model="sonar",
                )

            # –ü–∞—Ä—Å–∏–º –≤ —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
            rows = parse_news_rows(text)
            if not rows:
                st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –≤ —Ç–∞–±–ª–∏—Ü—É. –ü–æ–∫–∞–∑—ã–≤–∞—é —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç.")
                st.code(text, language="markdown")
                return

            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∞—Ç–µ (–ø–æ —É–±—ã–≤–∞–Ω–∏—é)
            df = pd.DataFrame(rows)
            # —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Ñ–æ—Ä–º–∞—Ç YYYY-MM-DD ‚Äî –æ—Ç—Å–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –æ—Å—Ç–∞–≤–∏–º —Å—Ç—Ä–æ–∫–æ–π
            df["_sort"] = pd.to_datetime(df["–î–∞—Ç–∞"], errors="coerce")
            df = df.sort_values("_sort", ascending=False).drop(columns=["_sort"])

            st.dataframe(
                df[["–î–∞—Ç–∞", "–ó–∞–≥–æ–ª–æ–≤–æ–∫", "–ö–ª—é—á–µ–≤–æ–µ", "–°—Å—ã–ª–∫–∞"]],
                use_container_width=True,
                column_config={
                    "–°—Å—ã–ª–∫–∞": st.column_config.LinkColumn("–°—Å—ã–ª–∫–∞", help="–û—Ç–∫—Ä—ã—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫"),
                }
            )

            # –ö–Ω–æ–ø–∫–∏ –≤—ã–≥—Ä—É–∑–∫–∏
            col_dl_txt.download_button(
                "–°–∫–∞—á–∞—Ç—å TXT",
                data=io.BytesIO(text.encode("utf-8")),
                file_name=f"news_run_{date.today().isoformat()}.txt",
                mime="text/plain"
            )
            col_dl_csv.download_button(
                "–°–∫–∞—á–∞—Ç—å CSV",
                data=io.BytesIO(df.to_csv(index=False).encode("utf-8")),
                file_name=f"news_run_{date.today().isoformat()}.csv",
                mime="text/csv"
            )
        except (PPLXNewsError, ValueError) as e:
            st.error(str(e))


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 5. –í–∫–ª–∞–¥–∫–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è: –¥–æ–±–∞–≤–ª—è–µ–º News Run –º–µ–∂–¥—É AI-Insight –∏ Advance Eye
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# 5. –í–∫–ª–∞–¥–∫–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è: –¥–æ–±–∞–≤–ª—è–µ–º News Run –º–µ–∂–¥—É AI-Insight –∏ Advance Eye
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
tab_ts, tab_ai, tab_news, tab_eye = st.tabs(
    ["‚è±Ô∏è Timesheet", "üìä AI-Insight", "üóû News Run", "üëÅÔ∏è Advance Eye"]
)

with tab_ts:
    # nikabot-style —Ñ–æ—Ä–º–∞ —É—á—ë—Ç–∞ –≤—Ä–µ–º–µ–Ω–∏ (autoselect –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –Ω–µ–¥–µ–ª—å–Ω–∞—è —Å–µ—Ç–∫–∞)
    render_timesheet_tab()

with tab_ai:
    run_ai_insight_tab()

with tab_news:
    run_news_run_tab()

with tab_eye:
    run_advance_eye_tab()
